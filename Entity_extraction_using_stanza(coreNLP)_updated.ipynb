{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "39c04e78333c4c8d93b3a887da8fdc37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c444c8266cc4618a8272869a6b36e49",
              "IPY_MODEL_a71d068301624624a76c43a22732398e",
              "IPY_MODEL_12358736d0a64586a11d3defa51f382c"
            ],
            "layout": "IPY_MODEL_0cd50460d4704707bd6803498669f068"
          }
        },
        "6c444c8266cc4618a8272869a6b36e49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1c54aa7441845fc9f664f6f7e21d235",
            "placeholder": "​",
            "style": "IPY_MODEL_2af4362e43c84e628b4d5536255a4a88",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "a71d068301624624a76c43a22732398e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_422630d1097b401bbf0c69103c9d615e",
            "max": 249072763,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_46ffae3a64614aa3ad3d9ca6fd6acacf",
            "value": 249072763
          }
        },
        "12358736d0a64586a11d3defa51f382c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9256f9c190c4f68a8344c212713444a",
            "placeholder": "​",
            "style": "IPY_MODEL_fabfb57dd03341309e23b7f20002e091",
            "value": " 249M/249M [00:03&lt;00:00, 42.7MB/s]"
          }
        },
        "0cd50460d4704707bd6803498669f068": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c54aa7441845fc9f664f6f7e21d235": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2af4362e43c84e628b4d5536255a4a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "422630d1097b401bbf0c69103c9d615e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46ffae3a64614aa3ad3d9ca6fd6acacf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9256f9c190c4f68a8344c212713444a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fabfb57dd03341309e23b7f20002e091": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "706cc90d418443249c366ded112814d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f42b26650c304c99a392c02ef6a529bb",
              "IPY_MODEL_38e8fb44567a4f489bf88cbb12347017",
              "IPY_MODEL_5697eb4982ec4f94ad794704d8381472"
            ],
            "layout": "IPY_MODEL_e5cd183c06114ceab84b484729c37757"
          }
        },
        "f42b26650c304c99a392c02ef6a529bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfa13087d5974480986e70ce3bedc9f1",
            "placeholder": "​",
            "style": "IPY_MODEL_d132889b97aa4ea49c9d4f56a22ef9b8",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "38e8fb44567a4f489bf88cbb12347017": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b43cb53cf59448f88623718c40093a0",
            "max": 1472232964,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b52bd488cf54c55be00c5cf05355d82",
            "value": 1472232964
          }
        },
        "5697eb4982ec4f94ad794704d8381472": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e0d1bb4b89c46ae9bc1a988a851d82e",
            "placeholder": "​",
            "style": "IPY_MODEL_6f4ad2561d7644beba4d2a04d842bf22",
            "value": " 1.47G/1.47G [00:11&lt;00:00, 256MB/s]"
          }
        },
        "e5cd183c06114ceab84b484729c37757": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfa13087d5974480986e70ce3bedc9f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d132889b97aa4ea49c9d4f56a22ef9b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b43cb53cf59448f88623718c40093a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b52bd488cf54c55be00c5cf05355d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e0d1bb4b89c46ae9bc1a988a851d82e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f4ad2561d7644beba4d2a04d842bf22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Entity Pairs (Triples) extraction using Stanza-CoreNLP client**"
      ],
      "metadata": {
        "id": "5rDFqn-gnrtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Credits and References:**\n",
        "https://github.com/SoniaMola/SemanticsExtraction/blob/main/README.md"
      ],
      "metadata": {
        "id": "MQ5vvM58bwh0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mount Drive"
      ],
      "metadata": {
        "id": "u3SN9em_pxrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "os.chdir('/content/drive/MyDrive')"
      ],
      "metadata": {
        "id": "HaV_OkqppuaL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1a77d95-5283-4d7d-90ad-17891d39c27f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Requirements\n",
        "\n"
      ],
      "metadata": {
        "id": "MujbyCLDrIYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Do it only first time!\n",
        "!wget https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip --no-check-certificate\n",
        "!unzip stanford-corenlp-full-2018-10-05.zip"
      ],
      "metadata": {
        "id": "JslOqvwYyppC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366b3890-cfa5-4c4e-c3d3-4eb3f9b91e48",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-13 15:39:09--  https://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-full-2018-10-05.zip [following]\n",
            "--2024-06-13 15:39:09--  https://downloads.cs.stanford.edu/nlp/software/stanford-corenlp-full-2018-10-05.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "WARNING: cannot verify downloads.cs.stanford.edu's certificate, issued by ‘CN=InCommon RSA Server CA,OU=InCommon,O=Internet2,L=Ann Arbor,ST=MI,C=US’:\n",
            "  Issued certificate has expired.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 393239982 (375M) [application/zip]\n",
            "Saving to: ‘stanford-corenlp-full-2018-10-05.zip’\n",
            "\n",
            "stanford-corenlp-fu 100%[===================>] 375.02M  5.10MB/s    in 69s     \n",
            "\n",
            "2024-06-13 15:40:19 (5.43 MB/s) - ‘stanford-corenlp-full-2018-10-05.zip’ saved [393239982/393239982]\n",
            "\n",
            "Archive:  stanford-corenlp-full-2018-10-05.zip\n",
            "   creating: stanford-corenlp-full-2018-10-05/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom-1.2.10-src.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/CoreNLP-to-HTML.xsl  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/README.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday-0.4.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LIBRARY-LICENSES  \n",
            "   creating: stanford-corenlp-full-2018-10-05/sutime/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/british.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/defs.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/spanish.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.sutime.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/sutime/english.holidays.sutime.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/ejml-0.23-src.zip  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/build.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-javadoc.jar  \n",
            "   creating: stanford-corenlp-full-2018-10-05/tokensregex/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/retokenize.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.properties  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/tokensregex/color.rules.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json-api-1.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-models.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/protobuf.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordDependenciesManual.pdf  \n",
            "   creating: stanford-corenlp-full-2018-10-05/patterns/\n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/example.properties  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/otherpeople.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/goldplaces.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/stopwords.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/presidents.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/names.txt  \n",
            " extracting: stanford-corenlp-full-2018-10-05/patterns/places.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/patterns/goldnames.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-simple.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/input.txt  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/xom.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/StanfordCoreNlpDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-core-2.3.0.1.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/RESOURCE-LICENSES  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.activation-api-1.2.0-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/slf4j-api.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/pom-java-11.xml  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ejml-0.23.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/javax.json.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/Makefile  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/corenlp.sh  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/joda-time-2.9-sources.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-api-2.4.0-b180830.0359.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jollyday.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/ShiftReduceDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/jaxb-impl-2.4.0-b180830.0438.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/stanford-corenlp-3.9.2.jar  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/SemgrexDemo.java  \n",
            "  inflating: stanford-corenlp-full-2018-10-05/LICENSE.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stanza\n",
        "!pip install textacy\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "#NLTK Installation\n",
        "import nltk\n",
        "import spacy\n",
        "nltk.download('all')\n",
        "!pip install word2number\n",
        "!pip install rdflib\n",
        "!pip install SPARQLWrapper\n",
        "!pip install get_ent_type_obj_using_flair\n",
        "!pip install tika"
      ],
      "metadata": {
        "id": "0MPhB07CNUAm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa94889d-b839-4992-cfd2-0d8398354c5a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stanza in /usr/local/lib/python3.10/dist-packages (1.8.2)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (from stanza) (2.12.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from stanza) (1.25.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (3.20.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from stanza) (2.31.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from stanza) (3.3)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from stanza) (0.10.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from stanza) (2.3.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stanza) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (1.12.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->stanza) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.3.0->stanza) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->stanza) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: textacy in /usr/local/lib/python3.10/dist-packages (0.13.0)\n",
            "Requirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (5.3.3)\n",
            "Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.0.10)\n",
            "Requirement already satisfied: cytoolz>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from textacy) (0.12.3)\n",
            "Requirement already satisfied: floret~=0.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (0.10.5)\n",
            "Requirement already satisfied: jellyfish>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.0.4)\n",
            "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.4.2)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.3)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.25.2)\n",
            "Requirement already satisfied: pyphen>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (0.15.0)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.2.2)\n",
            "Requirement already satisfied: spacy~=3.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.7.5)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.10/dist-packages (from textacy) (4.66.4)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2024.6.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->textacy) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.4.8)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.12.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.4.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy~=3.0->textacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (4.12.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy~=3.0->textacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy~=3.0->textacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy~=3.0->textacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy~=3.0->textacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy~=3.0->textacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy~=3.0->textacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy~=3.0->textacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy~=3.0->textacy) (0.1.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.6.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: word2number in /usr/local/lib/python3.10/dist-packages (1.1)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.10/dist-packages (7.0.0)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (0.6.1)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\n",
            "Requirement already satisfied: SPARQLWrapper in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.10/dist-packages (from SPARQLWrapper) (7.0.0)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (0.6.1)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->SPARQLWrapper) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CORENLP_HOME\"] = '/content/drive/MyDrive/stanford-corenlp-full-2018-10-05'\n"
      ],
      "metadata": {
        "id": "A9FpAU3iyhld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkjmmPY9DmoF"
      },
      "source": [
        "# Pre-processing the text\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONTRACTION_MAP = {\n",
        "\"ain't\": \"is not\",\n",
        "\"aren't\": \"are not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he will\",\n",
        "\"he'll've\": \"he he will have\",\n",
        "\"he's\": \"he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how is\",\n",
        "\"I'd\": \"I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I will\",\n",
        "\"I'll've\": \"I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"i'd\": \"i would\",\n",
        "\"i'd've\": \"i would have\",\n",
        "\"i'll\": \"i will\",\n",
        "\"i'll've\": \"i will have\",\n",
        "\"i'm\": \"i am\",\n",
        "\"i've\": \"i have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it will\",\n",
        "\"it'll've\": \"it will have\",\n",
        "\"it's\": \"it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she will\",\n",
        "\"she'll've\": \"she will have\",\n",
        "\"she's\": \"she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as\",\n",
        "\"that'd\": \"that would\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that is\",\n",
        "\"there'd\": \"there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there is\",\n",
        "\"they'd\": \"they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they will\",\n",
        "\"they'll've\": \"they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what will\",\n",
        "\"what'll've\": \"what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who will\",\n",
        "\"who'll've\": \"who will have\",\n",
        "\"who's\": \"who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you will\",\n",
        "\"you'll've\": \"you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ],
      "metadata": {
        "id": "FZbegaUyRhj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_references(text):\n",
        "    common_patterns = [\"References\"]\n",
        "\n",
        "    # Find the index of the earliest occurrence of the common pattern\n",
        "    start_index = len(text)\n",
        "    for pattern in common_patterns:\n",
        "        index = text.find(pattern)\n",
        "        if index != -1 and index < start_index:\n",
        "            start_index = index\n",
        "\n",
        "    # Remove everything after the start of the references section\n",
        "    if start_index < len(text):\n",
        "        text = text[:start_index]\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "3Nz2cfIERPG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def replace_contractions(text, contraction_map):\n",
        "    for contraction, replacement in contraction_map.items():\n",
        "        text = re.sub(r'\\b' + re.escape(contraction) + r'\\b', replacement, text)\n",
        "    return text\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "#     text = remove_special_character(text, \"<0x02>\")\n",
        "        # Construct the regular expression dynamically to match the special character sequence anywhere in the text\n",
        "    regex_pattern = re.escape(\"\\\\x02\")\n",
        "\n",
        "    # Remove the special character sequence anywhere it occurs\n",
        "    text = re.sub(regex_pattern, '', text)\n",
        "\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "    text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(r'\\{.*?\\}', '', text)\n",
        "    text = re.sub(r'\\<.*?\\>', '', text)\n",
        "\n",
        "    # re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    text = text.replace('\\r\\n', '')\n",
        "    text = text.replace('\\\\r\\\\n', '')\n",
        "    text = text.replace('\\\\r', '')\n",
        "    text = text.replace('\\\\n', '')\n",
        "    # Replace newline characters with empty string\n",
        "    text = text.replace('\\n', '')\n",
        "\n",
        "    # Replace '&' with 'and'\n",
        "    text = text.replace('&', 'and')\n",
        "\n",
        "    pattern = r'\\b(\\w+)-(\\w+)\\b'\n",
        "    text = re.sub(pattern, r'\\1\\2', text)\n",
        "    # Replace 'Fig.  X' with 'FigX' where X is any number\n",
        "    text = re.sub(r'Fig\\. *(\\d+)', r'Fig\\1', text)\n",
        "\n",
        "    # Replace contractions\n",
        "    text = replace_contractions(text, CONTRACTION_MAP)\n",
        "\n",
        "    # Use regex to replace multiple spaces with a single space\n",
        "    # text = re.sub(r'\\s+', ' ', text)\n",
        "\n",
        "    # Strip any leading or trailing spaces\n",
        "    # text = text.strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "uYHc8ELqRY0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_special_characters(text):\n",
        "  import re\n",
        "  # Create a regex pattern to match all characters except letter or number\n",
        "  pattern = r\"[^A-Za-z0-9.,?!;:']+\"\n",
        "  # Remove special characters from the string\n",
        "  text = re.sub(pattern, ' ', text)\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "mBRMe1AF7eWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tika import parser\n",
        "raw = parser.from_file('https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf')\n",
        "text = raw['content']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HN87HHKkaZNc",
        "outputId": "a671ac3c-6c57-4166-9bee-3696651b4d96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-06-17 17:45:03,688 [MainThread  ] [INFO ]  Retrieving https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf to /tmp/paper_files-paper-2017-file-3f5ee243547dee91fbd053c1c4a845aa-paper.pdf.\n",
            "INFO:tika.tika:Retrieving https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf to /tmp/paper_files-paper-2017-file-3f5ee243547dee91fbd053c1c4a845aa-paper.pdf.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = remove_references(text)\n",
        "# print(cleaned_text)\n",
        "\n",
        "# Use a regular expression to find hyphenated words split across lines\n",
        "pattern = r\"(\\b\\w+)-\\s+(\\w+\\b)\"\n",
        "# Replace the hyphen and the following whitespace with the second part of the word\n",
        "text = re.sub(pattern, r'\\1\\2', cleaned_text)\n",
        "\n",
        "text = remove_special_characters(text)\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2EHVcD8avIs",
        "outputId": "c50fa5cb-fb20-4e05-c82a-4b141ed8a18b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English to French translation task, our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. 1 Introduction Recurrent neural networks, long short term memory 12 and gated recurrent 7 neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation 29, 2, 5 . Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31, 21, 13 . Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot product attention, multi head attention and the parameter free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. Work performed while at Google Brain. Work performed while at Google Research. 31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach, CA, USA. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht 1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences 2, 16 . In all but a few cases 22 , however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions 11 . In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention weighted positions, an effect we counteract with Multi Head Attention as described in section 3.2. Self attention, sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task independent sentence representations 4, 22, 23, 19 . End to end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple language question answering and language modeling tasks 28 . To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self attention and discuss its advantages over models such as 14, 15 and 8 . 3 Model Architecture Most competitive neural sequence transduction models have an encoder decoder structure 5, 2, 29 . Here, the encoder maps an input sequence of symbol representations x1, ..., xn to a sequence of continuous representations z z1, ..., zn . Given z, the decoder then generates an output sequence y1, ..., ym of symbols one element at a time. At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self attention and point wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is composed of a stack of N 6 identical layers. Each layer has two sub layers. The first is a multi head self attention mechanism, and the second is a simple, position2 Figure 1: The Transformer model architecture. wise fully connected feed forward network. We employ a residual connection 10 around each of the two sub layers, followed by layer normalization 1 . That is, the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself. To facilitate these residual connections, all sub layers in the model, as well as the embedding layers, produce outputs of dimension dmodel 512. Decoder: The decoder is also composed of a stack of N 6 identical layers. In addition to the two sub layers in each encoder layer, the decoder inserts a third sub layer, which performs multi head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub layers, followed by layer normalization. We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot Product Attention We call our particular attention Scaled Dot Product Attention Figure 2 . The input consists of queries and keys of dimension dk, and values of dimension dv . We compute the dot products of the 3 Scaled Dot Product Attention Multi Head Attention Figure 2: left Scaled Dot Product Attention. right Multi Head Attention consists of several attention layers running in parallel. query with all keys, divide each by dk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V . We compute the matrix of outputs as: Attention Q,K, V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention. Dot product attention is identical to our algorithm, except for the scaling factor of 1 dk . Additive attention computes the compatibility function using a feed forward network with a single hidden layer. While the two are similar in theoretical complexity, dot product attention is much faster and more space efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk 3 . We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1 dk . 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv dimensional output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2. Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. 4To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q k dk i 1 qiki, has mean 0 and variance dk. 4 MultiHead Q,K, V Concat head1, ...,headh W O where headi Attention QWQ i ,KW K i , V WV i Where the projections are parameter matricesWQ i Rdmodel dk ,WK i Rdmodel dk ,WV i Rdmodel dv and WO Rhdv dmodel . In this work we employ h 8 parallel attention layers, or heads. For each of these we use dk dv dmodel h 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single head attention with full dimensionality. 3.2.3 Applications of Attention in our Model The Transformer uses multi head attention in three different ways: In encoder decoder attention layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder decoder attention mechanisms in sequence to sequence models such as 31, 2, 8 . The encoder contains self attention layers. In a self attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Similarly, self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto regressive property. We implement this inside of scaled dot product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections. See Figure 2. 3.3 Position wise Feed Forward Networks In addition to attention sub layers, each of the layers in our encoder and decoder contains a fully connected feed forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between. FFN x max 0, xW1 b1 W2 b2 2 While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel 512, and the inner layer has dimensionality dff 2048. 3.4 Embeddings and Softmax Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre softmax linear transformation, similar to 24 . In the embedding layers, we multiply those weights by dmodel. 3.5 Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add positional encodings to the input embeddings at the 5 Table 1: Maximum path lengths, per layer complexity and minimum number of sequential operations for different layer types. n is the sequence length, d is the representation dimension, k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention. Layer Type Complexity per Layer Sequential Maximum Path Length Operations Self Attention O n2 d O 1 O 1 Recurrent O n d2 O n O n Convolutional O k n d2 O 1 O logk n Self Attention restricted O r n d O 1 O n r bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed 8 . In this work, we use sine and cosine functions of different frequencies: PE pos,2i sin pos 100002i dmodel PE pos,2i 1 cos pos 100002i dmodel where pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2 to 10000 2 . We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos k can be represented as a linear function of PEpos. We also experimented with using learned positional embeddings 8 instead, and found that the two versions produced nearly identical results see Table 3 row E . We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1, ..., xn to another sequence of equal length z1, ..., zn , with xi, zi Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long range dependencies in the network. Learning long range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long range dependencies 11 . Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O n sequential operations. In terms of computational complexity, self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state of the art models in machine translations, such as word piece 31 and byte pair 25 representations. To improve computational performance for tasks involving very long sequences, self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position. This would increase the maximum path length to O n r . We plan to investigate this approach further in future work. A single convolutional layer with kernel width k n does not connect all pairs of input and output positions. Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels, or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions 6 , however, decrease the complexity considerably, to O k n d n d2 . Even with k n, however, the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer, the approach we take in our model. As side benefit, self attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training This section describes the training regime for our models. 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs. Sentences were encoded using byte pair encoding 3 , which has a shared sourcetarget vocabulary of about 37000 tokens. For English French, we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 . Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models, described on the bottom line of table 3 , step time was 1.0 seconds. The big models were trained for 300,000 steps 3.5 days . 5.3 Optimizer We used the Adam optimizer 17 with 1 0.9, 2 0.98 and 10 9. We varied the learning rate over the course of training, according to the formula: lrate d 0.5model min step num 0.5, step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup steps 4000. 5.4 Regularization We employ three types of regularization during training: Residual Dropout We apply dropout 27 to the output of each sub layer, before it is added to the sub layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop 0.1. 7 Table 2: The Transformer achieves better BLEU scores than previous state of the art models on the English to German and English to French newstest2014 tests at a fraction of the training cost. Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training, we employed label smoothing of value ls 0.1 30 . This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results 6.1 Machine Translation On the WMT 2014 English to German translation task, the big transformer model Transformer big in Table 2 outperforms the best previously reported models including ensembles by more than 2.0 BLEU, establishing a new state of the art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English to French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1 4 the training cost of the previous state of the art model. The Transformer big model trained for English to French used dropout rate Pdrop 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10 minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty 0.6 31 . These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length 50, but terminate early when possible 31 . Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single precision floating point capacity of each GPU 5. 6.2 Model Variations To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English to German translation on the development set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. 5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively. 8 Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English to German translation development set, newstest2013. Listed perplexities are per wordpiece, according to our byte pair encoding, and should not be compared to per word perplexities. N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows C and D that, as expected, bigger models are better, and dropout is very helpful in avoiding over fitting. In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model. 7 Conclusion In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English to German and WMT 2014 English to French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https: github.com tensorflow tensor2tensor. Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration. 9 https: github.com tensorflow tensor2tensor https: github.com tensorflow tensor2tensor \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inizialization of the client"
      ],
      "metadata": {
        "id": "rrh0JKpk1oBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd stanford-corenlp-full-2018-10-05"
      ],
      "metadata": {
        "id": "vjXVixGrKoGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c33d7c07-2b10-4889-bac1-99a494958227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/stanford-corenlp-full-2018-10-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import stanza\n",
        "# Import client module\n",
        "from stanza.server import CoreNLPClient\n",
        "\n",
        "client = CoreNLPClient(timeout=1500000000000, be_quiet=True, annotators=['openie'],\n",
        "endpoint='http://localhost:8100')\n",
        "client.start()\n",
        "import time\n",
        "time.sleep(10)\n",
        "document = client.annotate(text, properties={\"annotators\":\"tokenize,ssplit,pos,depparse,natlog,openie\",\n",
        "                                 \"outputFormat\": \"json\",\"openie.triple.strict\":\"true\"})"
      ],
      "metadata": {
        "id": "MPhjDJTuy5Pq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f15b90f-48c4-4eb5-e7fc-3f1263077a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Writing properties to tmp file: corenlp_server-e628b5125dd34746.props\n",
            "INFO:stanza:Starting server with command: java -Xmx5G -cp /content/drive/MyDrive/stanford-corenlp-full-2018-10-05/* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 8100 -timeout 1500000000000 -threads 5 -maxCharLength 100000 -quiet True -serverProperties corenlp_server-e628b5125dd34746.props -annotators openie -preload -outputFormat serialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entity pairs (Triples) extraction"
      ],
      "metadata": {
        "id": "w5dqcEEMyarF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "triples = []\n",
        "i = 0\n",
        "for sentence in document['sentences']:\n",
        "  tokens = sentence['tokens']\n",
        "  sent = ''\n",
        "  for i in range(len(tokens)):\n",
        "        sent += ' '\n",
        "        sent += tokens[i]['originalText']\n",
        "\n",
        "  i = i +1\n",
        "\n",
        "  for triple in sentence['openie']:\n",
        "    try:\n",
        "      triples.append({\n",
        "          'subject': triple['subject'],\n",
        "          'relation': triple['relation'],\n",
        "          'object': triple['object'],\n",
        "          'sentence': sent\n",
        "      })\n",
        "    except:\n",
        "      pass\n"
      ],
      "metadata": {
        "id": "1RmpitqrTt01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing triples\n",
        "for tripla in triples:\n",
        "  print(tripla)"
      ],
      "metadata": {
        "id": "AlSvYFeiaAi5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "563a9b57-60ee-41c1-d3a0-0c54fb331fcc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'subject': 'Attention', 'relation': 'is', 'object': 'All', 'sentence': ' Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .'}\n",
            "{'subject': 'convolutional', 'relation': 'include', 'object': 'encoder', 'sentence': ' Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .'}\n",
            "{'subject': 'convolutional', 'relation': 'include', 'object': 'decoder', 'sentence': ' Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .'}\n",
            "{'subject': 'Attention', 'relation': 'is All', 'object': 'you Need', 'sentence': ' Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .'}\n",
            "{'subject': 'you', 'relation': 'Need', 'object': 'Attention', 'sentence': ' Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .'}\n",
            "{'subject': 'performing models', 'relation': 'connect', 'object': 'encoder', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'performing models', 'relation': 'also connect decoder through', 'object': 'attention mechanism', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'models', 'relation': 'also connect encoder through', 'object': 'attention mechanism', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'performing models', 'relation': 'also connect encoder through', 'object': 'attention mechanism', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'models', 'relation': 'also connect', 'object': 'encoder', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'models', 'relation': 'also connect decoder through', 'object': 'attention mechanism', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'performing models', 'relation': 'also connect', 'object': 'encoder', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'performing models', 'relation': 'connect decoder through', 'object': 'attention mechanism', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'performing models', 'relation': 'connect', 'object': 'decoder', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'models', 'relation': 'connect', 'object': 'encoder', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'models', 'relation': 'also connect', 'object': 'decoder', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'models', 'relation': 'connect encoder through', 'object': 'attention mechanism', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'models', 'relation': 'connect decoder through', 'object': 'attention mechanism', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'performing models', 'relation': 'connect encoder through', 'object': 'attention mechanism', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'models', 'relation': 'connect', 'object': 'decoder', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'performing models', 'relation': 'also connect', 'object': 'decoder', 'sentence': ' The best performing models also connect the encoder and decoder through an attention mechanism .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'network architecture based solely on attention mechanisms dispensing with recurrence', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new network architecture based on attention mechanisms dispensing entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new network architecture based solely on attention mechanisms', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'simple network architecture based solely on attention mechanisms dispensing with recurrence', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'simple network architecture based on attention mechanisms', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'network architecture based on attention mechanisms', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'network architecture based on attention mechanisms dispensing with recurrence entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new network architecture based on attention mechanisms', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'network architecture', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'Transformer', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'simple network architecture based on attention mechanisms dispensing with recurrence', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new network architecture based solely on attention mechanisms dispensing', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new simple network architecture based on attention mechanisms dispensing entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new network architecture based solely on attention mechanisms dispensing with recurrence entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'network architecture based on attention mechanisms dispensing', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new network architecture based on attention mechanisms dispensing with recurrence entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new simple network architecture', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'network architecture based solely on attention mechanisms dispensing with recurrence entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'simple network architecture based solely on attention mechanisms dispensing entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new network architecture based solely on attention mechanisms dispensing entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'simple network architecture based solely on attention mechanisms dispensing', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'network architecture based on attention mechanisms dispensing entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new simple network architecture based solely on attention mechanisms dispensing entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'simple network architecture', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'network architecture based solely on attention mechanisms', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new simple network architecture based solely on attention mechanisms dispensing with recurrence', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new simple network architecture based solely on attention mechanisms dispensing with recurrence entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'network architecture based solely on attention mechanisms dispensing entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'network architecture based solely on attention mechanisms dispensing', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new simple network architecture based on attention mechanisms dispensing', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'simple network architecture based on attention mechanisms dispensing entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'simple network architecture based solely on attention mechanisms dispensing with recurrence entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'simple network architecture based on attention mechanisms dispensing with recurrence entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new simple network architecture based on attention mechanisms dispensing with recurrence entirely', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new network architecture based on attention mechanisms dispensing with recurrence', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new simple network architecture based on attention mechanisms', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'simple network architecture based on attention mechanisms dispensing', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new network architecture based on attention mechanisms dispensing', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new simple network architecture based on attention mechanisms dispensing with recurrence', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new network architecture based solely on attention mechanisms dispensing with recurrence', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'simple network architecture based solely on attention mechanisms', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new network architecture', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new simple network architecture based solely on attention mechanisms', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'network architecture based on attention mechanisms dispensing with recurrence', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'We', 'relation': 'propose', 'object': 'new simple network architecture based solely on attention mechanisms dispensing', 'sentence': ' We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'existing best results including ensembles', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'results by over 2 BLEU', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'best results by over 2 BLEU', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'existing best results including ensembles by over 2 BLEU', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'existing results by over 2 BLEU', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'existing best results by over 2 BLEU', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'best results including ensembles', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'results including ensembles by over 2 BLEU', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'existing results including ensembles', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'achieves', 'object': '28.4 BLEU on WMT 2014 Englishto German translation task', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'results', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'existing best results', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'best results including ensembles by over 2 BLEU', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'achieves', 'object': '28.4 BLEU', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'results including ensembles', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'best results', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'existing results', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'Our model', 'relation': 'improving over', 'object': 'existing results including ensembles by over 2 BLEU', 'sentence': ' Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'small fraction of training costs of models', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'new model state of art BLEU score of 41.0', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'new single model state', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'single model state of art BLEU score', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'model state of art BLEU score of 41.0', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'new single model state of art BLEU score of 41.0', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'single model state', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes model state after', 'object': 'training', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'new model state', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'small fraction of training costs of models from literature', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'model state', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'model state of art BLEU score', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'single model state of art BLEU score of 41.0', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'fraction of training costs', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes model state On', 'object': 'WMT 2014 English', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes model state On', 'object': 'WMT 2014 English to translation task', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'small fraction of training costs of best models', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'fraction', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'fraction of training costs of best models from literature', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'small fraction of training costs of best models from literature', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'small fraction of training costs', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'new model state of art BLEU score', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'small fraction', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'fraction of training costs of models from literature', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'fraction of training costs from literature', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes fraction On', 'object': 'WMT 2014 English to translation task', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes model state On', 'object': 'WMT 2014 English to French translation task', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'fraction of training costs of best models', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'fraction of training costs of models', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes fraction On', 'object': 'WMT 2014 English to French translation task', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'small fraction of training costs from literature', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes fraction after', 'object': 'training', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes fraction On', 'object': 'WMT 2014 English', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': 'our model', 'relation': 'establishes', 'object': 'new single model state of art BLEU score', 'sentence': ' On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .'}\n",
            "{'subject': '1 Introduction Recurrent neural networks', 'relation': 'memory in', 'object': 'particular', 'sentence': ' 1 Introduction Recurrent neural networks , long short term memory 12 and gated recurrent 7 neural networks in particular , have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation 29 , 2 , 5 .'}\n",
            "{'subject': 'Numerous efforts', 'relation': 'push', 'object': 'boundaries of recurrent language models', 'sentence': ' Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .'}\n",
            "{'subject': 'efforts', 'relation': 'push', 'object': 'boundaries', 'sentence': ' Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .'}\n",
            "{'subject': 'efforts', 'relation': 'push', 'object': 'boundaries of recurrent language models', 'sentence': ' Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .'}\n",
            "{'subject': 'efforts', 'relation': 'push', 'object': 'boundaries of language models', 'sentence': ' Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .'}\n",
            "{'subject': 'Numerous efforts', 'relation': 'push', 'object': 'boundaries of language models', 'sentence': ' Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .'}\n",
            "{'subject': 'Numerous efforts', 'relation': 'push', 'object': 'boundaries', 'sentence': ' Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .'}\n",
            "{'subject': 'order', 'relation': 'is', 'object': 'random', 'sentence': ' Listing order is random .'}\n",
            "{'subject': 'Listing order', 'relation': 'is', 'object': 'random', 'sentence': ' Listing order is random .'}\n",
            "{'subject': 'Jakob', 'relation': 'replacing', 'object': 'RNNs', 'sentence': ' Jakob proposed replacing RNNs with self attention and started the effort to evaluate this idea .'}\n",
            "{'subject': 'Jakob', 'relation': 'replacing RNNs with', 'object': 'self attention', 'sentence': ' Jakob proposed replacing RNNs with self attention and started the effort to evaluate this idea .'}\n",
            "{'subject': 'Ashish', 'relation': 'is with', 'object': 'Illia designed', 'sentence': ' Ashish , with Illia , designed and implemented the first Transformer models and has been crucially involved in every aspect of this work .'}\n",
            "{'subject': 'Noam', 'relation': 'became', 'object': 'other person involved in nearly detail', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'Noam', 'relation': 'became', 'object': 'person involved in nearly detail', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'Noam', 'relation': 'became', 'object': 'person', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'other person', 'relation': 'involved in', 'object': 'detail', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'person', 'relation': 'involved in', 'object': 'nearly detail', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'Noam', 'relation': 'proposed', 'object': 'multi head attention', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'Noam', 'relation': 'proposed', 'object': 'head attention', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'Noam', 'relation': 'became', 'object': 'person involved in detail', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'Noam', 'relation': 'proposed', 'object': 'scaled dot product attention', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'person', 'relation': 'involved in', 'object': 'detail', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'Noam', 'relation': 'proposed', 'object': 'parameter free position representation', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'Noam', 'relation': 'proposed', 'object': 'dot product attention', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'Noam', 'relation': 'became', 'object': 'other person involved in detail', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'Noam', 'relation': 'proposed', 'object': 'parameter position representation', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'other person', 'relation': 'involved in', 'object': 'nearly detail', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'Noam', 'relation': 'became', 'object': 'other person', 'sentence': ' Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .'}\n",
            "{'subject': 'evaluated countless model variants', 'relation': 'is in', 'object': 'our original codebase', 'sentence': ' Niki designed , implemented , tuned and evaluated countless model variants in our original codebase and tensor2tensor .'}\n",
            "{'subject': 'Llion', 'relation': 'experimented with', 'object': 'novel model variants', 'sentence': ' Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .'}\n",
            "{'subject': 'Llion', 'relation': 'experimented with', 'object': 'model variants', 'sentence': ' Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .'}\n",
            "{'subject': 'Llion', 'relation': 'also experimented with', 'object': 'novel model variants', 'sentence': ' Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .'}\n",
            "{'subject': 'Llion', 'relation': 'also experimented with', 'object': 'model variants', 'sentence': ' Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .'}\n",
            "{'subject': 'Lukasz', 'relation': 'spent at_time', 'object': 'days', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Aidan', 'relation': 'replacing', 'object': 'our earlier codebase', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Aidan', 'relation': 'replacing', 'object': 'our codebase', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Aidan', 'relation': 'spent at_time', 'object': 'days', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'results', 'relation': 'accelerating', 'object': 'our research', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Aidan', 'relation': 'designing', 'object': 'various parts of tensor2tensor', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Lukasz', 'relation': 'spent at_time', 'object': 'countless long days', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'results', 'relation': 'massively accelerating', 'object': 'our research', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Lukasz', 'relation': 'spent at_time', 'object': 'long days', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Aidan', 'relation': 'designing', 'object': 'various parts', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Aidan', 'relation': 'designing', 'object': 'parts of tensor2tensor', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Aidan', 'relation': 'spent at_time', 'object': 'countless days', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Lukasz', 'relation': 'spent at_time', 'object': 'countless days', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Aidan', 'relation': 'spent at_time', 'object': 'long days', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Aidan', 'relation': 'spent at_time', 'object': 'countless long days', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Aidan', 'relation': 'designing', 'object': 'parts', 'sentence': ' Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .'}\n",
            "{'subject': 'Work', 'relation': 'performed at', 'object': 'Google Research', 'sentence': ' Work performed while at Google Research .'}\n",
            "{'subject': '31st Conference', 'relation': 'NIPS', 'object': 'Long Beach', 'sentence': ' 31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .'}\n",
            "{'subject': 'Conference', 'relation': 'NIPS', 'object': 'CA', 'sentence': ' 31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .'}\n",
            "{'subject': 'Conference', 'relation': 'NIPS', 'object': '2017', 'sentence': ' 31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .'}\n",
            "{'subject': 'Conference', 'relation': 'NIPS', 'object': 'Long Beach', 'sentence': ' 31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .'}\n",
            "{'subject': '31st Conference', 'relation': 'NIPS', 'object': '2017', 'sentence': ' 31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .'}\n",
            "{'subject': '31st Conference', 'relation': 'NIPS', 'object': 'CA', 'sentence': ' 31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .'}\n",
            "{'subject': 'models', 'relation': 'factor', 'object': 'computation along symbol positions of input sequences', 'sentence': ' Recurrent models typically factor computation along the symbol positions of the input and output sequences .'}\n",
            "{'subject': 'Recurrent models', 'relation': 'factor', 'object': 'computation along symbol positions of input sequences', 'sentence': ' Recurrent models typically factor computation along the symbol positions of the input and output sequences .'}\n",
            "{'subject': 'Recurrent models', 'relation': 'typically factor', 'object': 'computation along symbol positions of input sequences', 'sentence': ' Recurrent models typically factor computation along the symbol positions of the input and output sequences .'}\n",
            "{'subject': 'models', 'relation': 'factor', 'object': 'computation along symbol positions', 'sentence': ' Recurrent models typically factor computation along the symbol positions of the input and output sequences .'}\n",
            "{'subject': 'models', 'relation': 'typically factor', 'object': 'computation', 'sentence': ' Recurrent models typically factor computation along the symbol positions of the input and output sequences .'}\n",
            "{'subject': 'models', 'relation': 'factor', 'object': 'computation', 'sentence': ' Recurrent models typically factor computation along the symbol positions of the input and output sequences .'}\n",
            "{'subject': 'Recurrent models', 'relation': 'factor', 'object': 'computation', 'sentence': ' Recurrent models typically factor computation along the symbol positions of the input and output sequences .'}\n",
            "{'subject': 'models', 'relation': 'typically factor', 'object': 'computation along symbol positions of input sequences', 'sentence': ' Recurrent models typically factor computation along the symbol positions of the input and output sequences .'}\n",
            "{'subject': 'Recurrent models', 'relation': 'typically factor', 'object': 'computation along symbol positions', 'sentence': ' Recurrent models typically factor computation along the symbol positions of the input and output sequences .'}\n",
            "{'subject': 'models', 'relation': 'typically factor', 'object': 'computation along symbol positions', 'sentence': ' Recurrent models typically factor computation along the symbol positions of the input and output sequences .'}\n",
            "{'subject': 'Recurrent models', 'relation': 'typically factor', 'object': 'computation', 'sentence': ' Recurrent models typically factor computation along the symbol positions of the input and output sequences .'}\n",
            "{'subject': 'Recurrent models', 'relation': 'factor', 'object': 'computation along symbol positions', 'sentence': ' Recurrent models typically factor computation along the symbol positions of the input and output sequences .'}\n",
            "{'subject': 'they', 'relation': 'Aligning positions to', 'object': 'steps', 'sentence': ' Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .'}\n",
            "{'subject': 'they', 'relation': 'Aligning positions to', 'object': 'steps in computation time', 'sentence': ' Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .'}\n",
            "{'subject': 'they', 'relation': 'Aligning', 'object': 'positions', 'sentence': ' Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .'}\n",
            "{'subject': 'sequence', 'relation': 'ht as', 'object': 'function of previous state ht 1', 'sentence': ' Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .'}\n",
            "{'subject': 'sequence', 'relation': 'ht as', 'object': 'function', 'sentence': ' Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .'}\n",
            "{'subject': 'steps', 'relation': 'is in', 'object': 'computation time', 'sentence': ' Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .'}\n",
            "{'subject': 'sequence', 'relation': 'ht as', 'object': 'function of state ht 1', 'sentence': ' Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .'}\n",
            "{'subject': 'sequence', 'relation': 'ht as', 'object': 'function of previous hidden state ht 1', 'sentence': ' Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .'}\n",
            "{'subject': 'sequence', 'relation': 'ht as', 'object': 'function of hidden state ht 1', 'sentence': ' Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .'}\n",
            "{'subject': 'memory constraints', 'relation': 'limit', 'object': 'batching', 'sentence': ' This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .'}\n",
            "{'subject': 'sequential nature', 'relation': 'precludes', 'object': 'parallelization within training examples', 'sentence': ' This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .'}\n",
            "{'subject': 'nature', 'relation': 'precludes', 'object': 'parallelization', 'sentence': ' This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .'}\n",
            "{'subject': 'sequential nature', 'relation': 'precludes parallelization', 'object': 'memory constraints limit', 'sentence': ' This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .'}\n",
            "{'subject': 'memory constraints', 'relation': 'batching across', 'object': 'examples', 'sentence': ' This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .'}\n",
            "{'subject': 'nature', 'relation': 'precludes', 'object': 'parallelization within training examples', 'sentence': ' This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .'}\n",
            "{'subject': 'nature', 'relation': 'precludes parallelization', 'object': 'memory constraints limit', 'sentence': ' This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .'}\n",
            "{'subject': 'sequential nature', 'relation': 'precludes', 'object': 'parallelization', 'sentence': ' This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .'}\n",
            "{'subject': 'memory constraints', 'relation': 'limit', 'object': 'batching across examples', 'sentence': ' This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .'}\n",
            "{'subject': 'work', 'relation': 'also improving model performance in case of', 'object': 'latter', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'Recent work', 'relation': 'improving model performance in case of', 'object': 'latter', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'work', 'relation': 'has achieved', 'object': 'improvements in efficiency', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'work', 'relation': 'improving model performance in case of', 'object': 'latter', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'Recent work', 'relation': 'also improving model performance in case of', 'object': 'latter', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'Recent work', 'relation': 'has achieved', 'object': 'significant improvements in computational efficiency', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'work', 'relation': 'has achieved', 'object': 'significant improvements in computational efficiency', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'work', 'relation': 'has achieved improvements through', 'object': 'factorization tricks 18', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'work', 'relation': 'also improving', 'object': 'model performance', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'work', 'relation': 'has achieved', 'object': 'significant improvements', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'work', 'relation': 'improving', 'object': 'model performance', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'work', 'relation': 'has achieved', 'object': 'significant improvements in efficiency', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'Recent work', 'relation': 'has achieved', 'object': 'improvements in computational efficiency', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'Recent work', 'relation': 'has achieved improvements through', 'object': 'factorization tricks 18', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'Recent work', 'relation': 'has achieved', 'object': 'significant improvements in efficiency', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'Recent work', 'relation': 'has achieved', 'object': 'significant improvements', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'Recent work', 'relation': 'improving', 'object': 'model performance', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'Recent work', 'relation': 'also improving', 'object': 'model performance', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'work', 'relation': 'has achieved', 'object': 'improvements in computational efficiency', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'significant improvements', 'relation': 'is in', 'object': 'computational efficiency', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'Recent work', 'relation': 'has achieved', 'object': 'improvements', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'work', 'relation': 'has achieved', 'object': 'improvements', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'Recent work', 'relation': 'has achieved', 'object': 'improvements in efficiency', 'sentence': ' Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'allowing without regard', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'allowing', 'object': 'modeling', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'part of', 'object': 'sequence modeling in various tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'integral part of', 'object': 'compelling sequence modeling in tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'compelling sequence modeling', 'relation': 'is in', 'object': 'various tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'part of sequence modeling in tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'allowing modeling without', 'object': 'regard', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'allowing without', 'object': 'regard to their distance', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'integral part of compelling sequence modeling in various tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'part of', 'object': 'sequence modeling', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'part of', 'object': 'compelling sequence modeling in tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'part of compelling sequence modeling in various tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'integral part', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'allowing modeling without', 'object': 'regard to their distance in input sequences', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'allowing without', 'object': 'regard to their distance in input sequences', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'allowing without regard to their distance', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'allowing', 'object': 'modeling of dependencies', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'integral part of compelling sequence modeling', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'part', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'integral part of sequence modeling in tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'allowing modeling without', 'object': 'regard to their distance', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'integral part of', 'object': 'compelling sequence modeling', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'their distance', 'relation': 'is in', 'object': 'input sequences', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'part of', 'object': 'sequence modeling in tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'integral part of sequence modeling', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'part of compelling sequence modeling', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'part of', 'object': 'compelling sequence modeling', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'allowing', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'part of sequence modeling in various tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'integral part of compelling sequence modeling in tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'integral part of', 'object': 'sequence modeling', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'integral part of sequence modeling in various tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'integral part of', 'object': 'sequence modeling in various tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'integral part of', 'object': 'compelling sequence modeling in various tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'part of', 'object': 'compelling sequence modeling in various tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'allowing without', 'object': 'regard', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'allowing without regard to their distance in input sequences', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'part of sequence modeling', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'have become', 'object': 'part of compelling sequence modeling in tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'Attention mechanisms', 'relation': 'integral part of', 'object': 'sequence modeling in tasks', 'sentence': ' Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .'}\n",
            "{'subject': 'attention mechanisms', 'relation': 'are', 'object': 'however used', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'such attention mechanisms', 'relation': 'are used in', 'object': 'conjunction with recurrent network', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'such attention mechanisms', 'relation': 'are used in', 'object': 'conjunction', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'attention mechanisms', 'relation': 'are used in', 'object': 'conjunction with network', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'attention mechanisms', 'relation': 'however are used in', 'object': 'conjunction with network', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'attention mechanisms', 'relation': 'however are used in', 'object': 'conjunction with recurrent network', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'such attention mechanisms', 'relation': 'however are used in', 'object': 'conjunction', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'attention mechanisms', 'relation': 'however are used in', 'object': 'conjunction', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'attention mechanisms', 'relation': 'are used in', 'object': 'conjunction', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'conjunction', 'relation': 'is with', 'object': 'recurrent network', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'such attention mechanisms', 'relation': 'are', 'object': 'used', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'such attention mechanisms', 'relation': 'however are used in', 'object': 'conjunction with recurrent network', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'such attention mechanisms', 'relation': 'are', 'object': 'however used', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'such attention mechanisms', 'relation': 'however are used in', 'object': 'conjunction with network', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'attention mechanisms', 'relation': 'are used in', 'object': 'conjunction with recurrent network', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'such attention mechanisms', 'relation': 'are used in', 'object': 'conjunction with network', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'attention mechanisms', 'relation': 'are', 'object': 'used', 'sentence': ' In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .'}\n",
            "{'subject': 'we', 'relation': 'propose model architecture In', 'object': 'work', 'sentence': ' In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .'}\n",
            "{'subject': 'we', 'relation': 'propose', 'object': 'relying', 'sentence': ' In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .'}\n",
            "{'subject': 'we', 'relation': 'propose', 'object': 'Transformer', 'sentence': ' In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .'}\n",
            "{'subject': 'we', 'relation': 'propose', 'object': 'instead relying', 'sentence': ' In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .'}\n",
            "{'subject': 'we', 'relation': 'propose', 'object': 'relying entirely', 'sentence': ' In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .'}\n",
            "{'subject': 'we', 'relation': 'propose', 'object': 'model architecture', 'sentence': ' In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .'}\n",
            "{'subject': 'we', 'relation': 'propose', 'object': 'instead relying entirely', 'sentence': ' In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .'}\n",
            "{'subject': 'we', 'relation': 'propose Transformer In', 'object': 'work', 'sentence': ' In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .'}\n",
            "{'subject': 'Transformer', 'relation': 'reach state in', 'object': 'translation quality', 'sentence': ' The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .'}\n",
            "{'subject': 'Transformer', 'relation': 'reach state', 'object': 'trained', 'sentence': ' The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .'}\n",
            "{'subject': 'Transformer', 'relation': 'reach state', 'object': 'trained for as little as twelve hours', 'sentence': ' The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .'}\n",
            "{'subject': 'Transformer', 'relation': 'reach', 'object': 'new state of art', 'sentence': ' The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .'}\n",
            "{'subject': 'Transformer', 'relation': 'allows for', 'object': 'parallelization', 'sentence': ' The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .'}\n",
            "{'subject': 'Transformer', 'relation': 'reach', 'object': 'state', 'sentence': ' The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .'}\n",
            "{'subject': 'Transformer', 'relation': 'reach state', 'object': 'trained for as little as twelve hours on eight P100 GPUs', 'sentence': ' The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .'}\n",
            "{'subject': 'Transformer', 'relation': 'reach', 'object': 'state of art', 'sentence': ' The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .'}\n",
            "{'subject': 'Transformer', 'relation': 'allows for', 'object': 'more parallelization', 'sentence': ' The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .'}\n",
            "{'subject': 'Transformer', 'relation': 'reach', 'object': 'new state', 'sentence': ' The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .'}\n",
            "{'subject': 'Transformer', 'relation': 'allows for', 'object': 'significantly more parallelization', 'sentence': ' The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .'}\n",
            "{'subject': 'all', 'relation': 'use', 'object': 'networks', 'sentence': ' 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .'}\n",
            "{'subject': 'all', 'relation': 'use networks as', 'object': 'basic building block', 'sentence': ' 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .'}\n",
            "{'subject': 'all', 'relation': 'use', 'object': 'convolutional networks', 'sentence': ' 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .'}\n",
            "{'subject': 'all', 'relation': 'use', 'object': 'convolutional neural networks', 'sentence': ' 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .'}\n",
            "{'subject': 'all', 'relation': 'use networks as', 'object': 'building block', 'sentence': ' 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .'}\n",
            "{'subject': 'all', 'relation': 'use', 'object': 'neural networks', 'sentence': ' 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .'}\n",
            "{'subject': 'number', 'relation': 'grows In', 'object': 'models', 'sentence': ' In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .'}\n",
            "{'subject': 'number', 'relation': 'grows for', 'object': 'linearly ConvS2S', 'sentence': ' In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .'}\n",
            "{'subject': 'number', 'relation': 'grows in', 'object': 'distance', 'sentence': ' In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .'}\n",
            "{'subject': 'number', 'relation': 'grows in', 'object': 'distance between positions', 'sentence': ' In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .'}\n",
            "{'subject': 'number', 'relation': 'grows for', 'object': 'ConvS2S', 'sentence': ' In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .'}\n",
            "{'subject': 'it', 'relation': 'learn', 'object': 'dependencies between distant positions 11', 'sentence': ' This makes it more difficult to learn dependencies between distant positions 11 .'}\n",
            "{'subject': 'it', 'relation': 'dependencies between', 'object': 'positions 11', 'sentence': ' This makes it more difficult to learn dependencies between distant positions 11 .'}\n",
            "{'subject': 'it', 'relation': 'learn', 'object': 'dependencies', 'sentence': ' This makes it more difficult to learn dependencies between distant positions 11 .'}\n",
            "{'subject': 'it', 'relation': 'learn', 'object': 'dependencies between positions 11', 'sentence': ' This makes it more difficult to learn dependencies between distant positions 11 .'}\n",
            "{'subject': 'it', 'relation': 'dependencies between', 'object': 'distant positions 11', 'sentence': ' This makes it more difficult to learn dependencies between distant positions 11 .'}\n",
            "{'subject': 'attention mechanism', 'relation': 'relating', 'object': 'different positions', 'sentence': ' Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .'}\n",
            "{'subject': 'attention mechanism', 'relation': 'relating', 'object': 'positions of sequence', 'sentence': ' Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .'}\n",
            "{'subject': 'Self attention', 'relation': 'is', 'object': 'attention mechanism', 'sentence': ' Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .'}\n",
            "{'subject': 'attention mechanism', 'relation': 'relating', 'object': 'positions of single sequence', 'sentence': ' Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .'}\n",
            "{'subject': 'attention mechanism', 'relation': 'relating', 'object': 'different positions of single sequence', 'sentence': ' Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .'}\n",
            "{'subject': 'attention mechanism', 'relation': 'relating', 'object': 'different positions of sequence', 'sentence': ' Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .'}\n",
            "{'subject': 'attention mechanism', 'relation': 'relating', 'object': 'positions', 'sentence': ' Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .'}\n",
            "{'subject': 'Self attention', 'relation': 'has', 'object': 'has used successfully', 'sentence': ' Self attention has been used successfully in a variety of tasks including reading comprehension , abstractive summarization , textual entailment and learning task independent sentence representations 4 , 22 , 23 , 19 .'}\n",
            "{'subject': 'Self attention', 'relation': 'has', 'object': 'has used', 'sentence': ' Self attention has been used successfully in a variety of tasks including reading comprehension , abstractive summarization , textual entailment and learning task independent sentence representations 4 , 22 , 23 , 19 .'}\n",
            "{'subject': 'Transformer', 'relation': 'is', 'object': 'first transduction model relying entirely on self attention', 'sentence': ' To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .'}\n",
            "{'subject': 'Transformer', 'relation': 'is', 'object': 'transduction model relying', 'sentence': ' To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .'}\n",
            "{'subject': 'Transformer', 'relation': 'is', 'object': 'first transduction model', 'sentence': ' To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .'}\n",
            "{'subject': 'Transformer', 'relation': 'is', 'object': 'transduction model relying on self attention', 'sentence': ' To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .'}\n",
            "{'subject': 'Transformer', 'relation': 'is', 'object': 'transduction model relying entirely on self attention', 'sentence': ' To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .'}\n",
            "{'subject': 'Transformer', 'relation': 'is', 'object': 'first transduction model relying', 'sentence': ' To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .'}\n",
            "{'subject': 'Transformer', 'relation': 'is', 'object': 'first transduction model relying on self attention', 'sentence': ' To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .'}\n",
            "{'subject': 'Transformer', 'relation': 'is', 'object': 'first transduction model relying entirely', 'sentence': ' To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .'}\n",
            "{'subject': 'Transformer', 'relation': 'is', 'object': 'transduction model', 'sentence': ' To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .'}\n",
            "{'subject': 'Transformer', 'relation': 'is', 'object': 'transduction model relying entirely', 'sentence': ' To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .'}\n",
            "{'subject': 'we', 'relation': 'discuss', 'object': 'its advantages', 'sentence': ' In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .'}\n",
            "{'subject': 'we', 'relation': 'will describe', 'object': 'Transformer', 'sentence': ' In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .'}\n",
            "{'subject': 'we', 'relation': 'will describe Transformer In', 'object': 'following sections', 'sentence': ' In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .'}\n",
            "{'subject': 'we', 'relation': 'will describe Transformer In', 'object': 'sections', 'sentence': ' In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .'}\n",
            "{'subject': 'we', 'relation': 'motivate', 'object': 'self attention', 'sentence': ' In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .'}\n",
            "{'subject': 'encoder', 'relation': 'Here maps', 'object': 'input sequence of symbol representations x1', 'sentence': ' Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .'}\n",
            "{'subject': 'encoder', 'relation': 'Here maps', 'object': 'xn to sequence of continuous representations', 'sentence': ' Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .'}\n",
            "{'subject': 'encoder', 'relation': 'maps', 'object': 'xn to sequence of representations', 'sentence': ' Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .'}\n",
            "{'subject': 'encoder', 'relation': 'maps', 'object': 'xn to sequence', 'sentence': ' Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .'}\n",
            "{'subject': 'encoder', 'relation': 'Here maps', 'object': 'input sequence', 'sentence': ' Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .'}\n",
            "{'subject': 'encoder', 'relation': 'maps', 'object': 'input sequence of symbol representations x1', 'sentence': ' Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .'}\n",
            "{'subject': 'encoder', 'relation': 'Here maps', 'object': 'xn to sequence of representations', 'sentence': ' Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .'}\n",
            "{'subject': 'encoder', 'relation': 'Here maps', 'object': 'xn', 'sentence': ' Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .'}\n",
            "{'subject': 'encoder', 'relation': 'maps', 'object': 'xn', 'sentence': ' Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .'}\n",
            "{'subject': 'encoder', 'relation': 'maps', 'object': 'input sequence', 'sentence': ' Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .'}\n",
            "{'subject': 'encoder', 'relation': 'Here maps', 'object': 'xn to sequence', 'sentence': ' Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .'}\n",
            "{'subject': 'encoder', 'relation': 'maps', 'object': 'xn to sequence of continuous representations', 'sentence': ' Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .'}\n",
            "{'subject': 'decoder', 'relation': 'generates', 'object': 'ym of symbols one element', 'sentence': ' Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .'}\n",
            "{'subject': 'decoder', 'relation': 'generates', 'object': 'ym of symbols one element at time', 'sentence': ' Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .'}\n",
            "{'subject': 'decoder', 'relation': 'generates output sequence y1 Given', 'object': 'z', 'sentence': ' Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .'}\n",
            "{'subject': 'output sequence y1', 'relation': 'ym of', 'object': 'symbols one element', 'sentence': ' Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .'}\n",
            "{'subject': 'decoder', 'relation': 'generates', 'object': 'output sequence y1', 'sentence': ' Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .'}\n",
            "{'subject': 'output sequence y1', 'relation': 'ym at', 'object': 'time', 'sentence': ' Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .'}\n",
            "{'subject': 'decoder', 'relation': 'generates', 'object': 'ym', 'sentence': ' Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .'}\n",
            "{'subject': 'decoder', 'relation': 'generates', 'object': 'ym at time', 'sentence': ' Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .'}\n",
            "{'subject': 'model', 'relation': 'consuming symbols as', 'object': 'additional input', 'sentence': ' At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .'}\n",
            "{'subject': 'model', 'relation': 'consuming', 'object': 'symbols', 'sentence': ' At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .'}\n",
            "{'subject': 'model', 'relation': 'consuming', 'object': 'generated symbols', 'sentence': ' At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .'}\n",
            "{'subject': 'model', 'relation': 'consuming', 'object': 'previously generated symbols', 'sentence': ' At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .'}\n",
            "{'subject': 'model', 'relation': 'consuming symbols as', 'object': 'input', 'sentence': ' At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .'}\n",
            "{'subject': 'overall architecture', 'relation': 'shown in', 'object': 'left halves Figure 1', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'overall architecture', 'relation': 'shown in', 'object': 'halves', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'overall architecture', 'relation': 'shown in', 'object': 'halves Figure 1', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'architecture', 'relation': 'using', 'object': 'wise', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'overall architecture', 'relation': 'using', 'object': 'self attention', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'architecture', 'relation': 'using', 'object': 'self attention', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'overall architecture', 'relation': 'using', 'object': 'wise', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'Transformer', 'relation': 'follows respectively', 'object': 'overall architecture', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'Transformer', 'relation': 'follows respectively', 'object': 'architecture', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'overall architecture', 'relation': 'using', 'object': 'stacked self attention', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'Transformer', 'relation': 'follows', 'object': 'architecture', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'architecture', 'relation': 'shown in', 'object': 'left halves', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'architecture', 'relation': 'using', 'object': 'point wise', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'architecture', 'relation': 'shown in', 'object': 'halves', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'architecture', 'relation': 'shown in', 'object': 'left halves Figure 1', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'architecture', 'relation': 'using', 'object': 'stacked self attention', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'point', 'relation': 'wise', 'object': 'fully connected layers for encoder', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'overall architecture', 'relation': 'using', 'object': 'point wise', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'overall architecture', 'relation': 'shown in', 'object': 'left halves', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'architecture', 'relation': 'shown in', 'object': 'halves Figure 1', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'Transformer', 'relation': 'follows', 'object': 'overall architecture', 'sentence': ' The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .'}\n",
            "{'subject': 'layer', 'relation': 'has', 'object': 'two sub layers', 'sentence': ' Each layer has two sub layers .'}\n",
            "{'subject': 'second', 'relation': 'is', 'object': 'simple', 'sentence': ' The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .'}\n",
            "{'subject': 'second', 'relation': 'is', 'object': 'simple Figure 1', 'sentence': ' The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .'}\n",
            "{'subject': 'first', 'relation': 'is', 'object': 'head self attention mechanism', 'sentence': ' The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .'}\n",
            "{'subject': 'second', 'relation': 'is', 'object': 'position2', 'sentence': ' The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .'}\n",
            "{'subject': 'first', 'relation': 'is', 'object': 'multi', 'sentence': ' The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .'}\n",
            "{'subject': 'second', 'relation': 'is', 'object': 'position2 Figure 1', 'sentence': ' The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .'}\n",
            "{'subject': 'first', 'relation': 'is', 'object': 'multi head self attention mechanism', 'sentence': ' The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .'}\n",
            "{'subject': 'second', 'relation': 'is', 'object': 'Figure 1', 'sentence': ' The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .'}\n",
            "{'subject': 'second', 'relation': 'is', 'object': 'simple position2 Figure 1', 'sentence': ' The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .'}\n",
            "{'subject': 'We', 'relation': 'employ', 'object': 'residual connection 10', 'sentence': ' We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .'}\n",
            "{'subject': 'We', 'relation': 'followed by', 'object': 'layer normalization 1', 'sentence': ' We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .'}\n",
            "{'subject': 'We', 'relation': 'employ', 'object': 'residual connection 10 around each two sub layers', 'sentence': ' We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .'}\n",
            "{'subject': 'We', 'relation': 'employ', 'object': 'connection 10 around each of two sub layers', 'sentence': ' We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .'}\n",
            "{'subject': 'We', 'relation': 'employ', 'object': 'residual connection 10 around each of two sub layers', 'sentence': ' We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .'}\n",
            "{'subject': 'We', 'relation': 'employ', 'object': 'connection 10', 'sentence': ' We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .'}\n",
            "{'subject': 'We', 'relation': 'employ', 'object': 'connection 10 around each two sub layers', 'sentence': ' We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .'}\n",
            "{'subject': 'Sublayer x', 'relation': 'is', 'object': 'where function', 'sentence': ' That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .'}\n",
            "{'subject': 'Sublayer x', 'relation': 'implemented by', 'object': 'sub layer', 'sentence': ' That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .'}\n",
            "{'subject': 'Sublayer x', 'relation': 'is', 'object': 'where function implemented by sub layer itself', 'sentence': ' That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .'}\n",
            "{'subject': 'Sublayer x', 'relation': 'is', 'object': 'where function implemented', 'sentence': ' That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .'}\n",
            "{'subject': 'Sublayer x', 'relation': 'is', 'object': 'function implemented by sub layer itself', 'sentence': ' That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .'}\n",
            "{'subject': 'Sublayer x', 'relation': 'is', 'object': 'function implemented by sub layer', 'sentence': ' That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .'}\n",
            "{'subject': 'Sublayer x', 'relation': 'implemented by', 'object': 'sub layer itself', 'sentence': ' That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .'}\n",
            "{'subject': 'Sublayer x', 'relation': 'is', 'object': 'function implemented', 'sentence': ' That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .'}\n",
            "{'subject': 'Sublayer x', 'relation': 'is', 'object': 'function', 'sentence': ' That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .'}\n",
            "{'subject': 'Sublayer x', 'relation': 'is', 'object': 'where function implemented by sub layer', 'sentence': ' That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .'}\n",
            "{'subject': 'decoder', 'relation': 'is composed of', 'object': 'stack of N 6 layers', 'sentence': ' Decoder : The decoder is also composed of a stack of N 6 identical layers .'}\n",
            "{'subject': 'decoder', 'relation': 'is also composed of', 'object': 'stack of N 6 identical layers', 'sentence': ' Decoder : The decoder is also composed of a stack of N 6 identical layers .'}\n",
            "{'subject': 'decoder', 'relation': 'is', 'object': 'also composed', 'sentence': ' Decoder : The decoder is also composed of a stack of N 6 identical layers .'}\n",
            "{'subject': 'decoder', 'relation': 'is composed of', 'object': 'stack', 'sentence': ' Decoder : The decoder is also composed of a stack of N 6 identical layers .'}\n",
            "{'subject': 'Decoder', 'relation': 'composed of', 'object': 'stack', 'sentence': ' Decoder : The decoder is also composed of a stack of N 6 identical layers .'}\n",
            "{'subject': 'decoder', 'relation': 'is also composed of', 'object': 'stack', 'sentence': ' Decoder : The decoder is also composed of a stack of N 6 identical layers .'}\n",
            "{'subject': 'decoder', 'relation': 'is composed of', 'object': 'stack of N 6 identical layers', 'sentence': ' Decoder : The decoder is also composed of a stack of N 6 identical layers .'}\n",
            "{'subject': 'decoder', 'relation': 'is', 'object': 'composed', 'sentence': ' Decoder : The decoder is also composed of a stack of N 6 identical layers .'}\n",
            "{'subject': 'decoder', 'relation': 'is also composed of', 'object': 'stack of N 6 layers', 'sentence': ' Decoder : The decoder is also composed of a stack of N 6 identical layers .'}\n",
            "{'subject': 'two sub layers', 'relation': 'is in', 'object': 'encoder layer', 'sentence': ' In addition to the two sub layers in each encoder layer , the decoder inserts a third sub layer , which performs multi head attention over the output of the encoder stack .'}\n",
            "{'subject': 'we', 'relation': 'Similar employ', 'object': 'residual connections around each of sub layers', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'we', 'relation': 'employ', 'object': 'connections', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'we', 'relation': 'Similar employ', 'object': 'connections around each of sub layers', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'we', 'relation': 'employ', 'object': 'residual connections around each of sub layers', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'we', 'relation': 'employ', 'object': 'residual connections around each sub layers', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'we', 'relation': 'employ', 'object': 'connections around each sub layers', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'we', 'relation': 'followed by', 'object': 'layer normalization', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'we', 'relation': 'Similar employ', 'object': 'connections around each sub layers', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'we', 'relation': 'Similar employ', 'object': 'residual connections around each sub layers', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'we', 'relation': 'Similar employ', 'object': 'residual connections', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'we', 'relation': 'Similar employ', 'object': 'connections', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'we', 'relation': 'employ', 'object': 'residual connections', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'we', 'relation': 'employ', 'object': 'connections around each of sub layers', 'sentence': ' Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .'}\n",
            "{'subject': 'self attention sub layer', 'relation': 'prevent positions', 'object': 'attending to positions', 'sentence': ' We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .'}\n",
            "{'subject': 'self attention sub layer', 'relation': 'prevent positions', 'object': 'attending', 'sentence': ' We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .'}\n",
            "{'subject': 'self attention sub layer', 'relation': 'prevent', 'object': 'positions', 'sentence': ' We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .'}\n",
            "{'subject': 'self attention sub layer', 'relation': 'is in', 'object': 'decoder', 'sentence': ' We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .'}\n",
            "{'subject': 'self attention sub layer', 'relation': 'prevent positions', 'object': 'attending to subsequent positions', 'sentence': ' We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .'}\n",
            "{'subject': 'predictions', 'relation': 'depend on', 'object': 'outputs at positions less than i. 3.2 Attention', 'sentence': ' This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .'}\n",
            "{'subject': 'predictions', 'relation': 'depend on', 'object': 'outputs', 'sentence': ' This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .'}\n",
            "{'subject': 'output embeddings', 'relation': 'are offset by', 'object': 'position', 'sentence': ' This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .'}\n",
            "{'subject': 'predictions', 'relation': 'depend on', 'object': 'outputs at positions', 'sentence': ' This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .'}\n",
            "{'subject': 'output embeddings', 'relation': 'are offset by', 'object': 'one position', 'sentence': ' This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .'}\n",
            "{'subject': 'masking', 'relation': 'combined with', 'object': 'fact', 'sentence': ' This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .'}\n",
            "{'subject': 'predictions', 'relation': 'depend on', 'object': 'outputs at positions less', 'sentence': ' This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .'}\n",
            "{'subject': 'output', 'relation': 'is computed as', 'object': 'weighted sum of values', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is', 'object': 'where computed by compatibility function', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'output', 'relation': 'is computed as', 'object': 'sum of values', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is', 'object': 'where computed by compatibility function of query', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is computed by', 'object': 'compatibility function', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'output', 'relation': 'is', 'object': 'computed', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is computed by', 'object': 'compatibility function of query with corresponding key', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'assigned to', 'object': 'value', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is', 'object': 'where computed by compatibility function of query with key', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is computed by', 'object': 'compatibility function of query with key', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is', 'object': 'where computed', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is', 'object': 'where computed by compatibility function with corresponding key', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is', 'object': 'where computed by compatibility function of query with corresponding key', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'output', 'relation': 'is computed as', 'object': 'weighted sum', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is computed by', 'object': 'compatibility function with corresponding key', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is computed by', 'object': 'compatibility function of query', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'compatibility function', 'relation': 'is with', 'object': 'corresponding key', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is computed by', 'object': 'compatibility function with key', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'output', 'relation': 'is computed as', 'object': 'sum', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is', 'object': 'computed', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'weight', 'relation': 'is', 'object': 'where computed by compatibility function with key', 'sentence': ' The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .'}\n",
            "{'subject': 'our particular attention', 'relation': 'Scaled', 'object': 'Dot Product Attention Figure 2', 'sentence': ' 3.2.1 Scaled Dot Product Attention We call our particular attention Scaled Dot Product Attention Figure 2 .'}\n",
            "{'subject': 'our attention', 'relation': 'Scaled', 'object': 'Dot Product Attention Figure 2', 'sentence': ' 3.2.1 Scaled Dot Product Attention We call our particular attention Scaled Dot Product Attention Figure 2 .'}\n",
            "{'subject': 'input', 'relation': 'consists of', 'object': 'queries of dimension dk', 'sentence': ' The input consists of queries and keys of dimension dk , and values of dimension dv .'}\n",
            "{'subject': 'input', 'relation': 'values of', 'object': 'dimension dv', 'sentence': ' The input consists of queries and keys of dimension dk , and values of dimension dv .'}\n",
            "{'subject': 'input', 'relation': 'consists of', 'object': 'queries', 'sentence': ' The input consists of queries and keys of dimension dk , and values of dimension dv .'}\n",
            "{'subject': 'Multi Head Attention', 'relation': 'consists of', 'object': 'several attention layers', 'sentence': ' right Multi Head Attention consists of several attention layers running in parallel .'}\n",
            "{'subject': 'Multi Head Attention', 'relation': 'consists of', 'object': 'several attention layers running', 'sentence': ' right Multi Head Attention consists of several attention layers running in parallel .'}\n",
            "{'subject': 'Multi Head Attention', 'relation': 'consists of', 'object': 'attention layers running in parallel', 'sentence': ' right Multi Head Attention consists of several attention layers running in parallel .'}\n",
            "{'subject': 'Multi Head Attention', 'relation': 'consists of', 'object': 'attention layers running', 'sentence': ' right Multi Head Attention consists of several attention layers running in parallel .'}\n",
            "{'subject': 'Multi Head Attention', 'relation': 'consists of', 'object': 'several attention layers running in parallel', 'sentence': ' right Multi Head Attention consists of several attention layers running in parallel .'}\n",
            "{'subject': 'Multi Head Attention', 'relation': 'consists of', 'object': 'attention layers', 'sentence': ' right Multi Head Attention consists of several attention layers running in parallel .'}\n",
            "{'subject': 'query', 'relation': 'is with', 'object': 'keys', 'sentence': ' query with all keys , divide each by dk , and apply a softmax function to obtain the weights on the values .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set packed together into matrix Q', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set packed', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set of queries packed into matrix Q', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set of queries simultaneously packed together', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set of queries packed together into matrix Q', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set simultaneously packed together into matrix Q', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set simultaneously packed into matrix Q', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set of queries simultaneously packed', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set packed together', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set of queries simultaneously', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set of queries packed together', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set of queries simultaneously packed into matrix Q', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set of queries packed', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set simultaneously packed', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function In', 'object': 'practice', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set simultaneously packed together', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute', 'object': 'attention function', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set packed into matrix Q', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set of queries simultaneously packed together into matrix Q', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set simultaneously', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'we', 'relation': 'compute attention function on', 'object': 'set of queries', 'sentence': ' In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .'}\n",
            "{'subject': 'keys', 'relation': 'are', 'object': 'also packed together', 'sentence': ' The keys and values are also packed together into matrices K and V .'}\n",
            "{'subject': 'keys', 'relation': 'are', 'object': 'packed', 'sentence': ' The keys and values are also packed together into matrices K and V .'}\n",
            "{'subject': 'keys', 'relation': 'are', 'object': 'packed together into matrices K', 'sentence': ' The keys and values are also packed together into matrices K and V .'}\n",
            "{'subject': 'keys', 'relation': 'are', 'object': 'also packed into matrices K', 'sentence': ' The keys and values are also packed together into matrices K and V .'}\n",
            "{'subject': 'keys', 'relation': 'are', 'object': 'packed together', 'sentence': ' The keys and values are also packed together into matrices K and V .'}\n",
            "{'subject': 'keys', 'relation': 'are', 'object': 'packed into matrices K', 'sentence': ' The keys and values are also packed together into matrices K and V .'}\n",
            "{'subject': 'keys', 'relation': 'are', 'object': 'also packed', 'sentence': ' The keys and values are also packed together into matrices K and V .'}\n",
            "{'subject': 'keys', 'relation': 'are', 'object': 'also packed together into matrices K', 'sentence': ' The keys and values are also packed together into matrices K and V .'}\n",
            "{'subject': 'We', 'relation': 'Attention', 'object': 'V softmax QKT dk V 1', 'sentence': ' We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .'}\n",
            "{'subject': 'We', 'relation': 'Attention', 'object': 'K', 'sentence': ' We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .'}\n",
            "{'subject': 'We', 'relation': 'Attention', 'object': 'dot product attention', 'sentence': ' We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .'}\n",
            "{'subject': 'We', 'relation': 'Attention', 'object': 'dot product multiplicative attention', 'sentence': ' We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .'}\n",
            "{'subject': 'We', 'relation': 'compute', 'object': 'matrix', 'sentence': ' We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .'}\n",
            "{'subject': 'We', 'relation': 'Attention', 'object': 'Q', 'sentence': ' We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .'}\n",
            "{'subject': 'We', 'relation': 'compute', 'object': 'matrix of outputs', 'sentence': ' We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .'}\n",
            "{'subject': 'compatibility function', 'relation': 'using', 'object': 'feed', 'sentence': ' Additive attention computes the compatibility function using a feed forward network with a single hidden layer .'}\n",
            "{'subject': 'Additive attention', 'relation': 'computes', 'object': 'compatibility function', 'sentence': ' Additive attention computes the compatibility function using a feed forward network with a single hidden layer .'}\n",
            "{'subject': 'attention', 'relation': 'computes', 'object': 'compatibility function', 'sentence': ' Additive attention computes the compatibility function using a feed forward network with a single hidden layer .'}\n",
            "{'subject': 'forward network', 'relation': 'is with', 'object': 'single hidden layer', 'sentence': ' Additive attention computes the compatibility function using a feed forward network with a single hidden layer .'}\n",
            "{'subject': 'it', 'relation': 'can', 'object': 'can implemented', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'dot product attention', 'relation': 'is much faster', 'object': 'similar', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'it', 'relation': 'using', 'object': 'highly optimized matrix multiplication code', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'two', 'relation': 'are similar in', 'object': 'theoretical complexity', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'dot product attention', 'relation': 'is faster', 'object': 'similar', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'it', 'relation': 'using', 'object': 'optimized matrix multiplication code', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'dot product attention', 'relation': 'is faster', 'object': 'similar in theoretical complexity', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'it', 'relation': 'using', 'object': 'matrix multiplication code', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'dot product attention', 'relation': 'is', 'object': 'much faster', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'dot product attention', 'relation': 'is', 'object': 'faster', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'two', 'relation': 'are', 'object': 'similar', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'dot product attention', 'relation': 'is faster', 'object': 'can implemented', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'dot product attention', 'relation': 'is much faster', 'object': 'similar in theoretical complexity', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'dot product attention', 'relation': 'is much faster', 'object': 'can implemented', 'sentence': ' While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .'}\n",
            "{'subject': 'two mechanisms', 'relation': 'perform for', 'object': 'small values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for small values mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for small values mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for values mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'scaling for values of dk 3', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for small values mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for small values of dk two mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for values of dk two mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'scaling', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'mechanisms', 'relation': 'perform similarly for', 'object': 'values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'scaling for larger values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'two mechanisms', 'relation': 'perform similarly for', 'object': 'small values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for values two mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for small values two mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for small values mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for small values of dk mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for values of dk two mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for small values of dk two mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'two mechanisms', 'relation': 'perform similarly for', 'object': 'small values of dk', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'scaling for', 'object': 'larger values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for values two mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms', 'object': 'dot product attention', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'scaling for', 'object': 'values of dk 3', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for values mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms', 'object': 'dot product attention', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for values two mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'two mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for values of dk mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for small values of dk mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for small values two mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'scaling for', 'object': 'values of dk 3', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for values of dk mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for values mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'mechanisms', 'relation': 'perform for', 'object': 'small values of dk', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'two mechanisms', 'relation': 'perform for', 'object': 'values of dk', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for small values of dk two mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for values mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'two mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'scaling for', 'object': 'larger values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'scaling for values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'two mechanisms', 'relation': 'perform for', 'object': 'values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'mechanisms', 'relation': 'perform similarly for', 'object': 'small values of dk', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'scaling for larger values of dk 3', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'scaling for values of dk 3', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'two mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'mechanisms', 'relation': 'perform for', 'object': 'values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'scaling for larger values of dk 3', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for small values two mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'scaling for values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'mechanisms', 'relation': 'perform similarly for', 'object': 'values of dk', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for small values of dk mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for values of dk two mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'scaling for larger values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'scaling for', 'object': 'larger values of dk 3', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'two mechanisms', 'relation': 'perform similarly for', 'object': 'values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'mechanisms', 'relation': 'perform similarly for', 'object': 'small values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'scaling', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'two mechanisms', 'relation': 'perform similarly for', 'object': 'values of dk', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for values of dk two mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'two mechanisms', 'relation': 'perform for', 'object': 'small values of dk', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'two mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'for small values of dk mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for small values two mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'scaling for', 'object': 'values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'scaling for', 'object': 'larger values of dk 3', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for values of dk mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for small values of dk two mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for values two mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'additive attention', 'relation': 'outperforms dot product attention', 'object': 'mechanisms perform', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'mechanisms', 'relation': 'perform for', 'object': 'values of dk', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'scaling for', 'object': 'values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'mechanisms', 'relation': 'perform for', 'object': 'small values', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'attention', 'relation': 'outperforms dot product attention', 'object': 'for values of dk mechanisms perform similarly', 'sentence': ' While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .'}\n",
            "{'subject': 'dot products', 'relation': 'grow for', 'object': 'values of dk', 'sentence': ' We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .'}\n",
            "{'subject': 'dot products', 'relation': 'grow in', 'object': 'magnitude', 'sentence': ' We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .'}\n",
            "{'subject': 'dot products', 'relation': 'pushing softmax function into', 'object': 'regions', 'sentence': ' We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .'}\n",
            "{'subject': 'dot products', 'relation': 'grow for', 'object': 'large values', 'sentence': ' We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .'}\n",
            "{'subject': 'it', 'relation': 'has', 'object': 'gradients 4', 'sentence': ' We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .'}\n",
            "{'subject': 'dot products', 'relation': 'grow for', 'object': 'values', 'sentence': ' We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .'}\n",
            "{'subject': 'dot products', 'relation': 'pushing', 'object': 'softmax function', 'sentence': ' We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .'}\n",
            "{'subject': 'it', 'relation': 'has', 'object': 'extremely small gradients 4', 'sentence': ' We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .'}\n",
            "{'subject': 'it', 'relation': 'has', 'object': 'small gradients 4', 'sentence': ' We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .'}\n",
            "{'subject': 'dot products', 'relation': 'grow', 'object': 'large', 'sentence': ' We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .'}\n",
            "{'subject': 'dot products', 'relation': 'grow for', 'object': 'large values of dk', 'sentence': ' We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .'}\n",
            "{'subject': 'we', 'relation': 'scale dot products by', 'object': '1 dk', 'sentence': ' To counteract this effect , we scale the dot products by 1 dk .'}\n",
            "{'subject': 'we', 'relation': 'scale', 'object': 'dot products', 'sentence': ' To counteract this effect , we scale the dot products by 1 dk .'}\n",
            "{'subject': 'we', 'relation': 'counteract', 'object': 'effect', 'sentence': ' To counteract this effect , we scale the dot products by 1 dk .'}\n",
            "{'subject': 'queries', 'relation': 'learned respectively', 'object': 'linear projections', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'queries', 'relation': 'learned', 'object': 'projections', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'linearly project', 'object': 'queries', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'project values h times with', 'object': 'different', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'linearly project', 'object': 'values h times', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'project', 'object': 'queries', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'linearly project values h times with', 'object': 'different', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'learned respectively', 'object': 'projections', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'learned respectively', 'object': 'linear projections', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'learned projections respectively to', 'object': 'dk', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'project queries with', 'object': 'different', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'learned', 'object': 'projections', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'project keys with', 'object': 'different', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'learned projections to', 'object': 'dk', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'project', 'object': 'keys', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'learned', 'object': 'linear projections', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'project', 'object': 'values h times', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'linearly project', 'object': 'keys', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'queries', 'relation': 'learned', 'object': 'linear projections', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'queries', 'relation': 'learned projections to', 'object': 'dk', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'linearly project keys with', 'object': 'different', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'it', 'relation': 'linearly project queries with', 'object': 'different', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'queries', 'relation': 'learned projections respectively to', 'object': 'dk', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'queries', 'relation': 'learned respectively', 'object': 'projections', 'sentence': ' 3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .'}\n",
            "{'subject': 'we', 'relation': 'yielding', 'object': 'dv dimensional output values', 'sentence': ' On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .'}\n",
            "{'subject': 'we', 'relation': 'yielding', 'object': 'dv output values', 'sentence': ' On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .'}\n",
            "{'subject': 'we', 'relation': 'perform', 'object': 'attention function', 'sentence': ' On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .'}\n",
            "{'subject': 'we', 'relation': 'perform attention function in', 'object': 'parallel', 'sentence': ' On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .'}\n",
            "{'subject': 'These', 'relation': 'depicted in', 'object': 'Figure 2', 'sentence': ' These are concatenated and once again projected , resulting in the final values , as depicted in Figure 2 .'}\n",
            "{'subject': 'model', 'relation': 'jointly attend to', 'object': 'information at different positions', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'jointly attend to', 'object': 'information from different representation subspaces at positions', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'Multi head attention', 'relation': 'allows', 'object': 'model', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'attend to', 'object': 'information from representation subspaces at positions', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'attend to', 'object': 'information at positions', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'head attention', 'relation': 'allows', 'object': 'model', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'jointly attend to', 'object': 'information from representation subspaces at positions', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'jointly attend to', 'object': 'information from representation subspaces', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'jointly attend to', 'object': 'information from different representation subspaces', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'jointly attend to', 'object': 'information from different representation subspaces at different positions', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'attend to', 'object': 'information from different representation subspaces at positions', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'attend to', 'object': 'information from different representation subspaces at different positions', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'attend to', 'object': 'information from representation subspaces', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'attend to', 'object': 'information from representation subspaces at different positions', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'attend to', 'object': 'information', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'jointly attend to', 'object': 'information from representation subspaces at different positions', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'attend to', 'object': 'information at different positions', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'jointly attend to', 'object': 'information at positions', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'jointly attend to', 'object': 'information', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'model', 'relation': 'attend to', 'object': 'information from different representation subspaces', 'sentence': ' Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .'}\n",
            "{'subject': 'averaging', 'relation': 'inhibits With', 'object': 'single attention head', 'sentence': ' With a single attention head , averaging inhibits this .'}\n",
            "{'subject': 'averaging', 'relation': 'inhibits With', 'object': 'attention head', 'sentence': ' With a single attention head , averaging inhibits this .'}\n",
            "{'subject': 'q', 'relation': 'components of are', 'object': 'independent random variables', 'sentence': ' 4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .'}\n",
            "{'subject': 'components', 'relation': 'are', 'object': 'variables', 'sentence': ' 4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .'}\n",
            "{'subject': 'dot products', 'relation': 'get', 'object': 'large', 'sentence': ' 4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .'}\n",
            "{'subject': 'components', 'relation': 'are', 'object': 'independent random variables', 'sentence': ' 4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .'}\n",
            "{'subject': 'q', 'relation': 'components of are', 'object': 'independent variables', 'sentence': ' 4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .'}\n",
            "{'subject': 'independent random variables', 'relation': 'is with', 'object': 'mean 0', 'sentence': ' 4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .'}\n",
            "{'subject': '4To', 'relation': 'illustrate', 'object': 'assume', 'sentence': ' 4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .'}\n",
            "{'subject': 'q', 'relation': 'components of are', 'object': 'variables', 'sentence': ' 4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .'}\n",
            "{'subject': 'q', 'relation': 'components of are', 'object': 'random variables', 'sentence': ' 4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .'}\n",
            "{'subject': 'components', 'relation': 'are', 'object': 'independent variables', 'sentence': ' 4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .'}\n",
            "{'subject': 'components', 'relation': 'are', 'object': 'random variables', 'sentence': ' 4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .'}\n",
            "{'subject': 'their', 'relation': 'product', 'object': 'q k dk i 1 qiki', 'sentence': ' Then their dot product , q k dk i 1 qiki , has mean 0 and variance dk .'}\n",
            "{'subject': 'we', 'relation': 'employ heads In', 'object': 'work', 'sentence': ' In this work we employ h 8 parallel attention layers , or heads .'}\n",
            "{'subject': 'we', 'relation': 'employ', 'object': 'h 8 parallel attention layers', 'sentence': ' In this work we employ h 8 parallel attention layers , or heads .'}\n",
            "{'subject': 'we', 'relation': 'employ', 'object': 'h 8 attention layers', 'sentence': ' In this work we employ h 8 parallel attention layers , or heads .'}\n",
            "{'subject': 'we', 'relation': 'employ', 'object': 'heads', 'sentence': ' In this work we employ h 8 parallel attention layers , or heads .'}\n",
            "{'subject': 'we', 'relation': 'use', 'object': 'dk dv dmodel h 64', 'sentence': ' For each of these we use dk dv dmodel h 64 .'}\n",
            "{'subject': 'cost', 'relation': 'is similar to', 'object': 'that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'Due is similar to', 'object': 'that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is similar to', 'object': 'that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'Due is similar to', 'object': 'that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'Due is similar to', 'object': 'that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'Due is similar to', 'object': 'that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is similar to', 'object': 'that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is similar to', 'object': 'that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'Due is similar to', 'object': 'that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is similar to', 'object': 'that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'Due is similar to', 'object': 'that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is similar to', 'object': 'that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'Due is similar to', 'object': 'that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'Due is similar to', 'object': 'that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is similar to', 'object': 'that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'Due is similar to', 'object': 'that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is similar to', 'object': 'that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is similar to', 'object': 'that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is similar to', 'object': 'that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is similar to', 'object': 'that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'Due is similar to', 'object': 'that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to dimension of head similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'Due is similar to', 'object': 'that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is similar to', 'object': 'that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'single head attention', 'relation': 'is with', 'object': 'full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is similar to', 'object': 'that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is similar to', 'object': 'that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'Due is similar to', 'object': 'that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'Due is similar to', 'object': 'that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is similar to', 'object': 'that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to dimension of head similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'Due is similar to', 'object': 'that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'Due is similar to', 'object': 'that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'Due is similar to', 'object': 'that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'Due is similar to', 'object': 'that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'Due is similar to', 'object': 'that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'Due is similar to', 'object': 'that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'Due is similar to', 'object': 'that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is similar to', 'object': 'that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is similar to', 'object': 'that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'Due is similar to', 'object': 'that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'Due is similar to', 'object': 'that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'Due is similar to', 'object': 'that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is similar to', 'object': 'that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is similar to', 'object': 'that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is similar to', 'object': 'that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'Due is similar to', 'object': 'that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is similar to', 'object': 'that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'Due is similar to', 'object': 'that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is similar to', 'object': 'that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is similar to', 'object': 'that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'Due is similar to', 'object': 'that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is similar to', 'object': 'that of single head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of single head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is similar to', 'object': 'that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is similar to', 'object': 'that of head attention with full dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'Due is similar to', 'object': 'that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'computational cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is similar to', 'object': 'that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'is', 'object': 'Due to reduced dimension of head similar to that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is', 'object': 'Due to dimension of head similar to that of head attention', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'cost', 'relation': 'is', 'object': 'Due similar', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total cost', 'relation': 'is similar to', 'object': 'that of single head attention with dimensionality', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'total computational cost', 'relation': 'Due is similar to', 'object': 'that', 'sentence': ' Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .'}\n",
            "{'subject': 'encoder', 'relation': 'contains', 'object': 'self attention layers', 'sentence': ' The encoder contains self attention layers .'}\n",
            "{'subject': 'output', 'relation': 'is in', 'object': 'encoder', 'sentence': ' In a self attention layer all of the keys , values and queries come from the same place , in this case , the output of the previous layer in the encoder .'}\n",
            "{'subject': 'self attention layers', 'relation': 'Similarly allow', 'object': 'position in decoder', 'sentence': ' Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .'}\n",
            "{'subject': 'self attention layers', 'relation': 'is in', 'object': 'decoder', 'sentence': ' Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .'}\n",
            "{'subject': 'self attention layers', 'relation': 'allow', 'object': 'position in decoder', 'sentence': ' Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .'}\n",
            "{'subject': 'We', 'relation': 'prevent', 'object': 'leftward information flow', 'sentence': ' We need to prevent leftward information flow in the decoder to preserve the auto regressive property .'}\n",
            "{'subject': 'We', 'relation': 'prevent', 'object': 'leftward information flow in decoder', 'sentence': ' We need to prevent leftward information flow in the decoder to preserve the auto regressive property .'}\n",
            "{'subject': 'leftward information flow', 'relation': 'is in', 'object': 'decoder', 'sentence': ' We need to prevent leftward information flow in the decoder to preserve the auto regressive property .'}\n",
            "{'subject': 'We', 'relation': 'prevent', 'object': 'information flow in decoder', 'sentence': ' We need to prevent leftward information flow in the decoder to preserve the auto regressive property .'}\n",
            "{'subject': 'We', 'relation': 'prevent', 'object': 'information flow', 'sentence': ' We need to prevent leftward information flow in the decoder to preserve the auto regressive property .'}\n",
            "{'subject': 'We', 'relation': 'implement', 'object': 'inside of dot product attention', 'sentence': ' We implement this inside of scaled dot product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections .'}\n",
            "{'subject': 'We', 'relation': 'implement', 'object': 'inside of scaled dot product attention', 'sentence': ' We implement this inside of scaled dot product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections .'}\n",
            "{'subject': 'We', 'relation': 'implement', 'object': 'inside', 'sentence': ' We implement this inside of scaled dot product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections .'}\n",
            "{'subject': 'two linear transformations', 'relation': 'is with', 'object': 'ReLU activation in', 'sentence': ' This consists of two linear transformations with a ReLU activation in between .'}\n",
            "{'subject': 'linear transformations', 'relation': 'are same across', 'object': 'different positions', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'transformations', 'relation': 'are same across', 'object': 'positions', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'they', 'relation': 'FFN use', 'object': 'different parameters', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'they', 'relation': 'use', 'object': 'different parameters', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'linear transformations', 'relation': 'are same across', 'object': 'positions', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'they', 'relation': 'FFN use parameters from', 'object': 'layer', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'they', 'relation': 'use', 'object': 'parameters', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'linear transformations', 'relation': 'are', 'object': 'same', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'they', 'relation': 'FFN use parameters from', 'object': 'layer to layer', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'they', 'relation': 'use parameters from', 'object': 'layer', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'transformations', 'relation': 'are', 'object': 'same', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'they', 'relation': 'use parameters from', 'object': 'layer to layer', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'transformations', 'relation': 'are same across', 'object': 'different positions', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'they', 'relation': 'FFN use', 'object': 'parameters', 'sentence': ' FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .'}\n",
            "{'subject': 'two convolutions', 'relation': 'is with', 'object': 'kernel size 1', 'sentence': ' Another way of describing this is as two convolutions with kernel size 1 .'}\n",
            "{'subject': 'layer', 'relation': 'has', 'object': 'dimensionality dff 2048', 'sentence': ' The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .'}\n",
            "{'subject': 'inner layer', 'relation': 'has', 'object': 'dimensionality dff 2048', 'sentence': ' The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .'}\n",
            "{'subject': 'dimensionality', 'relation': 'is', 'object': 'dmodel 512', 'sentence': ' The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .'}\n",
            "{'subject': 'we', 'relation': 'convert input tokens to', 'object': 'vectors of dimension dmodel', 'sentence': ' 3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .'}\n",
            "{'subject': 'we', 'relation': 'convert output tokens to', 'object': 'vectors', 'sentence': ' 3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .'}\n",
            "{'subject': 'we', 'relation': 'convert output tokens to', 'object': 'vectors of dimension dmodel', 'sentence': ' 3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .'}\n",
            "{'subject': 'we', 'relation': 'convert', 'object': 'output tokens', 'sentence': ' 3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .'}\n",
            "{'subject': 'we', 'relation': 'use', 'object': 'learned embeddings', 'sentence': ' 3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .'}\n",
            "{'subject': 'we', 'relation': 'convert', 'object': 'input tokens', 'sentence': ' 3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .'}\n",
            "{'subject': 'we', 'relation': 'use', 'object': 'embeddings', 'sentence': ' 3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .'}\n",
            "{'subject': 'we', 'relation': 'convert input tokens to', 'object': 'vectors', 'sentence': ' 3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .'}\n",
            "{'subject': 'We', 'relation': 'use', 'object': 'the usual', 'sentence': ' We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .'}\n",
            "{'subject': 'We', 'relation': 'also use', 'object': 'the usual', 'sentence': ' We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .'}\n",
            "{'subject': 'We', 'relation': 'use', 'object': 'the', 'sentence': ' We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .'}\n",
            "{'subject': 'We', 'relation': 'also use', 'object': 'the', 'sentence': ' We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .'}\n",
            "{'subject': 'we', 'relation': 'share', 'object': 'same weight matrix between two embedding layers similar', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'share', 'object': 'weight matrix between two embedding layers similar', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'share', 'object': 'weight matrix similar', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'share', 'object': 'same weight matrix between two embedding layers similar to 24', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'share', 'object': 'weight matrix similar to 24', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'share', 'object': 'same weight matrix similar', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'share', 'object': 'same weight matrix', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'share weight matrix In', 'object': 'our model', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'share', 'object': 'same weight matrix between two embedding layers', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'share', 'object': 'same weight matrix similar to 24', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'share', 'object': 'weight matrix', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'share', 'object': 'weight matrix between two embedding layers', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'share', 'object': 'weight matrix between two embedding layers similar to 24', 'sentence': ' In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .'}\n",
            "{'subject': 'we', 'relation': 'multiply weights In', 'object': 'embedding layers', 'sentence': ' In the embedding layers , we multiply those weights by dmodel .'}\n",
            "{'subject': 'we', 'relation': 'multiply', 'object': 'weights', 'sentence': ' In the embedding layers , we multiply those weights by dmodel .'}\n",
            "{'subject': 'we', 'relation': 'multiply weights by', 'object': 'dmodel', 'sentence': ' In the embedding layers , we multiply those weights by dmodel .'}\n",
            "{'subject': 'Positional Encoding', 'relation': 'must inject', 'object': 'information about position in sequence', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': '3.5 Positional Encoding', 'relation': 'must inject', 'object': 'information about relative position in sequence', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'model', 'relation': 'make', 'object': 'use', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': '3.5 Positional Encoding', 'relation': 'must inject', 'object': 'information about relative position of tokens', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': '3.5 Positional Encoding', 'relation': 'must inject', 'object': 'information about position in sequence', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'Positional Encoding', 'relation': 'must inject', 'object': 'information about relative position in sequence', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'Positional Encoding', 'relation': 'must inject', 'object': 'information about relative position of tokens', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': '3.5 Positional Encoding', 'relation': 'must inject', 'object': 'information about relative position of tokens in sequence', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'model', 'relation': 'use of', 'object': 'order', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'Positional Encoding', 'relation': 'must inject', 'object': 'information', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'model', 'relation': 'use of', 'object': 'order of sequence', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'Positional Encoding', 'relation': 'must inject', 'object': 'information about relative position', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': '3.5 Positional Encoding', 'relation': 'must inject', 'object': 'information', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': '3.5 Positional Encoding', 'relation': 'must inject', 'object': 'information about position of tokens in sequence', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'Positional Encoding', 'relation': 'must inject', 'object': 'information about position of tokens in sequence', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': '3.5 Positional Encoding', 'relation': 'must inject', 'object': 'information about relative position', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'model', 'relation': 'make', 'object': 'use of order', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'model', 'relation': 'make', 'object': 'use of order of sequence', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'relative position', 'relation': 'is in', 'object': 'sequence', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'Positional Encoding', 'relation': 'must inject', 'object': 'information about position of tokens', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'Positional Encoding', 'relation': 'must inject', 'object': 'information about position', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': '3.5 Positional Encoding', 'relation': 'must inject', 'object': 'information about position of tokens', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'Positional Encoding', 'relation': 'must inject', 'object': 'information about relative position of tokens in sequence', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': '3.5 Positional Encoding', 'relation': 'must inject', 'object': 'information about position', 'sentence': ' 3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .'}\n",
            "{'subject': 'we', 'relation': 'add encodings to', 'object': 'input embeddings', 'sentence': ' To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .'}\n",
            "{'subject': 'we', 'relation': 'add encodings To', 'object': 'end', 'sentence': ' To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .'}\n",
            "{'subject': 'we', 'relation': 'add', 'object': 'encodings', 'sentence': ' To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .'}\n",
            "{'subject': 'we', 'relation': 'add encodings at', 'object': '5 Table 1', 'sentence': ' To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .'}\n",
            "{'subject': 'we', 'relation': 'add', 'object': 'positional encodings', 'sentence': ' To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .'}\n",
            "{'subject': 'k', 'relation': 'size in', 'object': 'restricted self attention', 'sentence': ' n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .'}\n",
            "{'subject': 'k', 'relation': 'size in', 'object': 'self attention', 'sentence': ' n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .'}\n",
            "{'subject': 'size', 'relation': 'is in', 'object': 'restricted self attention', 'sentence': ' n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .'}\n",
            "{'subject': 'k', 'relation': 'is', 'object': 'kernel size of convolutions', 'sentence': ' n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .'}\n",
            "{'subject': 'k', 'relation': 'size of', 'object': 'neighborhood', 'sentence': ' n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .'}\n",
            "{'subject': 'k', 'relation': 'is', 'object': 'kernel size', 'sentence': ' n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .'}\n",
            "{'subject': 'd', 'relation': 'is', 'object': 'representation dimension', 'sentence': ' n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .'}\n",
            "{'subject': 'n', 'relation': 'is', 'object': 'sequence length', 'sentence': ' n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .'}\n",
            "{'subject': 'encodings', 'relation': 'have', 'object': 'dimension dmodel', 'sentence': ' The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .'}\n",
            "{'subject': 'encodings', 'relation': 'have', 'object': 'same dimension dmodel as embeddings', 'sentence': ' The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .'}\n",
            "{'subject': 'encodings', 'relation': 'have', 'object': 'same dimension dmodel', 'sentence': ' The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .'}\n",
            "{'subject': 'positional encodings', 'relation': 'have', 'object': 'same dimension dmodel', 'sentence': ' The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .'}\n",
            "{'subject': 'two', 'relation': 'can', 'object': 'can summed', 'sentence': ' The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .'}\n",
            "{'subject': 'encodings', 'relation': 'have dimension dmodel', 'object': 'can summed', 'sentence': ' The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .'}\n",
            "{'subject': 'positional encodings', 'relation': 'have', 'object': 'same dimension dmodel as embeddings', 'sentence': ' The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .'}\n",
            "{'subject': 'positional encodings', 'relation': 'have', 'object': 'dimension dmodel', 'sentence': ' The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .'}\n",
            "{'subject': 'positional encodings', 'relation': 'have', 'object': 'dimension dmodel as embeddings', 'sentence': ' The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .'}\n",
            "{'subject': 'encodings', 'relation': 'have', 'object': 'dimension dmodel as embeddings', 'sentence': ' The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .'}\n",
            "{'subject': 'positional encodings', 'relation': 'have dimension dmodel', 'object': 'can summed', 'sentence': ' The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .'}\n",
            "{'subject': 'i', 'relation': 'is', 'object': 'dimension', 'sentence': ' In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .'}\n",
            "{'subject': 'we', 'relation': 'use', 'object': 'sine functions of frequencies', 'sentence': ' In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .'}\n",
            "{'subject': 'we', 'relation': 'use', 'object': 'sine functions of different frequencies', 'sentence': ' In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .'}\n",
            "{'subject': 'pos', 'relation': 'is', 'object': 'position', 'sentence': ' In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .'}\n",
            "{'subject': 'pos', 'relation': 'is', 'object': 'where position', 'sentence': ' In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .'}\n",
            "{'subject': 'we', 'relation': 'use sine functions In', 'object': 'work', 'sentence': ' In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .'}\n",
            "{'subject': 'we', 'relation': 'use', 'object': 'sine functions', 'sentence': ' In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .'}\n",
            "{'subject': 'wavelengths', 'relation': 'form', 'object': 'progression', 'sentence': ' The wavelengths form a geometric progression from 2 to 10000 2 .'}\n",
            "{'subject': 'wavelengths', 'relation': 'form', 'object': 'geometric progression', 'sentence': ' The wavelengths form a geometric progression from 2 to 10000 2 .'}\n",
            "{'subject': 'PEpos k', 'relation': 'can', 'object': 'since for fixed offset k can represented as function of PEpos', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'it', 'relation': 'allow', 'object': 'learn', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'model', 'relation': 'easily learn', 'object': 'attend', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'We', 'relation': 'chose', 'object': 'function', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'it', 'relation': 'allow', 'object': 'easily learn', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'PEpos k', 'relation': 'can', 'object': 'since for fixed offset k can represented as function', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'model', 'relation': 'learn', 'object': 'attend', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'it', 'relation': 'allow', 'object': 'model', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'PEpos k', 'relation': 'can', 'object': 'since for fixed offset k can represented as linear function of PEpos', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'PEpos k', 'relation': 'can', 'object': 'since for fixed offset k can represented as linear function', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'model', 'relation': 'learn', 'object': 'attend by relative positions', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'model', 'relation': 'learn', 'object': 'attend by positions', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'model', 'relation': 'easily learn', 'object': 'attend by positions', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'PEpos k', 'relation': 'can', 'object': 'since for fixed offset k can represented', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'We', 'relation': 'chose function', 'object': 'we hypothesized', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'model', 'relation': 'easily learn', 'object': 'attend by relative positions', 'sentence': ' We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .'}\n",
            "{'subject': 'results', 'relation': 'see', 'object': 'Table 3 row E', 'sentence': ' We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .'}\n",
            "{'subject': 'We', 'relation': 'also experimented', 'object': 'using', 'sentence': ' We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .'}\n",
            "{'subject': 'We', 'relation': 'experimented', 'object': 'using', 'sentence': ' We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .'}\n",
            "{'subject': 'identical results', 'relation': 'see', 'object': 'Table 3 row E', 'sentence': ' We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .'}\n",
            "{'subject': 'model', 'relation': 'extrapolate to', 'object': 'sequence lengths longer than ones', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'We', 'relation': 'chose', 'object': 'sinusoidal version', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'model', 'relation': 'extrapolate to', 'object': 'sequence lengths longer than ones encountered', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'it', 'relation': 'may allow', 'object': 'extrapolate to sequence lengths longer', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'it', 'relation': 'may allow', 'object': 'extrapolate to sequence lengths', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'it', 'relation': 'may allow', 'object': 'extrapolate', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'it', 'relation': 'may allow', 'object': 'model', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'it', 'relation': 'may allow', 'object': 'extrapolate to sequence lengths longer than ones', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'We', 'relation': 'chose', 'object': 'version', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'model', 'relation': 'extrapolate to', 'object': 'sequence lengths longer', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'We', 'relation': 'chose version', 'object': 'it may allow', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'it', 'relation': 'may allow', 'object': 'extrapolate to sequence lengths longer than ones encountered during training', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'model', 'relation': 'extrapolate to', 'object': 'sequence lengths', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'model', 'relation': 'extrapolate to', 'object': 'sequence lengths longer than ones encountered during training', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'it', 'relation': 'may allow', 'object': 'extrapolate to sequence lengths longer than ones encountered', 'sentence': ' We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .'}\n",
            "{'subject': 'we', 'relation': 'compare', 'object': 'aspects of self attention layers', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'we', 'relation': 'compare aspects to', 'object': 'recurrent layers used', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': '4 Self Attention', 'relation': 'zn with', 'object': 'xi', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'we', 'relation': 'compare aspects to', 'object': 'layers', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'we', 'relation': 'compare aspects to', 'object': 'layers used', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'we', 'relation': 'compare', 'object': 'various aspects of self attention layers', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'section', 'relation': 'In Attention is', 'object': 'zn with xi such hidden layer', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'we', 'relation': 'compare aspects to', 'object': 'recurrent layers commonly used', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'hidden layer', 'relation': 'is in', 'object': 'typical sequence transduction encoder', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'section', 'relation': 'In Attention is', 'object': 'xn to sequence of equal length z1', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'we', 'relation': 'compare', 'object': 'aspects', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': '4 Self Attention', 'relation': 'zn such as', 'object': 'hidden layer in typical sequence transduction encoder', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'we', 'relation': 'compare', 'object': 'various aspects', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'hidden layer', 'relation': 'such as zn is', 'object': 'zi Rd', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': '4 Self Attention', 'relation': 'is In', 'object': 'section', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'we', 'relation': 'various aspects of', 'object': 'self attention layers', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'zn', 'relation': 'is with', 'object': 'xi', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'we', 'relation': 'compare aspects to', 'object': 'recurrent layers', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': '4 Self Attention', 'relation': 'xn to', 'object': 'sequence of equal length z1', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'we', 'relation': 'aspects of', 'object': 'self attention layers', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'we', 'relation': 'compare aspects to', 'object': 'layers commonly used', 'sentence': ' 4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .'}\n",
            "{'subject': 'we', 'relation': 'Motivating', 'object': 'our use of self attention', 'sentence': ' Motivating our use of self attention we consider three desiderata .'}\n",
            "{'subject': 'we', 'relation': 'Motivating', 'object': 'our use', 'sentence': ' Motivating our use of self attention we consider three desiderata .'}\n",
            "{'subject': 'we', 'relation': 'consider', 'object': 'three desiderata', 'sentence': ' Motivating our use of self attention we consider three desiderata .'}\n",
            "{'subject': 'One', 'relation': 'is', 'object': 'computational complexity', 'sentence': ' One is the total computational complexity per layer .'}\n",
            "{'subject': 'One', 'relation': 'is total computational complexity per', 'object': 'layer', 'sentence': ' One is the total computational complexity per layer .'}\n",
            "{'subject': 'One', 'relation': 'is total complexity per', 'object': 'layer', 'sentence': ' One is the total computational complexity per layer .'}\n",
            "{'subject': 'One', 'relation': 'is complexity per', 'object': 'layer', 'sentence': ' One is the total computational complexity per layer .'}\n",
            "{'subject': 'One', 'relation': 'is', 'object': 'total complexity', 'sentence': ' One is the total computational complexity per layer .'}\n",
            "{'subject': 'One', 'relation': 'is', 'object': 'total computational complexity', 'sentence': ' One is the total computational complexity per layer .'}\n",
            "{'subject': 'One', 'relation': 'is computational complexity per', 'object': 'layer', 'sentence': ' One is the total computational complexity per layer .'}\n",
            "{'subject': 'One', 'relation': 'is', 'object': 'complexity', 'sentence': ' One is the total computational complexity per layer .'}\n",
            "{'subject': 'Another', 'relation': 'is', 'object': 'amount', 'sentence': ' Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .'}\n",
            "{'subject': 'Another', 'relation': 'is amount of', 'object': 'computation', 'sentence': ' Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .'}\n",
            "{'subject': 'Another', 'relation': 'is amount', 'object': 'can parallelized', 'sentence': ' Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .'}\n",
            "{'subject': 'third', 'relation': 'is', 'object': 'path length', 'sentence': ' The third is the path length between long range dependencies in the network .'}\n",
            "{'subject': 'long range dependencies', 'relation': 'is in', 'object': 'network', 'sentence': ' The third is the path length between long range dependencies in the network .'}\n",
            "{'subject': 'third', 'relation': 'is', 'object': 'path length between range dependencies in network', 'sentence': ' The third is the path length between long range dependencies in the network .'}\n",
            "{'subject': 'third', 'relation': 'is', 'object': 'path length between long range dependencies', 'sentence': ' The third is the path length between long range dependencies in the network .'}\n",
            "{'subject': 'third', 'relation': 'is', 'object': 'path length between range dependencies', 'sentence': ' The third is the path length between long range dependencies in the network .'}\n",
            "{'subject': 'third', 'relation': 'is', 'object': 'path length between long range dependencies in network', 'sentence': ' The third is the path length between long range dependencies in the network .'}\n",
            "{'subject': 'dependencies', 'relation': 'is key challenge in', 'object': 'sequence transduction tasks', 'sentence': ' Learning long range dependencies is a key challenge in many sequence transduction tasks .'}\n",
            "{'subject': 'key challenge', 'relation': 'is in', 'object': 'many sequence transduction tasks', 'sentence': ' Learning long range dependencies is a key challenge in many sequence transduction tasks .'}\n",
            "{'subject': 'dependencies', 'relation': 'is key challenge in', 'object': 'many sequence transduction tasks', 'sentence': ' Learning long range dependencies is a key challenge in many sequence transduction tasks .'}\n",
            "{'subject': 'dependencies', 'relation': 'is challenge in', 'object': 'many sequence transduction tasks', 'sentence': ' Learning long range dependencies is a key challenge in many sequence transduction tasks .'}\n",
            "{'subject': 'dependencies', 'relation': 'is', 'object': 'key', 'sentence': ' Learning long range dependencies is a key challenge in many sequence transduction tasks .'}\n",
            "{'subject': 'dependencies', 'relation': 'is challenge in', 'object': 'sequence transduction tasks', 'sentence': ' Learning long range dependencies is a key challenge in many sequence transduction tasks .'}\n",
            "{'subject': 'signals', 'relation': 'traverse in', 'object': 'network', 'sentence': ' One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .'}\n",
            "{'subject': 'signals', 'relation': 'have', 'object': 'traverse', 'sentence': ' One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .'}\n",
            "{'subject': 'forward signals', 'relation': 'traverse in', 'object': 'network', 'sentence': ' One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .'}\n",
            "{'subject': 'forward signals', 'relation': 'have', 'object': 'traverse in network', 'sentence': ' One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .'}\n",
            "{'subject': 'signals', 'relation': 'have', 'object': 'traverse in network', 'sentence': ' One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .'}\n",
            "{'subject': 'forward signals', 'relation': 'have', 'object': 'traverse', 'sentence': ' One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .'}\n",
            "{'subject': 'it', 'relation': 'learn', 'object': 'long range dependencies 11', 'sentence': ' The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long range dependencies 11 .'}\n",
            "{'subject': 'it', 'relation': 'learn', 'object': 'range dependencies 11', 'sentence': ' The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long range dependencies 11 .'}\n",
            "{'subject': 'self attention layer', 'relation': 'connects positions with', 'object': 'constant number of sequentially executed operations', 'sentence': ' As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .'}\n",
            "{'subject': 'self attention layer', 'relation': 'connects positions with', 'object': 'number of executed operations', 'sentence': ' As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .'}\n",
            "{'subject': 'self attention layer', 'relation': 'connects positions with', 'object': 'constant number', 'sentence': ' As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .'}\n",
            "{'subject': 'self attention layer', 'relation': 'connects positions with', 'object': 'number of operations', 'sentence': ' As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .'}\n",
            "{'subject': 'self attention layer', 'relation': 'connects positions with', 'object': 'number', 'sentence': ' As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .'}\n",
            "{'subject': 'self attention layer', 'relation': 'connects', 'object': 'positions', 'sentence': ' As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .'}\n",
            "{'subject': 'self attention layer', 'relation': 'connects positions with', 'object': 'constant number of operations', 'sentence': ' As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .'}\n",
            "{'subject': 'self attention layer', 'relation': 'connects positions with', 'object': 'constant number of executed operations', 'sentence': ' As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .'}\n",
            "{'subject': 'self attention layer', 'relation': 'connects positions with', 'object': 'number of sequentially executed operations', 'sentence': ' As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .'}\n",
            "{'subject': 'self attention layers', 'relation': 'are faster than', 'object': 'recurrent layers', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'self attention layers', 'relation': 'are', 'object': 'In terms faster than layers', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'self attention layers', 'relation': 'are', 'object': 'In terms of computational complexity faster than layers', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'case', 'relation': 'is with', 'object': 'sentence representations used by state of art models in machine translations such word piece 31', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'sequence length n', 'relation': 'is', 'object': 'when smaller', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'self attention layers', 'relation': 'are faster In', 'object': 'terms of complexity', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'self attention layers', 'relation': 'are', 'object': 'In terms faster than recurrent layers', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'self attention layers', 'relation': 'are', 'object': 'In terms of computational complexity faster than recurrent layers', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'self attention layers', 'relation': 'are', 'object': 'faster', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'sequence length n', 'relation': 'is smaller than', 'object': 'representation dimensionality d', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'self attention layers', 'relation': 'are faster In', 'object': 'terms', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'self attention layers', 'relation': 'are faster In', 'object': 'terms of computational complexity', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'sequence length n', 'relation': 'is', 'object': 'smaller', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'self attention layers', 'relation': 'are', 'object': 'In terms of complexity faster than layers', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'sequence length n', 'relation': 'is', 'object': 'when smaller than representation dimensionality d', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'self attention layers', 'relation': 'are faster than', 'object': 'layers', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'self attention layers', 'relation': 'are', 'object': 'In terms of complexity faster than recurrent layers', 'sentence': ' In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .'}\n",
            "{'subject': 'self attention', 'relation': 'improve', 'object': 'performance for tasks', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'self attention', 'relation': 'considering', 'object': 'neighborhood', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'self attention', 'relation': 'considering', 'object': 'neighborhood of size r', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'self attention', 'relation': 'considering neighborhood in', 'object': '6 input sequence centered around respective output position', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'self attention', 'relation': 'considering', 'object': 'only neighborhood', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'self attention', 'relation': 'considering neighborhood in', 'object': '6 input sequence', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'self attention', 'relation': 'could', 'object': 'could restricted', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'self attention', 'relation': 'improve', 'object': 'computational performance for tasks', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'self attention', 'relation': 'improve', 'object': 'performance', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'self attention', 'relation': 'considering', 'object': 'only neighborhood of size r', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'self attention', 'relation': 'improve', 'object': 'computational performance', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'self attention', 'relation': 'considering neighborhood in', 'object': '6 input sequence centered', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'self attention', 'relation': 'considering neighborhood in', 'object': '6 input sequence centered around output position', 'sentence': ' To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .'}\n",
            "{'subject': 'We', 'relation': 'investigate', 'object': 'approach', 'sentence': ' We plan to investigate this approach further in future work .'}\n",
            "{'subject': 'We', 'relation': 'investigate approach further in', 'object': 'future work', 'sentence': ' We plan to investigate this approach further in future work .'}\n",
            "{'subject': 'We', 'relation': 'investigate further', 'object': 'approach', 'sentence': ' We plan to investigate this approach further in future work .'}\n",
            "{'subject': 'We', 'relation': 'investigate approach in', 'object': 'future work', 'sentence': ' We plan to investigate this approach further in future work .'}\n",
            "{'subject': 'single convolutional layer', 'relation': 'is with', 'object': 'kernel width k n', 'sentence': ' A single convolutional layer with kernel width k n does not connect all pairs of input and output positions .'}\n",
            "{'subject': 'stack', 'relation': 'increasing', 'object': 'length', 'sentence': ' Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .'}\n",
            "{'subject': 'increasing', 'relation': 'length of', 'object': 'longest paths between two positions in network', 'sentence': ' Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .'}\n",
            "{'subject': 'stack', 'relation': 'increasing', 'object': 'length of longest paths between two positions in network', 'sentence': ' Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .'}\n",
            "{'subject': 'case', 'relation': 'is in', 'object': 'case of dilated convolutions 15', 'sentence': ' Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .'}\n",
            "{'subject': 'Doing', 'relation': 'requires', 'object': 'stack of O n k convolutional layers', 'sentence': ' Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .'}\n",
            "{'subject': 'increasing', 'relation': 'length of', 'object': 'paths between two positions in network', 'sentence': ' Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .'}\n",
            "{'subject': 'Doing', 'relation': 'requires', 'object': 'stack of O n k layers', 'sentence': ' Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .'}\n",
            "{'subject': 'stack', 'relation': 'increasing', 'object': 'length of paths between two positions in network', 'sentence': ' Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .'}\n",
            "{'subject': 'Doing', 'relation': 'requires', 'object': 'stack', 'sentence': ' Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive than recurrent layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally expensive however than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive than recurrent layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally expensive however by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally more expensive than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive than recurrent layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive than recurrent layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally more expensive however by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive than layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally more expensive however by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive than layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive than layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are more expensive however by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally expensive by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive than layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally expensive however by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive than layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive than recurrent layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are expensive however by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive than recurrent layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally expensive however by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive than layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are expensive than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive than recurrent layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive than recurrent layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive than layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive than recurrent layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are expensive however than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally more expensive by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally expensive however than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive than recurrent layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive than layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive than recurrent layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive than layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally expensive than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive than recurrent layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally expensive than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive than layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally more expensive than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive than layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally expensive than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are expensive however by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive than recurrent layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive than layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally expensive however by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally more expensive by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive than recurrent layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive than recurrent layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally more expensive than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive than layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive than recurrent layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are expensive however than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are expensive by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive than layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally expensive than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are expensive by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive than layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive than recurrent layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive than layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are more expensive however by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are more expensive however by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive than layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are more expensive than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally expensive however than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally more expensive however by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive than recurrent layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally more expensive than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive than layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive than recurrent layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are expensive however by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive than recurrent layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive than recurrent layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive than layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally more expensive however than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive than layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are expensive by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive than layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally more expensive however by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally more expensive however by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are expensive than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are more expensive however than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are more expensive by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive than layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive than layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive than layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive than layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally expensive by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive than recurrent layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are expensive than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive than layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are expensive than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive than recurrent layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are expensive however by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive than recurrent layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive than layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive than recurrent layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are more expensive than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive than layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive than layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are more expensive than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally expensive by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive than layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive than recurrent layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally more expensive by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are expensive by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive than recurrent layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive than layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally expensive by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally more expensive however than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive than recurrent layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally more expensive however by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive than recurrent layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally expensive by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive than layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive than recurrent layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are expensive however by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive than layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are more expensive than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive than layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally expensive however than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally expensive however by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are more expensive by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive than layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are expensive however than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally expensive by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally more expensive by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally more expensive however than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive than layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally expensive however by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive than layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive than recurrent layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are more expensive by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive than recurrent layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally more expensive by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive than recurrent layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive than recurrent layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive than layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are more expensive however by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive than layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally more expensive than layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive than recurrent layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive than layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive than recurrent layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive than recurrent layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are expensive however by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are more expensive however than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive than recurrent layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive than layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive than layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive than recurrent layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are more expensive by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive than recurrent layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are more expensive by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are more expensive however by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive than layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are expensive by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive than recurrent layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are more expensive however than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are more expensive by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive than recurrent layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive than layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are generally more expensive by', 'object': 'factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are expensive by', 'object': 'factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive than recurrent layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'more expensive than recurrent layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are more expensive however by', 'object': 'factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are more expensive however than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive than recurrent layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally expensive than layers by factor', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'expensive than recurrent layers by factor of k. convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive than recurrent layers by factor of k. Separable convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'generally expensive than recurrent layers by factor however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'generally more expensive than layers by factor of k. convolutions 6', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are expensive however than', 'object': 'recurrent layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are generally more expensive however than', 'object': 'layers', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'Convolutional layers', 'relation': 'are', 'object': 'more expensive however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'layers', 'relation': 'are', 'object': 'expensive than layers by factor of k. Separable convolutions 6 however', 'sentence': ' Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .'}\n",
            "{'subject': 'complexity', 'relation': 'is equal to', 'object': 'combination of self attention layer', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is', 'object': 'Even with k n however equal to combination', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is', 'object': 'equal', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is', 'object': 'Even with k n equal to combination of self attention layer', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is', 'object': 'with k n equal to combination', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is', 'object': 'with k n however equal to combination of self attention layer', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is', 'object': 'Even with k n however equal to combination of self attention layer', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is', 'object': 'with k n equal to combination of self attention layer', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is equal with', 'object': 'k n', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is equal with', 'object': 'Even k n', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is', 'object': 'with k n however equal to combination', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'however is equal with', 'object': 'Even k n', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'however is equal to', 'object': 'combination of self attention layer', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is', 'object': 'however equal', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is', 'object': 'Even with k n equal to combination', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'is equal to', 'object': 'combination', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'however is equal with', 'object': 'k n', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'complexity', 'relation': 'however is equal to', 'object': 'combination', 'sentence': ' Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .'}\n",
            "{'subject': 'self attention', 'relation': 'could yield', 'object': 'more interpretable models', 'sentence': ' As side benefit , self attention could yield more interpretable models .'}\n",
            "{'subject': 'self attention', 'relation': 'could yield', 'object': 'interpretable models', 'sentence': ' As side benefit , self attention could yield more interpretable models .'}\n",
            "{'subject': 'We', 'relation': 'inspect', 'object': 'attention distributions', 'sentence': ' We inspect attention distributions from our models and present and discuss examples in the appendix .'}\n",
            "{'subject': 'We', 'relation': 'inspect attention distributions from', 'object': 'our models', 'sentence': ' We inspect attention distributions from our models and present and discuss examples in the appendix .'}\n",
            "{'subject': 'examples', 'relation': 'is in', 'object': 'appendix', 'sentence': ' We inspect attention distributions from our models and present and discuss examples in the appendix .'}\n",
            "{'subject': 'We', 'relation': 'discuss', 'object': 'examples', 'sentence': ' We inspect attention distributions from our models and present and discuss examples in the appendix .'}\n",
            "{'subject': 'We', 'relation': 'discuss', 'object': 'examples in appendix', 'sentence': ' We inspect attention distributions from our models and present and discuss examples in the appendix .'}\n",
            "{'subject': 'many', 'relation': 'exhibit', 'object': 'behavior', 'sentence': ' Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .'}\n",
            "{'subject': 'behavior', 'relation': 'related to', 'object': 'structure', 'sentence': ' Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .'}\n",
            "{'subject': 'many', 'relation': 'exhibit', 'object': 'behavior related to syntactic structure', 'sentence': ' Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .'}\n",
            "{'subject': 'many', 'relation': 'exhibit', 'object': 'behavior related to structure', 'sentence': ' Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .'}\n",
            "{'subject': 'many', 'relation': 'exhibit', 'object': 'behavior related', 'sentence': ' Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .'}\n",
            "{'subject': 'behavior', 'relation': 'related to', 'object': 'syntactic structure of sentences', 'sentence': ' Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .'}\n",
            "{'subject': 'many', 'relation': 'exhibit', 'object': 'behavior related to syntactic structure of sentences', 'sentence': ' Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .'}\n",
            "{'subject': 'behavior', 'relation': 'related to', 'object': 'structure of sentences', 'sentence': ' Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .'}\n",
            "{'subject': 'behavior', 'relation': 'related to', 'object': 'syntactic structure', 'sentence': ' Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .'}\n",
            "{'subject': 'many', 'relation': 'exhibit', 'object': 'behavior related to structure of sentences', 'sentence': ' Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'standard WMT 2014 English dataset consisting of about 4.5 million sentence pairs', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'WMT 2014 English German dataset consisting of about 4.5 million sentence pairs', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'standard WMT 2014 German dataset', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'standard WMT 2014 English dataset consisting', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'WMT 2014 English dataset consisting', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'standard WMT 2014 dataset consisting', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'WMT 2014 English dataset consisting of about 4.5 million sentence pairs', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'standard WMT 2014 dataset consisting of about 4.5 million sentence pairs', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'WMT 2014 English German dataset consisting', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'standard WMT 2014 German dataset consisting', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'WMT 2014 English dataset', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'WMT 2014 dataset consisting', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'WMT 2014 dataset consisting of about 4.5 million sentence pairs', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'WMT 2014 German dataset', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'WMT 2014 English German dataset', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'WMT 2014 dataset', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'WMT 2014 German dataset consisting of about 4.5 million sentence pairs', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'WMT 2014 German dataset consisting', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'standard WMT 2014 English dataset', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'standard WMT 2014 German dataset consisting of about 4.5 million sentence pairs', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'standard WMT 2014 dataset', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'standard WMT 2014 English German dataset consisting', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'We', 'relation': 'trained on', 'object': 'standard WMT 2014 English German dataset', 'sentence': ' 5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .'}\n",
            "{'subject': 'Sentences', 'relation': 'using', 'object': 'byte pair', 'sentence': ' Sentences were encoded using byte pair encoding 3 , which has a shared sourcetarget vocabulary of about 37000 tokens .'}\n",
            "{'subject': 'Sentences', 'relation': 'were', 'object': 'encoded', 'sentence': ' Sentences were encoded using byte pair encoding 3 , which has a shared sourcetarget vocabulary of about 37000 tokens .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'larger WMT 2014 English dataset consisting', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used For', 'object': 'French', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'significantly larger WMT 2014 English dataset', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'WMT 2014 English dataset', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'larger WMT 2014 English French dataset consisting', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'significantly larger WMT 2014 English dataset consisting of 36M sentences', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'larger WMT 2014 English dataset consisting of 36M sentences', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'WMT 2014 English dataset consisting', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'larger WMT 2014 English dataset', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'significantly larger WMT 2014 English dataset consisting', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'larger WMT 2014 English French dataset', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'WMT 2014 English French dataset', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'significantly larger WMT 2014 English French dataset consisting', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'WMT 2014 English French dataset consisting', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'WMT 2014 English French dataset consisting of 36M sentences', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used For', 'object': 'English French', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'larger WMT 2014 English French dataset consisting of 36M sentences', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'significantly larger WMT 2014 English French dataset consisting of 36M sentences', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used into', 'object': '32000 word piece vocabulary 31', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'WMT 2014 English dataset consisting of 36M sentences', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'significantly larger WMT 2014 English French dataset', 'sentence': ' For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .'}\n",
            "{'subject': 'Sentence pairs', 'relation': 'were batched together by', 'object': 'approximate sequence length', 'sentence': ' Sentence pairs were batched together by approximate sequence length .'}\n",
            "{'subject': 'Sentence pairs', 'relation': 'were batched by', 'object': 'approximate sequence length', 'sentence': ' Sentence pairs were batched together by approximate sequence length .'}\n",
            "{'subject': 'Sentence pairs', 'relation': 'were batched by', 'object': 'sequence length', 'sentence': ' Sentence pairs were batched together by approximate sequence length .'}\n",
            "{'subject': 'Sentence pairs', 'relation': 'were', 'object': 'batched together', 'sentence': ' Sentence pairs were batched together by approximate sequence length .'}\n",
            "{'subject': 'Sentence pairs', 'relation': 'were batched together by', 'object': 'sequence length', 'sentence': ' Sentence pairs were batched together by approximate sequence length .'}\n",
            "{'subject': 'Sentence pairs', 'relation': 'were', 'object': 'batched', 'sentence': ' Sentence pairs were batched together by approximate sequence length .'}\n",
            "{'subject': 'training batch', 'relation': 'contained', 'object': 'set', 'sentence': ' Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .'}\n",
            "{'subject': 'training batch', 'relation': 'contained', 'object': 'set of sentence pairs', 'sentence': ' Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .'}\n",
            "{'subject': 'We', 'relation': 'trained', 'object': 'our models', 'sentence': ' 5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs .'}\n",
            "{'subject': 'training step', 'relation': 'took', 'object': 'about 0.4 seconds', 'sentence': ' For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .'}\n",
            "{'subject': 'our base models', 'relation': 'described throughout', 'object': 'paper', 'sentence': ' For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .'}\n",
            "{'subject': 'our base models', 'relation': 'using', 'object': 'hyperparameters', 'sentence': ' For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .'}\n",
            "{'subject': 'We', 'relation': 'trained', 'object': 'base models', 'sentence': ' We trained the base models for a total of 100,000 steps or 12 hours .'}\n",
            "{'subject': 'step time', 'relation': 'described on', 'object': 'line', 'sentence': ' For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .'}\n",
            "{'subject': 'step time', 'relation': 'was', 'object': '1.0 seconds', 'sentence': ' For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .'}\n",
            "{'subject': 'step time', 'relation': 'described on', 'object': 'bottom line', 'sentence': ' For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .'}\n",
            "{'subject': 'step time', 'relation': 'described on', 'object': 'bottom line of table 3', 'sentence': ' For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .'}\n",
            "{'subject': 'step time', 'relation': 'was', 'object': 'For our models 1.0 seconds', 'sentence': ' For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .'}\n",
            "{'subject': 'step time', 'relation': 'described on', 'object': 'line of table 3', 'sentence': ' For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .'}\n",
            "{'subject': 'step time', 'relation': 'was', 'object': 'For our big models 1.0 seconds', 'sentence': ' For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .'}\n",
            "{'subject': 'big models', 'relation': 'were', 'object': 'trained', 'sentence': ' The big models were trained for 300,000 steps 3.5 days .'}\n",
            "{'subject': 'big models', 'relation': 'were trained for', 'object': '300,000 steps', 'sentence': ' The big models were trained for 300,000 steps 3.5 days .'}\n",
            "{'subject': 'models', 'relation': 'were trained for', 'object': '300,000 steps 3.5 days', 'sentence': ' The big models were trained for 300,000 steps 3.5 days .'}\n",
            "{'subject': 'models', 'relation': 'were trained for', 'object': '300,000 steps', 'sentence': ' The big models were trained for 300,000 steps 3.5 days .'}\n",
            "{'subject': 'big models', 'relation': 'were trained for', 'object': '300,000 steps 3.5 days', 'sentence': ' The big models were trained for 300,000 steps 3.5 days .'}\n",
            "{'subject': 'models', 'relation': 'were', 'object': 'trained', 'sentence': ' The big models were trained for 300,000 steps 3.5 days .'}\n",
            "{'subject': 'We', 'relation': 'used', 'object': 'Adam optimizer 17', 'sentence': ' 5.3 Optimizer We used the Adam optimizer 17 with 1 0.9 , 2 0.98 and 10 9 .'}\n",
            "{'subject': 'first warmup steps', 'relation': 'training', 'object': 'steps', 'sentence': ' We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .'}\n",
            "{'subject': 'warmup steps', 'relation': 'training', 'object': 'steps', 'sentence': ' We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .'}\n",
            "{'subject': 'We', 'relation': 'varied', 'object': 'learning rate', 'sentence': ' We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .'}\n",
            "{'subject': 'learning rate', 'relation': 'decreasing thereafter proportionally', 'object': 'it', 'sentence': ' We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .'}\n",
            "{'subject': 'learning rate', 'relation': 'decreasing proportionally', 'object': 'it', 'sentence': ' We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .'}\n",
            "{'subject': 'learning rate', 'relation': 'decreasing', 'object': 'it', 'sentence': ' We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .'}\n",
            "{'subject': 'We', 'relation': 'used', 'object': 'warmup steps 4000', 'sentence': ' We used warmup steps 4000 .'}\n",
            "{'subject': 'We', 'relation': 'employ', 'object': 'three types of regularization', 'sentence': ' 5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .'}\n",
            "{'subject': 'We', 'relation': 'employ', 'object': 'three types', 'sentence': ' 5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .'}\n",
            "{'subject': 'We', 'relation': 'apply', 'object': 'dropout 27', 'sentence': ' 5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .'}\n",
            "{'subject': 'sums', 'relation': 'is in', 'object': 'encoder', 'sentence': ' In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .'}\n",
            "{'subject': 'we', 'relation': 'apply dropout In', 'object': 'addition', 'sentence': ' In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .'}\n",
            "{'subject': 'we', 'relation': 'apply', 'object': 'dropout', 'sentence': ' In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .'}\n",
            "{'subject': 'we', 'relation': 'apply dropout to', 'object': 'sums', 'sentence': ' In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .'}\n",
            "{'subject': 'we', 'relation': 'apply dropout to', 'object': 'sums of embeddings', 'sentence': ' In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .'}\n",
            "{'subject': 'we', 'relation': 'apply dropout to', 'object': 'sums of embeddings in encoder', 'sentence': ' In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .'}\n",
            "{'subject': 'we', 'relation': 'apply dropout to', 'object': 'sums in encoder', 'sentence': ' In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .'}\n",
            "{'subject': 'we', 'relation': 'use', 'object': 'rate of Pdrop 0.1', 'sentence': ' For the base model , we use a rate of Pdrop 0.1 .'}\n",
            "{'subject': 'we', 'relation': 'use', 'object': 'rate', 'sentence': ' For the base model , we use a rate of Pdrop 0.1 .'}\n",
            "{'subject': 'we', 'relation': 'use rate For', 'object': 'base model', 'sentence': ' For the base model , we use a rate of Pdrop 0.1 .'}\n",
            "{'subject': 'we', 'relation': 'employed', 'object': 'label smoothing of value ls 0.1 30', 'sentence': ' Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .'}\n",
            "{'subject': 'Model BLEU', 'relation': 'Training', 'object': 'Cost FLOPs EN', 'sentence': ' Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .'}\n",
            "{'subject': 'Model BLEU', 'relation': 'Training', 'object': 'Cost FLOPs', 'sentence': ' Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .'}\n",
            "{'subject': 'we', 'relation': 'employed', 'object': 'label smoothing', 'sentence': ' Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .'}\n",
            "{'subject': 'model', 'relation': 'learns', 'object': 'unsure', 'sentence': ' This hurts perplexity , as the model learns to be more unsure , but improves accuracy and BLEU score .'}\n",
            "{'subject': 'model', 'relation': 'learns', 'object': 'more unsure', 'sentence': ' This hurts perplexity , as the model learns to be more unsure , but improves accuracy and BLEU score .'}\n",
            "{'subject': 'German translation task', 'relation': 'to Translation is', 'object': 'big transformer model Transformer big in Table 2', 'sentence': ' 6 Results 6.1 Machine Translation On the WMT 2014 English to German translation task , the big transformer model Transformer big in Table 2 outperforms the best previously reported models including ensembles by more than 2.0 BLEU , establishing a new state of the art BLEU score of 28.4 .'}\n",
            "{'subject': 'WMT 2014 English', 'relation': 'On Translation is', 'object': 'big transformer model Transformer big in Table 2', 'sentence': ' 6 Results 6.1 Machine Translation On the WMT 2014 English to German translation task , the big transformer model Transformer big in Table 2 outperforms the best previously reported models including ensembles by more than 2.0 BLEU , establishing a new state of the art BLEU score of 28.4 .'}\n",
            "{'subject': 'configuration', 'relation': 'is', 'object': 'listed', 'sentence': ' The configuration of this model is listed in the bottom line of Table 3 .'}\n",
            "{'subject': 'configuration', 'relation': 'is listed in', 'object': 'line of Table 3', 'sentence': ' The configuration of this model is listed in the bottom line of Table 3 .'}\n",
            "{'subject': 'configuration', 'relation': 'is listed in', 'object': 'bottom line of Table 3', 'sentence': ' The configuration of this model is listed in the bottom line of Table 3 .'}\n",
            "{'subject': 'configuration', 'relation': 'is listed in', 'object': 'bottom line', 'sentence': ' The configuration of this model is listed in the bottom line of Table 3 .'}\n",
            "{'subject': 'configuration', 'relation': 'is listed in', 'object': 'line', 'sentence': ' The configuration of this model is listed in the bottom line of Table 3 .'}\n",
            "{'subject': 'Training', 'relation': 'took on', 'object': '8 P100 GPUs', 'sentence': ' Training took 3.5 days on 8 P100 GPUs .'}\n",
            "{'subject': 'Training', 'relation': 'took at_time', 'object': '3.5 days', 'sentence': ' Training took 3.5 days on 8 P100 GPUs .'}\n",
            "{'subject': 'our big model', 'relation': 'achieves BLEU score On', 'object': 'WMT 2014 English', 'sentence': ' On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .'}\n",
            "{'subject': 'our big model', 'relation': 'achieves BLEU score On', 'object': 'WMT 2014 English to translation task', 'sentence': ' On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .'}\n",
            "{'subject': 'our big model', 'relation': 'achieves', 'object': 'BLEU score', 'sentence': ' On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .'}\n",
            "{'subject': 'our model', 'relation': 'achieves BLEU score On', 'object': 'WMT 2014 English to French translation task', 'sentence': ' On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .'}\n",
            "{'subject': 'our model', 'relation': 'achieves', 'object': 'BLEU score', 'sentence': ' On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .'}\n",
            "{'subject': 'our big model', 'relation': 'achieves', 'object': 'BLEU score of 41.0', 'sentence': ' On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .'}\n",
            "{'subject': 'our model', 'relation': 'achieves BLEU score On', 'object': 'WMT 2014 English', 'sentence': ' On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .'}\n",
            "{'subject': 'our model', 'relation': 'achieves', 'object': 'BLEU score of 41.0', 'sentence': ' On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .'}\n",
            "{'subject': 'our big model', 'relation': 'achieves BLEU score On', 'object': 'WMT 2014 English to French translation task', 'sentence': ' On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .'}\n",
            "{'subject': 'our model', 'relation': 'achieves BLEU score On', 'object': 'WMT 2014 English to translation task', 'sentence': ' On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .'}\n",
            "{'subject': 'we', 'relation': 'used model For', 'object': 'base models', 'sentence': ' For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'single model', 'sentence': ' For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .'}\n",
            "{'subject': 'we', 'relation': 'used For', 'object': 'base models', 'sentence': ' For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'model obtained', 'sentence': ' For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'model', 'sentence': ' For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .'}\n",
            "{'subject': 'we', 'relation': 'used', 'object': 'single model obtained', 'sentence': ' For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .'}\n",
            "{'subject': 'we', 'relation': 'averaged', 'object': 'last 20 checkpoints', 'sentence': ' For the big models , we averaged the last 20 checkpoints .'}\n",
            "{'subject': 'we', 'relation': 'averaged', 'object': '20 checkpoints', 'sentence': ' For the big models , we averaged the last 20 checkpoints .'}\n",
            "{'subject': 'We', 'relation': 'used with', 'object': 'beam size', 'sentence': ' We used beam search with a beam size of 4 and length penalty 0.6 31 .'}\n",
            "{'subject': 'We', 'relation': 'used', 'object': 'beam search', 'sentence': ' We used beam search with a beam size of 4 and length penalty 0.6 31 .'}\n",
            "{'subject': 'We', 'relation': 'used beam search with', 'object': 'beam size', 'sentence': ' We used beam search with a beam size of 4 and length penalty 0.6 31 .'}\n",
            "{'subject': 'hyperparameters', 'relation': 'were', 'object': 'chosen', 'sentence': ' These hyperparameters were chosen after experimentation on the development set .'}\n",
            "{'subject': 'hyperparameters', 'relation': 'were chosen after', 'object': 'experimentation on development set', 'sentence': ' These hyperparameters were chosen after experimentation on the development set .'}\n",
            "{'subject': 'hyperparameters', 'relation': 'were chosen after', 'object': 'experimentation', 'sentence': ' These hyperparameters were chosen after experimentation on the development set .'}\n",
            "{'subject': 'Table 2', 'relation': 'summarizes', 'object': 'our results', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table 2', 'relation': 'compares', 'object': 'our translation quality', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table 2', 'relation': 'compares training costs to', 'object': 'model architectures', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table', 'relation': 'compares', 'object': 'our translation quality', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table', 'relation': 'summarizes', 'object': 'our results', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table', 'relation': 'compares training costs to', 'object': 'other model architectures', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table', 'relation': 'compares', 'object': 'training costs', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table', 'relation': 'compares training costs to', 'object': 'model architectures', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table 2', 'relation': 'compares', 'object': 'training costs', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table 2', 'relation': 'compares training costs to', 'object': 'other model architectures from literature', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table', 'relation': 'compares training costs to', 'object': 'other model architectures from literature', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table 2', 'relation': 'compares training costs to', 'object': 'other model architectures', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table', 'relation': 'compares training costs to', 'object': 'model architectures from literature', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'Table 2', 'relation': 'compares training costs to', 'object': 'model architectures from literature', 'sentence': ' Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .'}\n",
            "{'subject': 'We', 'relation': 'estimate', 'object': 'number', 'sentence': ' We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .'}\n",
            "{'subject': 'We', 'relation': 'estimate', 'object': 'number', 'sentence': ' We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .'}\n",
            "{'subject': 'We', 'relation': 'estimate', 'object': 'estimate of precision', 'sentence': ' We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .'}\n",
            "{'subject': 'We', 'relation': 'estimate', 'object': 'estimate of sustained single precision', 'sentence': ' We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .'}\n",
            "{'subject': 'We', 'relation': 'estimate', 'object': 'number used', 'sentence': ' We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .'}\n",
            "{'subject': 'We', 'relation': 'estimate', 'object': 'estimate of single precision', 'sentence': ' We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .'}\n",
            "{'subject': 'We', 'relation': 'estimate', 'object': 'number of point operations used', 'sentence': ' We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .'}\n",
            "{'subject': 'We', 'relation': 'estimate', 'object': 'number of point operations', 'sentence': ' We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .'}\n",
            "{'subject': 'We', 'relation': 'estimate', 'object': 'estimate', 'sentence': ' We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .'}\n",
            "{'subject': 'We', 'relation': 'estimate', 'object': 'estimate of sustained precision', 'sentence': ' We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .'}\n",
            "{'subject': 'We', 'relation': 'estimate', 'object': 'number of floating point operations', 'sentence': ' We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .'}\n",
            "{'subject': 'We', 'relation': 'estimate', 'object': 'number of floating point operations used', 'sentence': ' We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change in performance to translation', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change on English to translation on development set', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'measuring', 'relation': 'change on', 'object': 'English', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change in performance to German translation', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change in performance to translation on development set', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change on English to German translation on development set', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change in performance on English to translation on development set', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change to translation on development set', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change in performance on English', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'measuring', 'relation': 'change to', 'object': 'German translation on development set', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'change', 'relation': 'is in', 'object': 'performance', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'varied in', 'object': 'different ways', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change to German translation', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'measuring', 'relation': 'change in', 'object': 'performance', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change to translation', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change to German translation on development set', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change in performance', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change in performance to German translation on development set', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change in performance on English to German translation on development set', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'varied', 'object': 'our base model', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change in performance on English to translation', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change on English to translation', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'varied in', 'object': 'ways', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'measuring', 'relation': 'change to', 'object': 'translation on development set', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change on English to German translation', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change in performance on English to German translation', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'measuring', 'relation': 'change to', 'object': 'translation', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'measuring', 'relation': 'change to', 'object': 'German translation', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'we', 'relation': 'measuring', 'object': 'change on English', 'sentence': ' 6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .'}\n",
            "{'subject': 'We', 'relation': 'described in', 'object': 'section', 'sentence': ' We used beam search as described in the previous section , but no checkpoint averaging .'}\n",
            "{'subject': 'We', 'relation': 'described in', 'object': 'previous section', 'sentence': ' We used beam search as described in the previous section , but no checkpoint averaging .'}\n",
            "{'subject': 'We', 'relation': 'present', 'object': 'results in Table 3', 'sentence': ' We present these results in Table 3 .'}\n",
            "{'subject': 'results', 'relation': 'is in', 'object': 'Table 3', 'sentence': ' We present these results in Table 3 .'}\n",
            "{'subject': 'We', 'relation': 'present', 'object': 'results', 'sentence': ' We present these results in Table 3 .'}\n",
            "{'subject': 'we', 'relation': 'vary In', 'object': 'Table', 'sentence': ' In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .'}\n",
            "{'subject': 'we', 'relation': 'keeping amount', 'object': 'described', 'sentence': ' In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .'}\n",
            "{'subject': 'we', 'relation': 'keeping amount', 'object': 'described in Section 3.2.2', 'sentence': ' In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .'}\n",
            "{'subject': 'we', 'relation': 'vary number In', 'object': 'Table', 'sentence': ' In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .'}\n",
            "{'subject': 'we', 'relation': 'vary', 'object': 'attention key', 'sentence': ' In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .'}\n",
            "{'subject': 'we', 'relation': 'vary', 'object': 'number', 'sentence': ' In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .'}\n",
            "{'subject': 'we', 'relation': 'vary', 'object': 'number of attention heads', 'sentence': ' In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .'}\n",
            "{'subject': 'we', 'relation': 'vary attention key In', 'object': 'Table', 'sentence': ' In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .'}\n",
            "{'subject': 'we', 'relation': 'keeping', 'object': 'amount of computation constant', 'sentence': ' In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .'}\n",
            "{'subject': 'we', 'relation': 'keeping', 'object': 'amount', 'sentence': ' In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .'}\n",
            "{'subject': 'single head attention', 'relation': 'is worse than', 'object': 'setting', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'single head attention', 'relation': 'is', 'object': '0.9 BLEU worse than best setting', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'single head attention', 'relation': 'is worse than', 'object': 'best setting', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'head attention', 'relation': 'is', 'object': '0.9 BLEU worse than setting', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'head attention', 'relation': 'is', 'object': 'worse', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'quality', 'relation': 'also drops off with', 'object': 'heads', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'single head attention', 'relation': 'is', 'object': 'worse', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'quality', 'relation': 'drops off with', 'object': 'many heads', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'single head attention', 'relation': 'is worse', 'object': '0.9 BLEU', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'quality', 'relation': 'also drops off with', 'object': 'many heads', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'head attention', 'relation': 'is worse than', 'object': 'best setting', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'single head attention', 'relation': 'is', 'object': '0.9 BLEU worse than setting', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'quality', 'relation': 'drops off with', 'object': 'too many heads', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'quality', 'relation': 'also drops off with', 'object': 'too many heads', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'head attention', 'relation': 'is worse than', 'object': 'setting', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'head attention', 'relation': 'is', 'object': '0.9 BLEU worse than best setting', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'quality', 'relation': 'drops off with', 'object': 'heads', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': 'head attention', 'relation': 'is worse', 'object': '0.9 BLEU', 'sentence': ' While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .'}\n",
            "{'subject': '5We', 'relation': 'used for', 'object': 'K80', 'sentence': ' 5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .'}\n",
            "{'subject': '5We', 'relation': 'used respectively for', 'object': 'K80', 'sentence': ' 5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .'}\n",
            "{'subject': '5We', 'relation': 'used', 'object': 'values', 'sentence': ' 5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .'}\n",
            "{'subject': '5We', 'relation': 'used values for', 'object': 'K80', 'sentence': ' 5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .'}\n",
            "{'subject': '5We', 'relation': 'used values respectively for', 'object': 'K80', 'sentence': ' 5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .'}\n",
            "{'subject': '5We', 'relation': 'used respectively', 'object': 'values', 'sentence': ' 5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .'}\n",
            "{'subject': 'values', 'relation': 'are identical to', 'object': 'those of base model', 'sentence': ' Unlisted values are identical to those of the base model .'}\n",
            "{'subject': 'values', 'relation': 'are', 'object': 'identical', 'sentence': ' Unlisted values are identical to those of the base model .'}\n",
            "{'subject': 'Unlisted values', 'relation': 'are identical to', 'object': 'those', 'sentence': ' Unlisted values are identical to those of the base model .'}\n",
            "{'subject': 'Unlisted values', 'relation': 'are', 'object': 'identical', 'sentence': ' Unlisted values are identical to those of the base model .'}\n",
            "{'subject': 'Unlisted values', 'relation': 'are identical to', 'object': 'those of base model', 'sentence': ' Unlisted values are identical to those of the base model .'}\n",
            "{'subject': 'values', 'relation': 'are identical to', 'object': 'those', 'sentence': ' Unlisted values are identical to those of the base model .'}\n",
            "{'subject': 'metrics', 'relation': 'are on', 'object': 'English', 'sentence': ' All metrics are on the English to German translation development set , newstest2013 .'}\n",
            "{'subject': 'metrics', 'relation': 'are on', 'object': 'English to translation development set', 'sentence': ' All metrics are on the English to German translation development set , newstest2013 .'}\n",
            "{'subject': 'metrics', 'relation': 'are on', 'object': 'English to German translation development set', 'sentence': ' All metrics are on the English to German translation development set , newstest2013 .'}\n",
            "{'subject': 'Listed perplexities', 'relation': 'are per', 'object': 'wordpiece', 'sentence': ' Listed perplexities are per wordpiece , according to our byte pair encoding , and should not be compared to per word perplexities .'}\n",
            "{'subject': 'perplexities', 'relation': 'are per', 'object': 'wordpiece', 'sentence': ' Listed perplexities are per wordpiece , according to our byte pair encoding , and should not be compared to per word perplexities .'}\n",
            "{'subject': 'model quality', 'relation': 'reducing', 'object': 'attention size dk', 'sentence': ' N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .'}\n",
            "{'subject': 'quality', 'relation': 'reducing', 'object': 'attention key size dk', 'sentence': ' N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .'}\n",
            "{'subject': 'steps', 'relation': 'dev', 'object': 'dev 106 base', 'sentence': ' N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .'}\n",
            "{'subject': 'quality', 'relation': 'reducing', 'object': 'attention size dk', 'sentence': ' N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .'}\n",
            "{'subject': 'model quality', 'relation': 'reducing', 'object': 'attention key size dk', 'sentence': ' N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .'}\n",
            "{'subject': 'We', 'relation': 'further observe in', 'object': 'rows C', 'sentence': ' We further observe in rows C and D that , as expected , bigger models are better , and dropout is very helpful in avoiding over fitting .'}\n",
            "{'subject': 'We', 'relation': 'observe in', 'object': 'rows C', 'sentence': ' We further observe in rows C and D that , as expected , bigger models are better , and dropout is very helpful in avoiding over fitting .'}\n",
            "{'subject': 'we', 'relation': 'observe results to', 'object': 'base model', 'sentence': ' In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .'}\n",
            "{'subject': 'we', 'relation': 'observe', 'object': 'results', 'sentence': ' In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .'}\n",
            "{'subject': 'we', 'relation': 'replace', 'object': 'our sinusoidal encoding', 'sentence': ' In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .'}\n",
            "{'subject': 'we', 'relation': 'observe', 'object': 'identical results', 'sentence': ' In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .'}\n",
            "{'subject': 'we', 'relation': 'observe', 'object': 'nearly identical results', 'sentence': ' In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .'}\n",
            "{'subject': 'we', 'relation': 'replace', 'object': 'our positional encoding', 'sentence': ' In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .'}\n",
            "{'subject': 'we', 'relation': 'replace', 'object': 'our encoding', 'sentence': ' In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .'}\n",
            "{'subject': 'we', 'relation': 'replace', 'object': 'our sinusoidal positional encoding', 'sentence': ' In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .'}\n",
            "{'subject': 'Transformer', 'relation': 'replacing', 'object': 'recurrent layers commonly used in encoder decoder architectures with multi', 'sentence': ' 7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .'}\n",
            "{'subject': 'Transformer', 'relation': 'replacing', 'object': 'layers commonly used in encoder decoder architectures with multi', 'sentence': ' 7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .'}\n",
            "{'subject': 'Transformer', 'relation': 'replacing', 'object': 'layers most commonly used in encoder decoder architectures with multi', 'sentence': ' 7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .'}\n",
            "{'subject': 'sequence transduction model', 'relation': 'based entirely on', 'object': 'attention', 'sentence': ' 7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .'}\n",
            "{'subject': '7 Conclusion', 'relation': 'is In', 'object': 'work', 'sentence': ' 7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .'}\n",
            "{'subject': 'first sequence transduction model', 'relation': 'based entirely on', 'object': 'attention', 'sentence': ' 7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .'}\n",
            "{'subject': 'Transformer', 'relation': 'replacing', 'object': 'recurrent layers most commonly used in encoder decoder architectures with multi', 'sentence': ' 7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .'}\n",
            "{'subject': 'Transformer', 'relation': 'headed', 'object': 'self attention', 'sentence': ' 7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .'}\n",
            "{'subject': 'sequence transduction model', 'relation': 'based on', 'object': 'attention', 'sentence': ' 7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .'}\n",
            "{'subject': 'first sequence transduction model', 'relation': 'based on', 'object': 'attention', 'sentence': ' 7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .'}\n",
            "{'subject': 'Transformer', 'relation': 'can', 'object': 'For translation tasks can trained faster', 'sentence': ' For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}\n",
            "{'subject': 'Transformer', 'relation': 'can', 'object': 'can trained', 'sentence': ' For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}\n",
            "{'subject': 'Transformer', 'relation': 'can', 'object': 'can trained faster than architectures', 'sentence': ' For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}\n",
            "{'subject': 'Transformer', 'relation': 'can', 'object': 'can trained faster', 'sentence': ' For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}\n",
            "{'subject': 'Transformer', 'relation': 'can', 'object': 'For translation tasks can trained', 'sentence': ' For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}\n",
            "{'subject': 'Transformer', 'relation': 'can', 'object': 'For translation tasks can trained significantly faster than architectures', 'sentence': ' For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}\n",
            "{'subject': 'Transformer', 'relation': 'can', 'object': 'can trained significantly faster', 'sentence': ' For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}\n",
            "{'subject': 'Transformer', 'relation': 'can', 'object': 'For translation tasks can trained significantly faster', 'sentence': ' For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}\n",
            "{'subject': 'Transformer', 'relation': 'can', 'object': 'can trained significantly faster than architectures', 'sentence': ' For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}\n",
            "{'subject': 'Transformer', 'relation': 'can', 'object': 'For translation tasks can trained faster than architectures', 'sentence': ' For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}\n",
            "{'subject': 'our model', 'relation': 'outperforms', 'object': 'even previously reported ensembles', 'sentence': ' In the former task our best model outperforms even all previously reported ensembles .'}\n",
            "{'subject': 'our model', 'relation': 'outperforms', 'object': 'previously reported ensembles', 'sentence': ' In the former task our best model outperforms even all previously reported ensembles .'}\n",
            "{'subject': 'our best model', 'relation': 'outperforms', 'object': 'even previously reported ensembles', 'sentence': ' In the former task our best model outperforms even all previously reported ensembles .'}\n",
            "{'subject': 'our best model', 'relation': 'outperforms', 'object': 'previously reported ensembles', 'sentence': ' In the former task our best model outperforms even all previously reported ensembles .'}\n",
            "{'subject': 'We', 'relation': 'are excited about', 'object': 'future of attention', 'sentence': ' We are excited about the future of attention based models and plan to apply them to other tasks .'}\n",
            "{'subject': 'We', 'relation': 'apply', 'object': 'them', 'sentence': ' We are excited about the future of attention based models and plan to apply them to other tasks .'}\n",
            "{'subject': 'We', 'relation': 'are excited about', 'object': 'future', 'sentence': ' We are excited about the future of attention based models and plan to apply them to other tasks .'}\n",
            "{'subject': 'We', 'relation': 'are', 'object': 'excited', 'sentence': ' We are excited about the future of attention based models and plan to apply them to other tasks .'}\n",
            "{'subject': 'problems', 'relation': 'involving', 'object': 'input modalities other than text', 'sentence': ' We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .'}\n",
            "{'subject': 'We', 'relation': 'extend Transformer to', 'object': 'problems', 'sentence': ' We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .'}\n",
            "{'subject': 'problems', 'relation': 'involving', 'object': 'input modalities', 'sentence': ' We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .'}\n",
            "{'subject': 'problems', 'relation': 'involving', 'object': 'input modalities other', 'sentence': ' We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .'}\n",
            "{'subject': 'We', 'relation': 'extend', 'object': 'Transformer', 'sentence': ' We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .'}\n",
            "{'subject': 'outputs', 'relation': 'large inputs such as', 'object': 'images', 'sentence': ' We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .'}\n",
            "{'subject': 'outputs', 'relation': 'inputs such as', 'object': 'images', 'sentence': ' We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .'}\n",
            "{'subject': 'code', 'relation': 'is available at', 'object': 'https', 'sentence': ' The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .'}\n",
            "{'subject': 'we', 'relation': 'train', 'object': 'our models', 'sentence': ' The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .'}\n",
            "{'subject': 'code', 'relation': 'is', 'object': 'available', 'sentence': ' The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .'}\n",
            "{'subject': 'We', 'relation': 'are', 'object': 'grateful to Nal Kalchbrenner for their fruitful comments', 'sentence': ' Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .'}\n",
            "{'subject': 'We', 'relation': 'are grateful for', 'object': 'their comments', 'sentence': ' Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .'}\n",
            "{'subject': 'We', 'relation': 'are grateful for', 'object': 'their fruitful comments', 'sentence': ' Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .'}\n",
            "{'subject': 'We', 'relation': 'are', 'object': 'grateful', 'sentence': ' Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .'}\n",
            "{'subject': 'We', 'relation': 'are grateful to', 'object': 'Nal Kalchbrenner', 'sentence': ' Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .'}\n",
            "{'subject': 'We', 'relation': 'are', 'object': 'grateful to Nal Kalchbrenner for their comments', 'sentence': ' Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(triples)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "WaaXkjQ3RVqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DBnll8AsRd5v",
        "outputId": "4f7155d0-7a20-4682-c491-daca0b9746e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       subject  \\\n",
              "0                                    Attention   \n",
              "1                                convolutional   \n",
              "2                                convolutional   \n",
              "3                                    Attention   \n",
              "4                                          you   \n",
              "5                            performing models   \n",
              "6                            performing models   \n",
              "7                                       models   \n",
              "8                            performing models   \n",
              "9                                       models   \n",
              "10                                      models   \n",
              "11                           performing models   \n",
              "12                           performing models   \n",
              "13                           performing models   \n",
              "14                                      models   \n",
              "15                                      models   \n",
              "16                                      models   \n",
              "17                                      models   \n",
              "18                           performing models   \n",
              "19                                      models   \n",
              "20                           performing models   \n",
              "21                                          We   \n",
              "22                                          We   \n",
              "23                                          We   \n",
              "24                                          We   \n",
              "25                                          We   \n",
              "26                                          We   \n",
              "27                                          We   \n",
              "28                                          We   \n",
              "29                                          We   \n",
              "30                                          We   \n",
              "31                                          We   \n",
              "32                                          We   \n",
              "33                                          We   \n",
              "34                                          We   \n",
              "35                                          We   \n",
              "36                                          We   \n",
              "37                                          We   \n",
              "38                                          We   \n",
              "39                                          We   \n",
              "40                                          We   \n",
              "41                                          We   \n",
              "42                                          We   \n",
              "43                                          We   \n",
              "44                                          We   \n",
              "45                                          We   \n",
              "46                                          We   \n",
              "47                                          We   \n",
              "48                                          We   \n",
              "49                                          We   \n",
              "50                                          We   \n",
              "51                                          We   \n",
              "52                                          We   \n",
              "53                                          We   \n",
              "54                                          We   \n",
              "55                                          We   \n",
              "56                                          We   \n",
              "57                                          We   \n",
              "58                                          We   \n",
              "59                                          We   \n",
              "60                                          We   \n",
              "61                                          We   \n",
              "62                                          We   \n",
              "63                                          We   \n",
              "64                                          We   \n",
              "65                                          We   \n",
              "66                                   Our model   \n",
              "67                                   Our model   \n",
              "68                                   Our model   \n",
              "69                                   Our model   \n",
              "70                                   Our model   \n",
              "71                                   Our model   \n",
              "72                                   Our model   \n",
              "73                                   Our model   \n",
              "74                                   Our model   \n",
              "75                                   Our model   \n",
              "76                                   Our model   \n",
              "77                                   Our model   \n",
              "78                                   Our model   \n",
              "79                                   Our model   \n",
              "80                                   Our model   \n",
              "81                                   Our model   \n",
              "82                                   Our model   \n",
              "83                                   Our model   \n",
              "84                                   our model   \n",
              "85                                   our model   \n",
              "86                                   our model   \n",
              "87                                   our model   \n",
              "88                                   our model   \n",
              "89                                   our model   \n",
              "90                                   our model   \n",
              "91                                   our model   \n",
              "92                                   our model   \n",
              "93                                   our model   \n",
              "94                                   our model   \n",
              "95                                   our model   \n",
              "96                                   our model   \n",
              "97                                   our model   \n",
              "98                                   our model   \n",
              "99                                   our model   \n",
              "100                                  our model   \n",
              "101                                  our model   \n",
              "102                                  our model   \n",
              "103                                  our model   \n",
              "104                                  our model   \n",
              "105                                  our model   \n",
              "106                                  our model   \n",
              "107                                  our model   \n",
              "108                                  our model   \n",
              "109                                  our model   \n",
              "110                                  our model   \n",
              "111                                  our model   \n",
              "112                                  our model   \n",
              "113                                  our model   \n",
              "114                                  our model   \n",
              "115                                  our model   \n",
              "116                                  our model   \n",
              "117                                  our model   \n",
              "118   1 Introduction Recurrent neural networks   \n",
              "119                           Numerous efforts   \n",
              "120                                    efforts   \n",
              "121                                    efforts   \n",
              "122                                    efforts   \n",
              "123                           Numerous efforts   \n",
              "124                           Numerous efforts   \n",
              "125                                      order   \n",
              "126                              Listing order   \n",
              "127                                      Jakob   \n",
              "128                                      Jakob   \n",
              "129                                     Ashish   \n",
              "130                                       Noam   \n",
              "131                                       Noam   \n",
              "132                                       Noam   \n",
              "133                               other person   \n",
              "134                                     person   \n",
              "135                                       Noam   \n",
              "136                                       Noam   \n",
              "137                                       Noam   \n",
              "138                                       Noam   \n",
              "139                                     person   \n",
              "140                                       Noam   \n",
              "141                                       Noam   \n",
              "142                                       Noam   \n",
              "143                                       Noam   \n",
              "144                               other person   \n",
              "145                                       Noam   \n",
              "146         evaluated countless model variants   \n",
              "147                                      Llion   \n",
              "148                                      Llion   \n",
              "149                                      Llion   \n",
              "150                                      Llion   \n",
              "151                                     Lukasz   \n",
              "152                                      Aidan   \n",
              "153                                      Aidan   \n",
              "154                                      Aidan   \n",
              "155                                    results   \n",
              "156                                      Aidan   \n",
              "157                                     Lukasz   \n",
              "158                                    results   \n",
              "159                                     Lukasz   \n",
              "160                                      Aidan   \n",
              "161                                      Aidan   \n",
              "162                                      Aidan   \n",
              "163                                     Lukasz   \n",
              "164                                      Aidan   \n",
              "165                                      Aidan   \n",
              "166                                      Aidan   \n",
              "167                                       Work   \n",
              "168                            31st Conference   \n",
              "169                                 Conference   \n",
              "170                                 Conference   \n",
              "171                                 Conference   \n",
              "172                            31st Conference   \n",
              "173                            31st Conference   \n",
              "174                                     models   \n",
              "175                           Recurrent models   \n",
              "176                           Recurrent models   \n",
              "177                                     models   \n",
              "178                                     models   \n",
              "179                                     models   \n",
              "180                           Recurrent models   \n",
              "181                                     models   \n",
              "182                           Recurrent models   \n",
              "183                                     models   \n",
              "184                           Recurrent models   \n",
              "185                           Recurrent models   \n",
              "186                                       they   \n",
              "187                                       they   \n",
              "188                                       they   \n",
              "189                                   sequence   \n",
              "190                                   sequence   \n",
              "191                                      steps   \n",
              "192                                   sequence   \n",
              "193                                   sequence   \n",
              "194                                   sequence   \n",
              "195                         memory constraints   \n",
              "196                          sequential nature   \n",
              "197                                     nature   \n",
              "198                          sequential nature   \n",
              "199                         memory constraints   \n",
              "200                                     nature   \n",
              "201                                     nature   \n",
              "202                          sequential nature   \n",
              "203                         memory constraints   \n",
              "204                                       work   \n",
              "205                                Recent work   \n",
              "206                                       work   \n",
              "207                                       work   \n",
              "208                                Recent work   \n",
              "209                                Recent work   \n",
              "210                                       work   \n",
              "211                                       work   \n",
              "212                                       work   \n",
              "213                                       work   \n",
              "214                                       work   \n",
              "215                                       work   \n",
              "216                                Recent work   \n",
              "217                                Recent work   \n",
              "218                                Recent work   \n",
              "219                                Recent work   \n",
              "220                                Recent work   \n",
              "221                                Recent work   \n",
              "222                                       work   \n",
              "223                   significant improvements   \n",
              "224                                Recent work   \n",
              "225                                       work   \n",
              "226                                Recent work   \n",
              "227                       Attention mechanisms   \n",
              "228                       Attention mechanisms   \n",
              "229                       Attention mechanisms   \n",
              "230                       Attention mechanisms   \n",
              "231               compelling sequence modeling   \n",
              "232                       Attention mechanisms   \n",
              "233                       Attention mechanisms   \n",
              "234                       Attention mechanisms   \n",
              "235                       Attention mechanisms   \n",
              "236                       Attention mechanisms   \n",
              "237                       Attention mechanisms   \n",
              "238                       Attention mechanisms   \n",
              "239                       Attention mechanisms   \n",
              "240                       Attention mechanisms   \n",
              "241                       Attention mechanisms   \n",
              "242                       Attention mechanisms   \n",
              "243                       Attention mechanisms   \n",
              "244                       Attention mechanisms   \n",
              "245                       Attention mechanisms   \n",
              "246                       Attention mechanisms   \n",
              "247                       Attention mechanisms   \n",
              "248                       Attention mechanisms   \n",
              "249                             their distance   \n",
              "250                       Attention mechanisms   \n",
              "251                       Attention mechanisms   \n",
              "252                       Attention mechanisms   \n",
              "253                       Attention mechanisms   \n",
              "254                       Attention mechanisms   \n",
              "255                       Attention mechanisms   \n",
              "256                       Attention mechanisms   \n",
              "257                       Attention mechanisms   \n",
              "258                       Attention mechanisms   \n",
              "259                       Attention mechanisms   \n",
              "260                       Attention mechanisms   \n",
              "261                       Attention mechanisms   \n",
              "262                       Attention mechanisms   \n",
              "263                       Attention mechanisms   \n",
              "264                       Attention mechanisms   \n",
              "265                       Attention mechanisms   \n",
              "266                       Attention mechanisms   \n",
              "267                       attention mechanisms   \n",
              "268                  such attention mechanisms   \n",
              "269                  such attention mechanisms   \n",
              "270                       attention mechanisms   \n",
              "271                       attention mechanisms   \n",
              "272                       attention mechanisms   \n",
              "273                  such attention mechanisms   \n",
              "274                       attention mechanisms   \n",
              "275                       attention mechanisms   \n",
              "276                                conjunction   \n",
              "277                  such attention mechanisms   \n",
              "278                  such attention mechanisms   \n",
              "279                  such attention mechanisms   \n",
              "280                  such attention mechanisms   \n",
              "281                       attention mechanisms   \n",
              "282                  such attention mechanisms   \n",
              "283                       attention mechanisms   \n",
              "284                                         we   \n",
              "285                                         we   \n",
              "286                                         we   \n",
              "287                                         we   \n",
              "288                                         we   \n",
              "289                                         we   \n",
              "290                                         we   \n",
              "291                                         we   \n",
              "292                                Transformer   \n",
              "293                                Transformer   \n",
              "294                                Transformer   \n",
              "295                                Transformer   \n",
              "296                                Transformer   \n",
              "297                                Transformer   \n",
              "298                                Transformer   \n",
              "299                                Transformer   \n",
              "300                                Transformer   \n",
              "301                                Transformer   \n",
              "302                                Transformer   \n",
              "303                                        all   \n",
              "304                                        all   \n",
              "305                                        all   \n",
              "306                                        all   \n",
              "307                                        all   \n",
              "308                                        all   \n",
              "309                                     number   \n",
              "310                                     number   \n",
              "311                                     number   \n",
              "312                                     number   \n",
              "313                                     number   \n",
              "314                                         it   \n",
              "315                                         it   \n",
              "316                                         it   \n",
              "317                                         it   \n",
              "318                                         it   \n",
              "319                        attention mechanism   \n",
              "320                        attention mechanism   \n",
              "321                             Self attention   \n",
              "322                        attention mechanism   \n",
              "323                        attention mechanism   \n",
              "324                        attention mechanism   \n",
              "325                        attention mechanism   \n",
              "326                             Self attention   \n",
              "327                             Self attention   \n",
              "328                                Transformer   \n",
              "329                                Transformer   \n",
              "330                                Transformer   \n",
              "331                                Transformer   \n",
              "332                                Transformer   \n",
              "333                                Transformer   \n",
              "334                                Transformer   \n",
              "335                                Transformer   \n",
              "336                                Transformer   \n",
              "337                                Transformer   \n",
              "338                                         we   \n",
              "339                                         we   \n",
              "340                                         we   \n",
              "341                                         we   \n",
              "342                                         we   \n",
              "343                                    encoder   \n",
              "344                                    encoder   \n",
              "345                                    encoder   \n",
              "346                                    encoder   \n",
              "347                                    encoder   \n",
              "348                                    encoder   \n",
              "349                                    encoder   \n",
              "350                                    encoder   \n",
              "351                                    encoder   \n",
              "352                                    encoder   \n",
              "353                                    encoder   \n",
              "354                                    encoder   \n",
              "355                                    decoder   \n",
              "356                                    decoder   \n",
              "357                                    decoder   \n",
              "358                         output sequence y1   \n",
              "359                                    decoder   \n",
              "360                         output sequence y1   \n",
              "361                                    decoder   \n",
              "362                                    decoder   \n",
              "363                                      model   \n",
              "364                                      model   \n",
              "365                                      model   \n",
              "366                                      model   \n",
              "367                                      model   \n",
              "368                       overall architecture   \n",
              "369                       overall architecture   \n",
              "370                       overall architecture   \n",
              "371                               architecture   \n",
              "372                       overall architecture   \n",
              "373                               architecture   \n",
              "374                       overall architecture   \n",
              "375                                Transformer   \n",
              "376                                Transformer   \n",
              "377                       overall architecture   \n",
              "378                                Transformer   \n",
              "379                               architecture   \n",
              "380                               architecture   \n",
              "381                               architecture   \n",
              "382                               architecture   \n",
              "383                               architecture   \n",
              "384                                      point   \n",
              "385                       overall architecture   \n",
              "386                       overall architecture   \n",
              "387                               architecture   \n",
              "388                                Transformer   \n",
              "389                                      layer   \n",
              "390                                     second   \n",
              "391                                     second   \n",
              "392                                      first   \n",
              "393                                     second   \n",
              "394                                      first   \n",
              "395                                     second   \n",
              "396                                      first   \n",
              "397                                     second   \n",
              "398                                     second   \n",
              "399                                         We   \n",
              "400                                         We   \n",
              "401                                         We   \n",
              "402                                         We   \n",
              "403                                         We   \n",
              "404                                         We   \n",
              "405                                         We   \n",
              "406                                 Sublayer x   \n",
              "407                                 Sublayer x   \n",
              "408                                 Sublayer x   \n",
              "409                                 Sublayer x   \n",
              "410                                 Sublayer x   \n",
              "411                                 Sublayer x   \n",
              "412                                 Sublayer x   \n",
              "413                                 Sublayer x   \n",
              "414                                 Sublayer x   \n",
              "415                                 Sublayer x   \n",
              "416                                    decoder   \n",
              "417                                    decoder   \n",
              "418                                    decoder   \n",
              "419                                    decoder   \n",
              "420                                    Decoder   \n",
              "421                                    decoder   \n",
              "422                                    decoder   \n",
              "423                                    decoder   \n",
              "424                                    decoder   \n",
              "425                             two sub layers   \n",
              "426                                         we   \n",
              "427                                         we   \n",
              "428                                         we   \n",
              "429                                         we   \n",
              "430                                         we   \n",
              "431                                         we   \n",
              "432                                         we   \n",
              "433                                         we   \n",
              "434                                         we   \n",
              "435                                         we   \n",
              "436                                         we   \n",
              "437                                         we   \n",
              "438                                         we   \n",
              "439                   self attention sub layer   \n",
              "440                   self attention sub layer   \n",
              "441                   self attention sub layer   \n",
              "442                   self attention sub layer   \n",
              "443                   self attention sub layer   \n",
              "444                                predictions   \n",
              "445                                predictions   \n",
              "446                          output embeddings   \n",
              "447                                predictions   \n",
              "448                          output embeddings   \n",
              "449                                    masking   \n",
              "450                                predictions   \n",
              "451                                     output   \n",
              "452                                     weight   \n",
              "453                                     output   \n",
              "454                                     weight   \n",
              "455                                     weight   \n",
              "456                                     output   \n",
              "457                                     weight   \n",
              "458                                     weight   \n",
              "459                                     weight   \n",
              "460                                     weight   \n",
              "461                                     weight   \n",
              "462                                     weight   \n",
              "463                                     weight   \n",
              "464                                     output   \n",
              "465                                     weight   \n",
              "466                                     weight   \n",
              "467                     compatibility function   \n",
              "468                                     weight   \n",
              "469                                     output   \n",
              "470                                     weight   \n",
              "471                                     weight   \n",
              "472                   our particular attention   \n",
              "473                              our attention   \n",
              "474                                      input   \n",
              "475                                      input   \n",
              "476                                      input   \n",
              "477                       Multi Head Attention   \n",
              "478                       Multi Head Attention   \n",
              "479                       Multi Head Attention   \n",
              "480                       Multi Head Attention   \n",
              "481                       Multi Head Attention   \n",
              "482                       Multi Head Attention   \n",
              "483                                      query   \n",
              "484                                         we   \n",
              "485                                         we   \n",
              "486                                         we   \n",
              "487                                         we   \n",
              "488                                         we   \n",
              "489                                         we   \n",
              "490                                         we   \n",
              "491                                         we   \n",
              "492                                         we   \n",
              "493                                         we   \n",
              "494                                         we   \n",
              "495                                         we   \n",
              "496                                         we   \n",
              "497                                         we   \n",
              "498                                         we   \n",
              "499                                         we   \n",
              "500                                         we   \n",
              "501                                         we   \n",
              "502                                         we   \n",
              "503                                         we   \n",
              "504                                         we   \n",
              "505                                         we   \n",
              "506                                       keys   \n",
              "507                                       keys   \n",
              "508                                       keys   \n",
              "509                                       keys   \n",
              "510                                       keys   \n",
              "511                                       keys   \n",
              "512                                       keys   \n",
              "513                                       keys   \n",
              "514                                         We   \n",
              "515                                         We   \n",
              "516                                         We   \n",
              "517                                         We   \n",
              "518                                         We   \n",
              "519                                         We   \n",
              "520                                         We   \n",
              "521                     compatibility function   \n",
              "522                         Additive attention   \n",
              "523                                  attention   \n",
              "524                            forward network   \n",
              "525                                         it   \n",
              "526                      dot product attention   \n",
              "527                                         it   \n",
              "528                                        two   \n",
              "529                      dot product attention   \n",
              "530                                         it   \n",
              "531                      dot product attention   \n",
              "532                                         it   \n",
              "533                      dot product attention   \n",
              "534                      dot product attention   \n",
              "535                                        two   \n",
              "536                      dot product attention   \n",
              "537                      dot product attention   \n",
              "538                      dot product attention   \n",
              "539                             two mechanisms   \n",
              "540                         additive attention   \n",
              "541                                  attention   \n",
              "542                         additive attention   \n",
              "543                                  attention   \n",
              "544                                  attention   \n",
              "545                                  attention   \n",
              "546                         additive attention   \n",
              "547                                  attention   \n",
              "548                                  attention   \n",
              "549                                 mechanisms   \n",
              "550                                  attention   \n",
              "551                             two mechanisms   \n",
              "552                         additive attention   \n",
              "553                         additive attention   \n",
              "554                         additive attention   \n",
              "555                                  attention   \n",
              "556                                  attention   \n",
              "557                         additive attention   \n",
              "558                             two mechanisms   \n",
              "559                         additive attention   \n",
              "560                         additive attention   \n",
              "561                                  attention   \n",
              "562                                  attention   \n",
              "563                         additive attention   \n",
              "564                         additive attention   \n",
              "565                                  attention   \n",
              "566                                  attention   \n",
              "567                         additive attention   \n",
              "568                                  attention   \n",
              "569                                  attention   \n",
              "570                         additive attention   \n",
              "571                         additive attention   \n",
              "572                         additive attention   \n",
              "573                                  attention   \n",
              "574                                 mechanisms   \n",
              "575                             two mechanisms   \n",
              "576                         additive attention   \n",
              "577                                  attention   \n",
              "578                                  attention   \n",
              "579                                  attention   \n",
              "580                         additive attention   \n",
              "581                             two mechanisms   \n",
              "582                                 mechanisms   \n",
              "583                         additive attention   \n",
              "584                         additive attention   \n",
              "585                         additive attention   \n",
              "586                                 mechanisms   \n",
              "587                                  attention   \n",
              "588                                  attention   \n",
              "589                                  attention   \n",
              "590                                 mechanisms   \n",
              "591                         additive attention   \n",
              "592                                  attention   \n",
              "593                         additive attention   \n",
              "594                         additive attention   \n",
              "595                             two mechanisms   \n",
              "596                         additive attention   \n",
              "597                                 mechanisms   \n",
              "598                         additive attention   \n",
              "599                             two mechanisms   \n",
              "600                         additive attention   \n",
              "601                             two mechanisms   \n",
              "602                         additive attention   \n",
              "603                         additive attention   \n",
              "604                                  attention   \n",
              "605                         additive attention   \n",
              "606                                  attention   \n",
              "607                                  attention   \n",
              "608                                  attention   \n",
              "609                                  attention   \n",
              "610                         additive attention   \n",
              "611                                 mechanisms   \n",
              "612                                  attention   \n",
              "613                                 mechanisms   \n",
              "614                                  attention   \n",
              "615                               dot products   \n",
              "616                               dot products   \n",
              "617                               dot products   \n",
              "618                               dot products   \n",
              "619                                         it   \n",
              "620                               dot products   \n",
              "621                               dot products   \n",
              "622                                         it   \n",
              "623                                         it   \n",
              "624                               dot products   \n",
              "625                               dot products   \n",
              "626                                         we   \n",
              "627                                         we   \n",
              "628                                         we   \n",
              "629                                    queries   \n",
              "630                                    queries   \n",
              "631                                         it   \n",
              "632                                         it   \n",
              "633                                         it   \n",
              "634                                         it   \n",
              "635                                         it   \n",
              "636                                         it   \n",
              "637                                         it   \n",
              "638                                         it   \n",
              "639                                         it   \n",
              "640                                         it   \n",
              "641                                         it   \n",
              "642                                         it   \n",
              "643                                         it   \n",
              "644                                         it   \n",
              "645                                         it   \n",
              "646                                         it   \n",
              "647                                    queries   \n",
              "648                                    queries   \n",
              "649                                         it   \n",
              "650                                         it   \n",
              "651                                    queries   \n",
              "652                                    queries   \n",
              "653                                         we   \n",
              "654                                         we   \n",
              "655                                         we   \n",
              "656                                         we   \n",
              "657                                      These   \n",
              "658                                      model   \n",
              "659                                      model   \n",
              "660                       Multi head attention   \n",
              "661                                      model   \n",
              "662                                      model   \n",
              "663                             head attention   \n",
              "664                                      model   \n",
              "665                                      model   \n",
              "666                                      model   \n",
              "667                                      model   \n",
              "668                                      model   \n",
              "669                                      model   \n",
              "670                                      model   \n",
              "671                                      model   \n",
              "672                                      model   \n",
              "673                                      model   \n",
              "674                                      model   \n",
              "675                                      model   \n",
              "676                                      model   \n",
              "677                                      model   \n",
              "678                                  averaging   \n",
              "679                                  averaging   \n",
              "680                                          q   \n",
              "681                                 components   \n",
              "682                               dot products   \n",
              "683                                 components   \n",
              "684                                          q   \n",
              "685               independent random variables   \n",
              "686                                        4To   \n",
              "687                                          q   \n",
              "688                                          q   \n",
              "689                                 components   \n",
              "690                                 components   \n",
              "691                                      their   \n",
              "692                                         we   \n",
              "693                                         we   \n",
              "694                                         we   \n",
              "695                                         we   \n",
              "696                                         we   \n",
              "697                                       cost   \n",
              "698                                 total cost   \n",
              "699                                       cost   \n",
              "700                   total computational cost   \n",
              "701                                       cost   \n",
              "702                                 total cost   \n",
              "703                         computational cost   \n",
              "704                                       cost   \n",
              "705                   total computational cost   \n",
              "706                         computational cost   \n",
              "707                                 total cost   \n",
              "708                   total computational cost   \n",
              "709                                 total cost   \n",
              "710                         computational cost   \n",
              "711                         computational cost   \n",
              "712                                       cost   \n",
              "713                   total computational cost   \n",
              "714                         computational cost   \n",
              "715                                       cost   \n",
              "716                                 total cost   \n",
              "717                                 total cost   \n",
              "718                                       cost   \n",
              "719                   total computational cost   \n",
              "720                   total computational cost   \n",
              "721                   total computational cost   \n",
              "722                                       cost   \n",
              "723                         computational cost   \n",
              "724                                 total cost   \n",
              "725                   total computational cost   \n",
              "726                   total computational cost   \n",
              "727                                 total cost   \n",
              "728                                       cost   \n",
              "729                                 total cost   \n",
              "730                                       cost   \n",
              "731                                       cost   \n",
              "732                                 total cost   \n",
              "733                         computational cost   \n",
              "734                         computational cost   \n",
              "735                                 total cost   \n",
              "736                         computational cost   \n",
              "737                   total computational cost   \n",
              "738                         computational cost   \n",
              "739                         computational cost   \n",
              "740                         computational cost   \n",
              "741                                 total cost   \n",
              "742                         computational cost   \n",
              "743                   total computational cost   \n",
              "744                                       cost   \n",
              "745                      single head attention   \n",
              "746                         computational cost   \n",
              "747                   total computational cost   \n",
              "748                   total computational cost   \n",
              "749                                 total cost   \n",
              "750                         computational cost   \n",
              "751                   total computational cost   \n",
              "752                                 total cost   \n",
              "753                   total computational cost   \n",
              "754                         computational cost   \n",
              "755                                       cost   \n",
              "756                                       cost   \n",
              "757                                       cost   \n",
              "758                         computational cost   \n",
              "759                         computational cost   \n",
              "760                         computational cost   \n",
              "761                                       cost   \n",
              "762                                 total cost   \n",
              "763                   total computational cost   \n",
              "764                                       cost   \n",
              "765                                       cost   \n",
              "766                   total computational cost   \n",
              "767                                       cost   \n",
              "768                   total computational cost   \n",
              "769                                 total cost   \n",
              "770                         computational cost   \n",
              "771                   total computational cost   \n",
              "772                   total computational cost   \n",
              "773                   total computational cost   \n",
              "774                                 total cost   \n",
              "775                         computational cost   \n",
              "776                                       cost   \n",
              "777                         computational cost   \n",
              "778                         computational cost   \n",
              "779                                       cost   \n",
              "780                         computational cost   \n",
              "781                         computational cost   \n",
              "782                                 total cost   \n",
              "783                         computational cost   \n",
              "784                                 total cost   \n",
              "785                                 total cost   \n",
              "786                                       cost   \n",
              "787                                 total cost   \n",
              "788                   total computational cost   \n",
              "789                         computational cost   \n",
              "790                                 total cost   \n",
              "791                         computational cost   \n",
              "792                   total computational cost   \n",
              "793                                       cost   \n",
              "794                                 total cost   \n",
              "795                                       cost   \n",
              "796                         computational cost   \n",
              "797                                 total cost   \n",
              "798                                       cost   \n",
              "799                   total computational cost   \n",
              "800                                 total cost   \n",
              "801                                       cost   \n",
              "802                                 total cost   \n",
              "803                   total computational cost   \n",
              "804                                       cost   \n",
              "805                                 total cost   \n",
              "806                   total computational cost   \n",
              "807                         computational cost   \n",
              "808                   total computational cost   \n",
              "809                   total computational cost   \n",
              "810                   total computational cost   \n",
              "811                                       cost   \n",
              "812                                 total cost   \n",
              "813                                 total cost   \n",
              "814                                       cost   \n",
              "815                                       cost   \n",
              "816                         computational cost   \n",
              "817                                 total cost   \n",
              "818                                       cost   \n",
              "819                         computational cost   \n",
              "820                   total computational cost   \n",
              "821                   total computational cost   \n",
              "822                                 total cost   \n",
              "823                                       cost   \n",
              "824                                 total cost   \n",
              "825                   total computational cost   \n",
              "826                                    encoder   \n",
              "827                                     output   \n",
              "828                      self attention layers   \n",
              "829                      self attention layers   \n",
              "830                      self attention layers   \n",
              "831                                         We   \n",
              "832                                         We   \n",
              "833                  leftward information flow   \n",
              "834                                         We   \n",
              "835                                         We   \n",
              "836                                         We   \n",
              "837                                         We   \n",
              "838                                         We   \n",
              "839                 two linear transformations   \n",
              "840                     linear transformations   \n",
              "841                            transformations   \n",
              "842                                       they   \n",
              "843                                       they   \n",
              "844                     linear transformations   \n",
              "845                                       they   \n",
              "846                                       they   \n",
              "847                     linear transformations   \n",
              "848                                       they   \n",
              "849                                       they   \n",
              "850                            transformations   \n",
              "851                                       they   \n",
              "852                            transformations   \n",
              "853                                       they   \n",
              "854                           two convolutions   \n",
              "855                                      layer   \n",
              "856                                inner layer   \n",
              "857                             dimensionality   \n",
              "858                                         we   \n",
              "859                                         we   \n",
              "860                                         we   \n",
              "861                                         we   \n",
              "862                                         we   \n",
              "863                                         we   \n",
              "864                                         we   \n",
              "865                                         we   \n",
              "866                                         We   \n",
              "867                                         We   \n",
              "868                                         We   \n",
              "869                                         We   \n",
              "870                                         we   \n",
              "871                                         we   \n",
              "872                                         we   \n",
              "873                                         we   \n",
              "874                                         we   \n",
              "875                                         we   \n",
              "876                                         we   \n",
              "877                                         we   \n",
              "878                                         we   \n",
              "879                                         we   \n",
              "880                                         we   \n",
              "881                                         we   \n",
              "882                                         we   \n",
              "883                                         we   \n",
              "884                                         we   \n",
              "885                                         we   \n",
              "886                        Positional Encoding   \n",
              "887                    3.5 Positional Encoding   \n",
              "888                                      model   \n",
              "889                    3.5 Positional Encoding   \n",
              "890                    3.5 Positional Encoding   \n",
              "891                        Positional Encoding   \n",
              "892                        Positional Encoding   \n",
              "893                    3.5 Positional Encoding   \n",
              "894                                      model   \n",
              "895                        Positional Encoding   \n",
              "896                                      model   \n",
              "897                        Positional Encoding   \n",
              "898                    3.5 Positional Encoding   \n",
              "899                    3.5 Positional Encoding   \n",
              "900                        Positional Encoding   \n",
              "901                    3.5 Positional Encoding   \n",
              "902                                      model   \n",
              "903                                      model   \n",
              "904                          relative position   \n",
              "905                        Positional Encoding   \n",
              "906                        Positional Encoding   \n",
              "907                    3.5 Positional Encoding   \n",
              "908                        Positional Encoding   \n",
              "909                    3.5 Positional Encoding   \n",
              "910                                         we   \n",
              "911                                         we   \n",
              "912                                         we   \n",
              "913                                         we   \n",
              "914                                         we   \n",
              "915                                          k   \n",
              "916                                          k   \n",
              "917                                       size   \n",
              "918                                          k   \n",
              "919                                          k   \n",
              "920                                          k   \n",
              "921                                          d   \n",
              "922                                          n   \n",
              "923                                  encodings   \n",
              "924                                  encodings   \n",
              "925                                  encodings   \n",
              "926                       positional encodings   \n",
              "927                                        two   \n",
              "928                                  encodings   \n",
              "929                       positional encodings   \n",
              "930                       positional encodings   \n",
              "931                       positional encodings   \n",
              "932                                  encodings   \n",
              "933                       positional encodings   \n",
              "934                                          i   \n",
              "935                                         we   \n",
              "936                                         we   \n",
              "937                                        pos   \n",
              "938                                        pos   \n",
              "939                                         we   \n",
              "940                                         we   \n",
              "941                                wavelengths   \n",
              "942                                wavelengths   \n",
              "943                                    PEpos k   \n",
              "944                                         it   \n",
              "945                                      model   \n",
              "946                                         We   \n",
              "947                                         it   \n",
              "948                                    PEpos k   \n",
              "949                                      model   \n",
              "950                                         it   \n",
              "951                                    PEpos k   \n",
              "952                                    PEpos k   \n",
              "953                                      model   \n",
              "954                                      model   \n",
              "955                                      model   \n",
              "956                                    PEpos k   \n",
              "957                                         We   \n",
              "958                                      model   \n",
              "959                                    results   \n",
              "960                                         We   \n",
              "961                                         We   \n",
              "962                          identical results   \n",
              "963                                      model   \n",
              "964                                         We   \n",
              "965                                      model   \n",
              "966                                         it   \n",
              "967                                         it   \n",
              "968                                         it   \n",
              "969                                         it   \n",
              "970                                         it   \n",
              "971                                         We   \n",
              "972                                      model   \n",
              "973                                         We   \n",
              "974                                         it   \n",
              "975                                      model   \n",
              "976                                      model   \n",
              "977                                         it   \n",
              "978                                         we   \n",
              "979                                         we   \n",
              "980                           4 Self Attention   \n",
              "981                                         we   \n",
              "982                                         we   \n",
              "983                                         we   \n",
              "984                                    section   \n",
              "985                                         we   \n",
              "986                               hidden layer   \n",
              "987                                    section   \n",
              "988                                         we   \n",
              "989                           4 Self Attention   \n",
              "990                                         we   \n",
              "991                               hidden layer   \n",
              "992                           4 Self Attention   \n",
              "993                                         we   \n",
              "994                                         zn   \n",
              "995                                         we   \n",
              "996                           4 Self Attention   \n",
              "997                                         we   \n",
              "998                                         we   \n",
              "999                                         we   \n",
              "1000                                        we   \n",
              "1001                                        we   \n",
              "1002                                       One   \n",
              "1003                                       One   \n",
              "1004                                       One   \n",
              "1005                                       One   \n",
              "1006                                       One   \n",
              "1007                                       One   \n",
              "1008                                       One   \n",
              "1009                                       One   \n",
              "1010                                   Another   \n",
              "1011                                   Another   \n",
              "1012                                   Another   \n",
              "1013                                     third   \n",
              "1014                   long range dependencies   \n",
              "1015                                     third   \n",
              "1016                                     third   \n",
              "1017                                     third   \n",
              "1018                                     third   \n",
              "1019                              dependencies   \n",
              "1020                             key challenge   \n",
              "1021                              dependencies   \n",
              "1022                              dependencies   \n",
              "1023                              dependencies   \n",
              "1024                              dependencies   \n",
              "1025                                   signals   \n",
              "1026                                   signals   \n",
              "1027                           forward signals   \n",
              "1028                           forward signals   \n",
              "1029                                   signals   \n",
              "1030                           forward signals   \n",
              "1031                                        it   \n",
              "1032                                        it   \n",
              "1033                      self attention layer   \n",
              "1034                      self attention layer   \n",
              "1035                      self attention layer   \n",
              "1036                      self attention layer   \n",
              "1037                      self attention layer   \n",
              "1038                      self attention layer   \n",
              "1039                      self attention layer   \n",
              "1040                      self attention layer   \n",
              "1041                      self attention layer   \n",
              "1042                     self attention layers   \n",
              "1043                     self attention layers   \n",
              "1044                     self attention layers   \n",
              "1045                                      case   \n",
              "1046                         sequence length n   \n",
              "1047                     self attention layers   \n",
              "1048                     self attention layers   \n",
              "1049                     self attention layers   \n",
              "1050                     self attention layers   \n",
              "1051                         sequence length n   \n",
              "1052                     self attention layers   \n",
              "1053                     self attention layers   \n",
              "1054                         sequence length n   \n",
              "1055                     self attention layers   \n",
              "1056                         sequence length n   \n",
              "1057                     self attention layers   \n",
              "1058                     self attention layers   \n",
              "1059                            self attention   \n",
              "1060                            self attention   \n",
              "1061                            self attention   \n",
              "1062                            self attention   \n",
              "1063                            self attention   \n",
              "1064                            self attention   \n",
              "1065                            self attention   \n",
              "1066                            self attention   \n",
              "1067                            self attention   \n",
              "1068                            self attention   \n",
              "1069                            self attention   \n",
              "1070                            self attention   \n",
              "1071                            self attention   \n",
              "1072                                        We   \n",
              "1073                                        We   \n",
              "1074                                        We   \n",
              "1075                                        We   \n",
              "1076                single convolutional layer   \n",
              "1077                                     stack   \n",
              "1078                                increasing   \n",
              "1079                                     stack   \n",
              "1080                                      case   \n",
              "1081                                     Doing   \n",
              "1082                                increasing   \n",
              "1083                                     Doing   \n",
              "1084                                     stack   \n",
              "1085                                     Doing   \n",
              "1086                      Convolutional layers   \n",
              "1087                                    layers   \n",
              "1088                                    layers   \n",
              "1089                      Convolutional layers   \n",
              "1090                                    layers   \n",
              "1091                      Convolutional layers   \n",
              "1092                      Convolutional layers   \n",
              "1093                      Convolutional layers   \n",
              "1094                                    layers   \n",
              "1095                      Convolutional layers   \n",
              "1096                      Convolutional layers   \n",
              "1097                      Convolutional layers   \n",
              "1098                      Convolutional layers   \n",
              "1099                      Convolutional layers   \n",
              "1100                                    layers   \n",
              "1101                                    layers   \n",
              "1102                      Convolutional layers   \n",
              "1103                      Convolutional layers   \n",
              "1104                                    layers   \n",
              "1105                      Convolutional layers   \n",
              "1106                      Convolutional layers   \n",
              "1107                                    layers   \n",
              "1108                                    layers   \n",
              "1109                                    layers   \n",
              "1110                                    layers   \n",
              "1111                                    layers   \n",
              "1112                      Convolutional layers   \n",
              "1113                      Convolutional layers   \n",
              "1114                                    layers   \n",
              "1115                      Convolutional layers   \n",
              "1116                                    layers   \n",
              "1117                      Convolutional layers   \n",
              "1118                      Convolutional layers   \n",
              "1119                      Convolutional layers   \n",
              "1120                      Convolutional layers   \n",
              "1121                                    layers   \n",
              "1122                      Convolutional layers   \n",
              "1123                      Convolutional layers   \n",
              "1124                      Convolutional layers   \n",
              "1125                                    layers   \n",
              "1126                                    layers   \n",
              "1127                      Convolutional layers   \n",
              "1128                      Convolutional layers   \n",
              "1129                                    layers   \n",
              "1130                      Convolutional layers   \n",
              "1131                      Convolutional layers   \n",
              "1132                      Convolutional layers   \n",
              "1133                      Convolutional layers   \n",
              "1134                      Convolutional layers   \n",
              "1135                      Convolutional layers   \n",
              "1136                      Convolutional layers   \n",
              "1137                      Convolutional layers   \n",
              "1138                      Convolutional layers   \n",
              "1139                                    layers   \n",
              "1140                                    layers   \n",
              "1141                      Convolutional layers   \n",
              "1142                                    layers   \n",
              "1143                                    layers   \n",
              "1144                                    layers   \n",
              "1145                      Convolutional layers   \n",
              "1146                      Convolutional layers   \n",
              "1147                      Convolutional layers   \n",
              "1148                                    layers   \n",
              "1149                                    layers   \n",
              "1150                                    layers   \n",
              "1151                      Convolutional layers   \n",
              "1152                      Convolutional layers   \n",
              "1153                      Convolutional layers   \n",
              "1154                      Convolutional layers   \n",
              "1155                                    layers   \n",
              "1156                                    layers   \n",
              "1157                                    layers   \n",
              "1158                                    layers   \n",
              "1159                                    layers   \n",
              "1160                                    layers   \n",
              "1161                                    layers   \n",
              "1162                                    layers   \n",
              "1163                      Convolutional layers   \n",
              "1164                                    layers   \n",
              "1165                      Convolutional layers   \n",
              "1166                                    layers   \n",
              "1167                                    layers   \n",
              "1168                      Convolutional layers   \n",
              "1169                      Convolutional layers   \n",
              "1170                      Convolutional layers   \n",
              "1171                      Convolutional layers   \n",
              "1172                                    layers   \n",
              "1173                      Convolutional layers   \n",
              "1174                      Convolutional layers   \n",
              "1175                      Convolutional layers   \n",
              "1176                      Convolutional layers   \n",
              "1177                                    layers   \n",
              "1178                      Convolutional layers   \n",
              "1179                      Convolutional layers   \n",
              "1180                                    layers   \n",
              "1181                      Convolutional layers   \n",
              "1182                                    layers   \n",
              "1183                      Convolutional layers   \n",
              "1184                                    layers   \n",
              "1185                      Convolutional layers   \n",
              "1186                      Convolutional layers   \n",
              "1187                                    layers   \n",
              "1188                      Convolutional layers   \n",
              "1189                      Convolutional layers   \n",
              "1190                                    layers   \n",
              "1191                                    layers   \n",
              "1192                                    layers   \n",
              "1193                      Convolutional layers   \n",
              "1194                      Convolutional layers   \n",
              "1195                                    layers   \n",
              "1196                      Convolutional layers   \n",
              "1197                                    layers   \n",
              "1198                      Convolutional layers   \n",
              "1199                      Convolutional layers   \n",
              "1200                                    layers   \n",
              "1201                      Convolutional layers   \n",
              "1202                                    layers   \n",
              "1203                                    layers   \n",
              "1204                                    layers   \n",
              "1205                                    layers   \n",
              "1206                      Convolutional layers   \n",
              "1207                                    layers   \n",
              "1208                                    layers   \n",
              "1209                                    layers   \n",
              "1210                                    layers   \n",
              "1211                                    layers   \n",
              "1212                      Convolutional layers   \n",
              "1213                      Convolutional layers   \n",
              "1214                      Convolutional layers   \n",
              "1215                                    layers   \n",
              "1216                      Convolutional layers   \n",
              "1217                                    layers   \n",
              "1218                                    layers   \n",
              "1219                      Convolutional layers   \n",
              "1220                                    layers   \n",
              "1221                                    layers   \n",
              "1222                                    layers   \n",
              "1223                                    layers   \n",
              "1224                      Convolutional layers   \n",
              "1225                      Convolutional layers   \n",
              "1226                                    layers   \n",
              "1227                                    layers   \n",
              "1228                      Convolutional layers   \n",
              "1229                      Convolutional layers   \n",
              "1230                                    layers   \n",
              "1231                                    layers   \n",
              "1232                                    layers   \n",
              "1233                                    layers   \n",
              "1234                                    layers   \n",
              "1235                      Convolutional layers   \n",
              "1236                      Convolutional layers   \n",
              "1237                                    layers   \n",
              "1238                                    layers   \n",
              "1239                      Convolutional layers   \n",
              "1240                                    layers   \n",
              "1241                      Convolutional layers   \n",
              "1242                      Convolutional layers   \n",
              "1243                                    layers   \n",
              "1244                                    layers   \n",
              "1245                                    layers   \n",
              "1246                      Convolutional layers   \n",
              "1247                                    layers   \n",
              "1248                                    layers   \n",
              "1249                                    layers   \n",
              "1250                      Convolutional layers   \n",
              "1251                                    layers   \n",
              "1252                      Convolutional layers   \n",
              "1253                                    layers   \n",
              "1254                                    layers   \n",
              "1255                                    layers   \n",
              "1256                                    layers   \n",
              "1257                                    layers   \n",
              "1258                      Convolutional layers   \n",
              "1259                                    layers   \n",
              "1260                                    layers   \n",
              "1261                      Convolutional layers   \n",
              "1262                      Convolutional layers   \n",
              "1263                      Convolutional layers   \n",
              "1264                      Convolutional layers   \n",
              "1265                                    layers   \n",
              "1266                                    layers   \n",
              "1267                                    layers   \n",
              "1268                      Convolutional layers   \n",
              "1269                      Convolutional layers   \n",
              "1270                      Convolutional layers   \n",
              "1271                      Convolutional layers   \n",
              "1272                                    layers   \n",
              "1273                      Convolutional layers   \n",
              "1274                                    layers   \n",
              "1275                                    layers   \n",
              "1276                      Convolutional layers   \n",
              "1277                                    layers   \n",
              "1278                                complexity   \n",
              "1279                                complexity   \n",
              "1280                                complexity   \n",
              "1281                                complexity   \n",
              "1282                                complexity   \n",
              "1283                                complexity   \n",
              "1284                                complexity   \n",
              "1285                                complexity   \n",
              "1286                                complexity   \n",
              "1287                                complexity   \n",
              "1288                                complexity   \n",
              "1289                                complexity   \n",
              "1290                                complexity   \n",
              "1291                                complexity   \n",
              "1292                                complexity   \n",
              "1293                                complexity   \n",
              "1294                                complexity   \n",
              "1295                                complexity   \n",
              "1296                            self attention   \n",
              "1297                            self attention   \n",
              "1298                                        We   \n",
              "1299                                        We   \n",
              "1300                                  examples   \n",
              "1301                                        We   \n",
              "1302                                        We   \n",
              "1303                                      many   \n",
              "1304                                  behavior   \n",
              "1305                                      many   \n",
              "1306                                      many   \n",
              "1307                                      many   \n",
              "1308                                  behavior   \n",
              "1309                                      many   \n",
              "1310                                  behavior   \n",
              "1311                                  behavior   \n",
              "1312                                      many   \n",
              "1313                                        We   \n",
              "1314                                        We   \n",
              "1315                                        We   \n",
              "1316                                        We   \n",
              "1317                                        We   \n",
              "1318                                        We   \n",
              "1319                                        We   \n",
              "1320                                        We   \n",
              "1321                                        We   \n",
              "1322                                        We   \n",
              "1323                                        We   \n",
              "1324                                        We   \n",
              "1325                                        We   \n",
              "1326                                        We   \n",
              "1327                                        We   \n",
              "1328                                        We   \n",
              "1329                                        We   \n",
              "1330                                        We   \n",
              "1331                                        We   \n",
              "1332                                        We   \n",
              "1333                                        We   \n",
              "1334                                        We   \n",
              "1335                                        We   \n",
              "1336                                        We   \n",
              "1337                                 Sentences   \n",
              "1338                                 Sentences   \n",
              "1339                                        we   \n",
              "1340                                        we   \n",
              "1341                                        we   \n",
              "1342                                        we   \n",
              "1343                                        we   \n",
              "1344                                        we   \n",
              "1345                                        we   \n",
              "1346                                        we   \n",
              "1347                                        we   \n",
              "1348                                        we   \n",
              "1349                                        we   \n",
              "1350                                        we   \n",
              "1351                                        we   \n",
              "1352                                        we   \n",
              "1353                                        we   \n",
              "1354                                        we   \n",
              "1355                                        we   \n",
              "1356                                        we   \n",
              "1357                                        we   \n",
              "1358                                        we   \n",
              "1359                                        we   \n",
              "1360                            Sentence pairs   \n",
              "1361                            Sentence pairs   \n",
              "1362                            Sentence pairs   \n",
              "1363                            Sentence pairs   \n",
              "1364                            Sentence pairs   \n",
              "1365                            Sentence pairs   \n",
              "1366                            training batch   \n",
              "1367                            training batch   \n",
              "1368                                        We   \n",
              "1369                             training step   \n",
              "1370                           our base models   \n",
              "1371                           our base models   \n",
              "1372                                        We   \n",
              "1373                                 step time   \n",
              "1374                                 step time   \n",
              "1375                                 step time   \n",
              "1376                                 step time   \n",
              "1377                                 step time   \n",
              "1378                                 step time   \n",
              "1379                                 step time   \n",
              "1380                                big models   \n",
              "1381                                big models   \n",
              "1382                                    models   \n",
              "1383                                    models   \n",
              "1384                                big models   \n",
              "1385                                    models   \n",
              "1386                                        We   \n",
              "1387                        first warmup steps   \n",
              "1388                              warmup steps   \n",
              "1389                                        We   \n",
              "1390                             learning rate   \n",
              "1391                             learning rate   \n",
              "1392                             learning rate   \n",
              "1393                                        We   \n",
              "1394                                        We   \n",
              "1395                                        We   \n",
              "1396                                        We   \n",
              "1397                                      sums   \n",
              "1398                                        we   \n",
              "1399                                        we   \n",
              "1400                                        we   \n",
              "1401                                        we   \n",
              "1402                                        we   \n",
              "1403                                        we   \n",
              "1404                                        we   \n",
              "1405                                        we   \n",
              "1406                                        we   \n",
              "1407                                        we   \n",
              "1408                                Model BLEU   \n",
              "1409                                Model BLEU   \n",
              "1410                                        we   \n",
              "1411                                     model   \n",
              "1412                                     model   \n",
              "1413                   German translation task   \n",
              "1414                          WMT 2014 English   \n",
              "1415                             configuration   \n",
              "1416                             configuration   \n",
              "1417                             configuration   \n",
              "1418                             configuration   \n",
              "1419                             configuration   \n",
              "1420                                  Training   \n",
              "1421                                  Training   \n",
              "1422                             our big model   \n",
              "1423                             our big model   \n",
              "1424                             our big model   \n",
              "1425                                 our model   \n",
              "1426                                 our model   \n",
              "1427                             our big model   \n",
              "1428                                 our model   \n",
              "1429                                 our model   \n",
              "1430                             our big model   \n",
              "1431                                 our model   \n",
              "1432                                        we   \n",
              "1433                                        we   \n",
              "1434                                        we   \n",
              "1435                                        we   \n",
              "1436                                        we   \n",
              "1437                                        we   \n",
              "1438                                        we   \n",
              "1439                                        we   \n",
              "1440                                        We   \n",
              "1441                                        We   \n",
              "1442                                        We   \n",
              "1443                           hyperparameters   \n",
              "1444                           hyperparameters   \n",
              "1445                           hyperparameters   \n",
              "1446                                   Table 2   \n",
              "1447                                   Table 2   \n",
              "1448                                   Table 2   \n",
              "1449                                     Table   \n",
              "1450                                     Table   \n",
              "1451                                     Table   \n",
              "1452                                     Table   \n",
              "1453                                     Table   \n",
              "1454                                   Table 2   \n",
              "1455                                   Table 2   \n",
              "1456                                     Table   \n",
              "1457                                   Table 2   \n",
              "1458                                     Table   \n",
              "1459                                   Table 2   \n",
              "1460                                        We   \n",
              "1461                                        We   \n",
              "1462                                        We   \n",
              "1463                                        We   \n",
              "1464                                        We   \n",
              "1465                                        We   \n",
              "1466                                        We   \n",
              "1467                                        We   \n",
              "1468                                        We   \n",
              "1469                                        We   \n",
              "1470                                        We   \n",
              "1471                                        We   \n",
              "1472                                        we   \n",
              "1473                                        we   \n",
              "1474                                 measuring   \n",
              "1475                                        we   \n",
              "1476                                        we   \n",
              "1477                                        we   \n",
              "1478                                        we   \n",
              "1479                                        we   \n",
              "1480                                        we   \n",
              "1481                                        we   \n",
              "1482                                 measuring   \n",
              "1483                                    change   \n",
              "1484                                        we   \n",
              "1485                                        we   \n",
              "1486                                 measuring   \n",
              "1487                                        we   \n",
              "1488                                        we   \n",
              "1489                                        we   \n",
              "1490                                        we   \n",
              "1491                                        we   \n",
              "1492                                        we   \n",
              "1493                                        we   \n",
              "1494                                        we   \n",
              "1495                                        we   \n",
              "1496                                 measuring   \n",
              "1497                                        we   \n",
              "1498                                        we   \n",
              "1499                                 measuring   \n",
              "1500                                 measuring   \n",
              "1501                                        we   \n",
              "1502                                        We   \n",
              "1503                                        We   \n",
              "1504                                        We   \n",
              "1505                                   results   \n",
              "1506                                        We   \n",
              "1507                                        we   \n",
              "1508                                        we   \n",
              "1509                                        we   \n",
              "1510                                        we   \n",
              "1511                                        we   \n",
              "1512                                        we   \n",
              "1513                                        we   \n",
              "1514                                        we   \n",
              "1515                                        we   \n",
              "1516                                        we   \n",
              "1517                     single head attention   \n",
              "1518                     single head attention   \n",
              "1519                     single head attention   \n",
              "1520                            head attention   \n",
              "1521                            head attention   \n",
              "1522                                   quality   \n",
              "1523                     single head attention   \n",
              "1524                                   quality   \n",
              "1525                     single head attention   \n",
              "1526                                   quality   \n",
              "1527                            head attention   \n",
              "1528                     single head attention   \n",
              "1529                                   quality   \n",
              "1530                                   quality   \n",
              "1531                            head attention   \n",
              "1532                            head attention   \n",
              "1533                                   quality   \n",
              "1534                            head attention   \n",
              "1535                                       5We   \n",
              "1536                                       5We   \n",
              "1537                                       5We   \n",
              "1538                                       5We   \n",
              "1539                                       5We   \n",
              "1540                                       5We   \n",
              "1541                                    values   \n",
              "1542                                    values   \n",
              "1543                           Unlisted values   \n",
              "1544                           Unlisted values   \n",
              "1545                           Unlisted values   \n",
              "1546                                    values   \n",
              "1547                                   metrics   \n",
              "1548                                   metrics   \n",
              "1549                                   metrics   \n",
              "1550                       Listed perplexities   \n",
              "1551                              perplexities   \n",
              "1552                             model quality   \n",
              "1553                                   quality   \n",
              "1554                                     steps   \n",
              "1555                                   quality   \n",
              "1556                             model quality   \n",
              "1557                                        We   \n",
              "1558                                        We   \n",
              "1559                                        we   \n",
              "1560                                        we   \n",
              "1561                                        we   \n",
              "1562                                        we   \n",
              "1563                                        we   \n",
              "1564                                        we   \n",
              "1565                                        we   \n",
              "1566                                        we   \n",
              "1567                               Transformer   \n",
              "1568                               Transformer   \n",
              "1569                               Transformer   \n",
              "1570               sequence transduction model   \n",
              "1571                              7 Conclusion   \n",
              "1572         first sequence transduction model   \n",
              "1573                               Transformer   \n",
              "1574                               Transformer   \n",
              "1575               sequence transduction model   \n",
              "1576         first sequence transduction model   \n",
              "1577                               Transformer   \n",
              "1578                               Transformer   \n",
              "1579                               Transformer   \n",
              "1580                               Transformer   \n",
              "1581                               Transformer   \n",
              "1582                               Transformer   \n",
              "1583                               Transformer   \n",
              "1584                               Transformer   \n",
              "1585                               Transformer   \n",
              "1586                               Transformer   \n",
              "1587                                 our model   \n",
              "1588                                 our model   \n",
              "1589                            our best model   \n",
              "1590                            our best model   \n",
              "1591                                        We   \n",
              "1592                                        We   \n",
              "1593                                        We   \n",
              "1594                                        We   \n",
              "1595                                  problems   \n",
              "1596                                        We   \n",
              "1597                                  problems   \n",
              "1598                                  problems   \n",
              "1599                                        We   \n",
              "1600                                   outputs   \n",
              "1601                                   outputs   \n",
              "1602                                      code   \n",
              "1603                                        we   \n",
              "1604                                      code   \n",
              "1605                                        We   \n",
              "1606                                        We   \n",
              "1607                                        We   \n",
              "1608                                        We   \n",
              "1609                                        We   \n",
              "1610                                        We   \n",
              "\n",
              "                                         relation  \\\n",
              "0                                              is   \n",
              "1                                         include   \n",
              "2                                         include   \n",
              "3                                          is All   \n",
              "4                                            Need   \n",
              "5                                         connect   \n",
              "6                    also connect decoder through   \n",
              "7                    also connect encoder through   \n",
              "8                    also connect encoder through   \n",
              "9                                    also connect   \n",
              "10                   also connect decoder through   \n",
              "11                                   also connect   \n",
              "12                        connect decoder through   \n",
              "13                                        connect   \n",
              "14                                        connect   \n",
              "15                                   also connect   \n",
              "16                        connect encoder through   \n",
              "17                        connect decoder through   \n",
              "18                        connect encoder through   \n",
              "19                                        connect   \n",
              "20                                   also connect   \n",
              "21                                        propose   \n",
              "22                                        propose   \n",
              "23                                        propose   \n",
              "24                                        propose   \n",
              "25                                        propose   \n",
              "26                                        propose   \n",
              "27                                        propose   \n",
              "28                                        propose   \n",
              "29                                        propose   \n",
              "30                                        propose   \n",
              "31                                        propose   \n",
              "32                                        propose   \n",
              "33                                        propose   \n",
              "34                                        propose   \n",
              "35                                        propose   \n",
              "36                                        propose   \n",
              "37                                        propose   \n",
              "38                                        propose   \n",
              "39                                        propose   \n",
              "40                                        propose   \n",
              "41                                        propose   \n",
              "42                                        propose   \n",
              "43                                        propose   \n",
              "44                                        propose   \n",
              "45                                        propose   \n",
              "46                                        propose   \n",
              "47                                        propose   \n",
              "48                                        propose   \n",
              "49                                        propose   \n",
              "50                                        propose   \n",
              "51                                        propose   \n",
              "52                                        propose   \n",
              "53                                        propose   \n",
              "54                                        propose   \n",
              "55                                        propose   \n",
              "56                                        propose   \n",
              "57                                        propose   \n",
              "58                                        propose   \n",
              "59                                        propose   \n",
              "60                                        propose   \n",
              "61                                        propose   \n",
              "62                                        propose   \n",
              "63                                        propose   \n",
              "64                                        propose   \n",
              "65                                        propose   \n",
              "66                                 improving over   \n",
              "67                                 improving over   \n",
              "68                                 improving over   \n",
              "69                                 improving over   \n",
              "70                                 improving over   \n",
              "71                                 improving over   \n",
              "72                                 improving over   \n",
              "73                                 improving over   \n",
              "74                                 improving over   \n",
              "75                                       achieves   \n",
              "76                                 improving over   \n",
              "77                                 improving over   \n",
              "78                                 improving over   \n",
              "79                                       achieves   \n",
              "80                                 improving over   \n",
              "81                                 improving over   \n",
              "82                                 improving over   \n",
              "83                                 improving over   \n",
              "84                                    establishes   \n",
              "85                                    establishes   \n",
              "86                                    establishes   \n",
              "87                                    establishes   \n",
              "88                                    establishes   \n",
              "89                                    establishes   \n",
              "90                                    establishes   \n",
              "91                  establishes model state after   \n",
              "92                                    establishes   \n",
              "93                                    establishes   \n",
              "94                                    establishes   \n",
              "95                                    establishes   \n",
              "96                                    establishes   \n",
              "97                                    establishes   \n",
              "98                     establishes model state On   \n",
              "99                     establishes model state On   \n",
              "100                                   establishes   \n",
              "101                                   establishes   \n",
              "102                                   establishes   \n",
              "103                                   establishes   \n",
              "104                                   establishes   \n",
              "105                                   establishes   \n",
              "106                                   establishes   \n",
              "107                                   establishes   \n",
              "108                                   establishes   \n",
              "109                       establishes fraction On   \n",
              "110                    establishes model state On   \n",
              "111                                   establishes   \n",
              "112                                   establishes   \n",
              "113                       establishes fraction On   \n",
              "114                                   establishes   \n",
              "115                    establishes fraction after   \n",
              "116                       establishes fraction On   \n",
              "117                                   establishes   \n",
              "118                                     memory in   \n",
              "119                                          push   \n",
              "120                                          push   \n",
              "121                                          push   \n",
              "122                                          push   \n",
              "123                                          push   \n",
              "124                                          push   \n",
              "125                                            is   \n",
              "126                                            is   \n",
              "127                                     replacing   \n",
              "128                           replacing RNNs with   \n",
              "129                                       is with   \n",
              "130                                        became   \n",
              "131                                        became   \n",
              "132                                        became   \n",
              "133                                   involved in   \n",
              "134                                   involved in   \n",
              "135                                      proposed   \n",
              "136                                      proposed   \n",
              "137                                        became   \n",
              "138                                      proposed   \n",
              "139                                   involved in   \n",
              "140                                      proposed   \n",
              "141                                      proposed   \n",
              "142                                        became   \n",
              "143                                      proposed   \n",
              "144                                   involved in   \n",
              "145                                        became   \n",
              "146                                         is in   \n",
              "147                             experimented with   \n",
              "148                             experimented with   \n",
              "149                        also experimented with   \n",
              "150                        also experimented with   \n",
              "151                                 spent at_time   \n",
              "152                                     replacing   \n",
              "153                                     replacing   \n",
              "154                                 spent at_time   \n",
              "155                                  accelerating   \n",
              "156                                     designing   \n",
              "157                                 spent at_time   \n",
              "158                        massively accelerating   \n",
              "159                                 spent at_time   \n",
              "160                                     designing   \n",
              "161                                     designing   \n",
              "162                                 spent at_time   \n",
              "163                                 spent at_time   \n",
              "164                                 spent at_time   \n",
              "165                                 spent at_time   \n",
              "166                                     designing   \n",
              "167                                  performed at   \n",
              "168                                          NIPS   \n",
              "169                                          NIPS   \n",
              "170                                          NIPS   \n",
              "171                                          NIPS   \n",
              "172                                          NIPS   \n",
              "173                                          NIPS   \n",
              "174                                        factor   \n",
              "175                                        factor   \n",
              "176                              typically factor   \n",
              "177                                        factor   \n",
              "178                              typically factor   \n",
              "179                                        factor   \n",
              "180                                        factor   \n",
              "181                              typically factor   \n",
              "182                              typically factor   \n",
              "183                              typically factor   \n",
              "184                              typically factor   \n",
              "185                                        factor   \n",
              "186                         Aligning positions to   \n",
              "187                         Aligning positions to   \n",
              "188                                      Aligning   \n",
              "189                                         ht as   \n",
              "190                                         ht as   \n",
              "191                                         is in   \n",
              "192                                         ht as   \n",
              "193                                         ht as   \n",
              "194                                         ht as   \n",
              "195                                         limit   \n",
              "196                                     precludes   \n",
              "197                                     precludes   \n",
              "198                     precludes parallelization   \n",
              "199                               batching across   \n",
              "200                                     precludes   \n",
              "201                     precludes parallelization   \n",
              "202                                     precludes   \n",
              "203                                         limit   \n",
              "204   also improving model performance in case of   \n",
              "205        improving model performance in case of   \n",
              "206                                  has achieved   \n",
              "207        improving model performance in case of   \n",
              "208   also improving model performance in case of   \n",
              "209                                  has achieved   \n",
              "210                                  has achieved   \n",
              "211             has achieved improvements through   \n",
              "212                                also improving   \n",
              "213                                  has achieved   \n",
              "214                                     improving   \n",
              "215                                  has achieved   \n",
              "216                                  has achieved   \n",
              "217             has achieved improvements through   \n",
              "218                                  has achieved   \n",
              "219                                  has achieved   \n",
              "220                                     improving   \n",
              "221                                also improving   \n",
              "222                                  has achieved   \n",
              "223                                         is in   \n",
              "224                                  has achieved   \n",
              "225                                  has achieved   \n",
              "226                                  has achieved   \n",
              "227                                   have become   \n",
              "228                                      allowing   \n",
              "229                                       part of   \n",
              "230                              integral part of   \n",
              "231                                         is in   \n",
              "232                                   have become   \n",
              "233                     allowing modeling without   \n",
              "234                              allowing without   \n",
              "235                                   have become   \n",
              "236                                       part of   \n",
              "237                                       part of   \n",
              "238                                   have become   \n",
              "239                                   have become   \n",
              "240                     allowing modeling without   \n",
              "241                              allowing without   \n",
              "242                                   have become   \n",
              "243                                      allowing   \n",
              "244                                   have become   \n",
              "245                                   have become   \n",
              "246                                   have become   \n",
              "247                     allowing modeling without   \n",
              "248                              integral part of   \n",
              "249                                         is in   \n",
              "250                                       part of   \n",
              "251                                   have become   \n",
              "252                                   have become   \n",
              "253                                       part of   \n",
              "254                                   have become   \n",
              "255                                   have become   \n",
              "256                                   have become   \n",
              "257                              integral part of   \n",
              "258                                   have become   \n",
              "259                              integral part of   \n",
              "260                              integral part of   \n",
              "261                                       part of   \n",
              "262                              allowing without   \n",
              "263                                   have become   \n",
              "264                                   have become   \n",
              "265                                   have become   \n",
              "266                              integral part of   \n",
              "267                                           are   \n",
              "268                                   are used in   \n",
              "269                                   are used in   \n",
              "270                                   are used in   \n",
              "271                           however are used in   \n",
              "272                           however are used in   \n",
              "273                           however are used in   \n",
              "274                           however are used in   \n",
              "275                                   are used in   \n",
              "276                                       is with   \n",
              "277                                           are   \n",
              "278                           however are used in   \n",
              "279                                           are   \n",
              "280                           however are used in   \n",
              "281                                   are used in   \n",
              "282                                   are used in   \n",
              "283                                           are   \n",
              "284                 propose model architecture In   \n",
              "285                                       propose   \n",
              "286                                       propose   \n",
              "287                                       propose   \n",
              "288                                       propose   \n",
              "289                                       propose   \n",
              "290                                       propose   \n",
              "291                        propose Transformer In   \n",
              "292                                reach state in   \n",
              "293                                   reach state   \n",
              "294                                   reach state   \n",
              "295                                         reach   \n",
              "296                                    allows for   \n",
              "297                                         reach   \n",
              "298                                   reach state   \n",
              "299                                         reach   \n",
              "300                                    allows for   \n",
              "301                                         reach   \n",
              "302                                    allows for   \n",
              "303                                           use   \n",
              "304                               use networks as   \n",
              "305                                           use   \n",
              "306                                           use   \n",
              "307                               use networks as   \n",
              "308                                           use   \n",
              "309                                      grows In   \n",
              "310                                     grows for   \n",
              "311                                      grows in   \n",
              "312                                      grows in   \n",
              "313                                     grows for   \n",
              "314                                         learn   \n",
              "315                          dependencies between   \n",
              "316                                         learn   \n",
              "317                                         learn   \n",
              "318                          dependencies between   \n",
              "319                                      relating   \n",
              "320                                      relating   \n",
              "321                                            is   \n",
              "322                                      relating   \n",
              "323                                      relating   \n",
              "324                                      relating   \n",
              "325                                      relating   \n",
              "326                                           has   \n",
              "327                                           has   \n",
              "328                                            is   \n",
              "329                                            is   \n",
              "330                                            is   \n",
              "331                                            is   \n",
              "332                                            is   \n",
              "333                                            is   \n",
              "334                                            is   \n",
              "335                                            is   \n",
              "336                                            is   \n",
              "337                                            is   \n",
              "338                                       discuss   \n",
              "339                                 will describe   \n",
              "340                  will describe Transformer In   \n",
              "341                  will describe Transformer In   \n",
              "342                                      motivate   \n",
              "343                                     Here maps   \n",
              "344                                     Here maps   \n",
              "345                                          maps   \n",
              "346                                          maps   \n",
              "347                                     Here maps   \n",
              "348                                          maps   \n",
              "349                                     Here maps   \n",
              "350                                     Here maps   \n",
              "351                                          maps   \n",
              "352                                          maps   \n",
              "353                                     Here maps   \n",
              "354                                          maps   \n",
              "355                                     generates   \n",
              "356                                     generates   \n",
              "357            generates output sequence y1 Given   \n",
              "358                                         ym of   \n",
              "359                                     generates   \n",
              "360                                         ym at   \n",
              "361                                     generates   \n",
              "362                                     generates   \n",
              "363                          consuming symbols as   \n",
              "364                                     consuming   \n",
              "365                                     consuming   \n",
              "366                                     consuming   \n",
              "367                          consuming symbols as   \n",
              "368                                      shown in   \n",
              "369                                      shown in   \n",
              "370                                      shown in   \n",
              "371                                         using   \n",
              "372                                         using   \n",
              "373                                         using   \n",
              "374                                         using   \n",
              "375                          follows respectively   \n",
              "376                          follows respectively   \n",
              "377                                         using   \n",
              "378                                       follows   \n",
              "379                                      shown in   \n",
              "380                                         using   \n",
              "381                                      shown in   \n",
              "382                                      shown in   \n",
              "383                                         using   \n",
              "384                                          wise   \n",
              "385                                         using   \n",
              "386                                      shown in   \n",
              "387                                      shown in   \n",
              "388                                       follows   \n",
              "389                                           has   \n",
              "390                                            is   \n",
              "391                                            is   \n",
              "392                                            is   \n",
              "393                                            is   \n",
              "394                                            is   \n",
              "395                                            is   \n",
              "396                                            is   \n",
              "397                                            is   \n",
              "398                                            is   \n",
              "399                                        employ   \n",
              "400                                   followed by   \n",
              "401                                        employ   \n",
              "402                                        employ   \n",
              "403                                        employ   \n",
              "404                                        employ   \n",
              "405                                        employ   \n",
              "406                                            is   \n",
              "407                                implemented by   \n",
              "408                                            is   \n",
              "409                                            is   \n",
              "410                                            is   \n",
              "411                                            is   \n",
              "412                                implemented by   \n",
              "413                                            is   \n",
              "414                                            is   \n",
              "415                                            is   \n",
              "416                                is composed of   \n",
              "417                           is also composed of   \n",
              "418                                            is   \n",
              "419                                is composed of   \n",
              "420                                   composed of   \n",
              "421                           is also composed of   \n",
              "422                                is composed of   \n",
              "423                                            is   \n",
              "424                           is also composed of   \n",
              "425                                         is in   \n",
              "426                                Similar employ   \n",
              "427                                        employ   \n",
              "428                                Similar employ   \n",
              "429                                        employ   \n",
              "430                                        employ   \n",
              "431                                        employ   \n",
              "432                                   followed by   \n",
              "433                                Similar employ   \n",
              "434                                Similar employ   \n",
              "435                                Similar employ   \n",
              "436                                Similar employ   \n",
              "437                                        employ   \n",
              "438                                        employ   \n",
              "439                             prevent positions   \n",
              "440                             prevent positions   \n",
              "441                                       prevent   \n",
              "442                                         is in   \n",
              "443                             prevent positions   \n",
              "444                                     depend on   \n",
              "445                                     depend on   \n",
              "446                                 are offset by   \n",
              "447                                     depend on   \n",
              "448                                 are offset by   \n",
              "449                                 combined with   \n",
              "450                                     depend on   \n",
              "451                                is computed as   \n",
              "452                                            is   \n",
              "453                                is computed as   \n",
              "454                                            is   \n",
              "455                                is computed by   \n",
              "456                                            is   \n",
              "457                                is computed by   \n",
              "458                                   assigned to   \n",
              "459                                            is   \n",
              "460                                is computed by   \n",
              "461                                            is   \n",
              "462                                            is   \n",
              "463                                            is   \n",
              "464                                is computed as   \n",
              "465                                is computed by   \n",
              "466                                is computed by   \n",
              "467                                       is with   \n",
              "468                                is computed by   \n",
              "469                                is computed as   \n",
              "470                                            is   \n",
              "471                                            is   \n",
              "472                                        Scaled   \n",
              "473                                        Scaled   \n",
              "474                                   consists of   \n",
              "475                                     values of   \n",
              "476                                   consists of   \n",
              "477                                   consists of   \n",
              "478                                   consists of   \n",
              "479                                   consists of   \n",
              "480                                   consists of   \n",
              "481                                   consists of   \n",
              "482                                   consists of   \n",
              "483                                       is with   \n",
              "484                 compute attention function on   \n",
              "485                 compute attention function on   \n",
              "486                 compute attention function on   \n",
              "487                 compute attention function on   \n",
              "488                 compute attention function on   \n",
              "489                 compute attention function on   \n",
              "490                 compute attention function on   \n",
              "491                 compute attention function on   \n",
              "492                 compute attention function on   \n",
              "493                 compute attention function on   \n",
              "494                 compute attention function on   \n",
              "495                 compute attention function on   \n",
              "496                 compute attention function on   \n",
              "497                 compute attention function on   \n",
              "498                 compute attention function on   \n",
              "499                 compute attention function In   \n",
              "500                 compute attention function on   \n",
              "501                                       compute   \n",
              "502                 compute attention function on   \n",
              "503                 compute attention function on   \n",
              "504                 compute attention function on   \n",
              "505                 compute attention function on   \n",
              "506                                           are   \n",
              "507                                           are   \n",
              "508                                           are   \n",
              "509                                           are   \n",
              "510                                           are   \n",
              "511                                           are   \n",
              "512                                           are   \n",
              "513                                           are   \n",
              "514                                     Attention   \n",
              "515                                     Attention   \n",
              "516                                     Attention   \n",
              "517                                     Attention   \n",
              "518                                       compute   \n",
              "519                                     Attention   \n",
              "520                                       compute   \n",
              "521                                         using   \n",
              "522                                      computes   \n",
              "523                                      computes   \n",
              "524                                       is with   \n",
              "525                                           can   \n",
              "526                                is much faster   \n",
              "527                                         using   \n",
              "528                                are similar in   \n",
              "529                                     is faster   \n",
              "530                                         using   \n",
              "531                                     is faster   \n",
              "532                                         using   \n",
              "533                                            is   \n",
              "534                                            is   \n",
              "535                                           are   \n",
              "536                                     is faster   \n",
              "537                                is much faster   \n",
              "538                                is much faster   \n",
              "539                                   perform for   \n",
              "540             outperforms dot product attention   \n",
              "541             outperforms dot product attention   \n",
              "542             outperforms dot product attention   \n",
              "543             outperforms dot product attention   \n",
              "544             outperforms dot product attention   \n",
              "545             outperforms dot product attention   \n",
              "546             outperforms dot product attention   \n",
              "547             outperforms dot product attention   \n",
              "548             outperforms dot product attention   \n",
              "549                         perform similarly for   \n",
              "550             outperforms dot product attention   \n",
              "551                         perform similarly for   \n",
              "552             outperforms dot product attention   \n",
              "553             outperforms dot product attention   \n",
              "554             outperforms dot product attention   \n",
              "555             outperforms dot product attention   \n",
              "556             outperforms dot product attention   \n",
              "557             outperforms dot product attention   \n",
              "558                         perform similarly for   \n",
              "559                                   scaling for   \n",
              "560             outperforms dot product attention   \n",
              "561                                   outperforms   \n",
              "562                                   scaling for   \n",
              "563             outperforms dot product attention   \n",
              "564                                   outperforms   \n",
              "565             outperforms dot product attention   \n",
              "566             outperforms dot product attention   \n",
              "567             outperforms dot product attention   \n",
              "568             outperforms dot product attention   \n",
              "569             outperforms dot product attention   \n",
              "570             outperforms dot product attention   \n",
              "571                                   scaling for   \n",
              "572             outperforms dot product attention   \n",
              "573             outperforms dot product attention   \n",
              "574                                   perform for   \n",
              "575                                   perform for   \n",
              "576             outperforms dot product attention   \n",
              "577             outperforms dot product attention   \n",
              "578             outperforms dot product attention   \n",
              "579                                   scaling for   \n",
              "580             outperforms dot product attention   \n",
              "581                                   perform for   \n",
              "582                         perform similarly for   \n",
              "583             outperforms dot product attention   \n",
              "584             outperforms dot product attention   \n",
              "585             outperforms dot product attention   \n",
              "586                                   perform for   \n",
              "587             outperforms dot product attention   \n",
              "588             outperforms dot product attention   \n",
              "589             outperforms dot product attention   \n",
              "590                         perform similarly for   \n",
              "591             outperforms dot product attention   \n",
              "592             outperforms dot product attention   \n",
              "593             outperforms dot product attention   \n",
              "594                                   scaling for   \n",
              "595                         perform similarly for   \n",
              "596             outperforms dot product attention   \n",
              "597                         perform similarly for   \n",
              "598             outperforms dot product attention   \n",
              "599                         perform similarly for   \n",
              "600             outperforms dot product attention   \n",
              "601                                   perform for   \n",
              "602             outperforms dot product attention   \n",
              "603             outperforms dot product attention   \n",
              "604             outperforms dot product attention   \n",
              "605                                   scaling for   \n",
              "606                                   scaling for   \n",
              "607             outperforms dot product attention   \n",
              "608             outperforms dot product attention   \n",
              "609             outperforms dot product attention   \n",
              "610             outperforms dot product attention   \n",
              "611                                   perform for   \n",
              "612                                   scaling for   \n",
              "613                                   perform for   \n",
              "614             outperforms dot product attention   \n",
              "615                                      grow for   \n",
              "616                                       grow in   \n",
              "617                 pushing softmax function into   \n",
              "618                                      grow for   \n",
              "619                                           has   \n",
              "620                                      grow for   \n",
              "621                                       pushing   \n",
              "622                                           has   \n",
              "623                                           has   \n",
              "624                                          grow   \n",
              "625                                      grow for   \n",
              "626                         scale dot products by   \n",
              "627                                         scale   \n",
              "628                                    counteract   \n",
              "629                          learned respectively   \n",
              "630                                       learned   \n",
              "631                              linearly project   \n",
              "632                   project values h times with   \n",
              "633                              linearly project   \n",
              "634                                       project   \n",
              "635          linearly project values h times with   \n",
              "636                          learned respectively   \n",
              "637                          learned respectively   \n",
              "638           learned projections respectively to   \n",
              "639                          project queries with   \n",
              "640                                       learned   \n",
              "641                             project keys with   \n",
              "642                        learned projections to   \n",
              "643                                       project   \n",
              "644                                       learned   \n",
              "645                                       project   \n",
              "646                              linearly project   \n",
              "647                                       learned   \n",
              "648                        learned projections to   \n",
              "649                    linearly project keys with   \n",
              "650                 linearly project queries with   \n",
              "651           learned projections respectively to   \n",
              "652                          learned respectively   \n",
              "653                                      yielding   \n",
              "654                                      yielding   \n",
              "655                                       perform   \n",
              "656                 perform attention function in   \n",
              "657                                   depicted in   \n",
              "658                             jointly attend to   \n",
              "659                             jointly attend to   \n",
              "660                                        allows   \n",
              "661                                     attend to   \n",
              "662                                     attend to   \n",
              "663                                        allows   \n",
              "664                             jointly attend to   \n",
              "665                             jointly attend to   \n",
              "666                             jointly attend to   \n",
              "667                             jointly attend to   \n",
              "668                                     attend to   \n",
              "669                                     attend to   \n",
              "670                                     attend to   \n",
              "671                                     attend to   \n",
              "672                                     attend to   \n",
              "673                             jointly attend to   \n",
              "674                                     attend to   \n",
              "675                             jointly attend to   \n",
              "676                             jointly attend to   \n",
              "677                                     attend to   \n",
              "678                                 inhibits With   \n",
              "679                                 inhibits With   \n",
              "680                             components of are   \n",
              "681                                           are   \n",
              "682                                           get   \n",
              "683                                           are   \n",
              "684                             components of are   \n",
              "685                                       is with   \n",
              "686                                    illustrate   \n",
              "687                             components of are   \n",
              "688                             components of are   \n",
              "689                                           are   \n",
              "690                                           are   \n",
              "691                                       product   \n",
              "692                               employ heads In   \n",
              "693                                        employ   \n",
              "694                                        employ   \n",
              "695                                        employ   \n",
              "696                                           use   \n",
              "697                                 is similar to   \n",
              "698                             Due is similar to   \n",
              "699                                            is   \n",
              "700                                 is similar to   \n",
              "701                             Due is similar to   \n",
              "702                                            is   \n",
              "703                                            is   \n",
              "704                             Due is similar to   \n",
              "705                                            is   \n",
              "706                                            is   \n",
              "707                             Due is similar to   \n",
              "708                                 is similar to   \n",
              "709                                            is   \n",
              "710                                 is similar to   \n",
              "711                                            is   \n",
              "712                             Due is similar to   \n",
              "713                                            is   \n",
              "714                                            is   \n",
              "715                                            is   \n",
              "716                                 is similar to   \n",
              "717                             Due is similar to   \n",
              "718                                 is similar to   \n",
              "719                             Due is similar to   \n",
              "720                                            is   \n",
              "721                                            is   \n",
              "722                                            is   \n",
              "723                             Due is similar to   \n",
              "724                                 is similar to   \n",
              "725                             Due is similar to   \n",
              "726                                 is similar to   \n",
              "727                                 is similar to   \n",
              "728                                            is   \n",
              "729                                 is similar to   \n",
              "730                                 is similar to   \n",
              "731                             Due is similar to   \n",
              "732                                            is   \n",
              "733                             Due is similar to   \n",
              "734                                            is   \n",
              "735                                            is   \n",
              "736                                            is   \n",
              "737                                            is   \n",
              "738                                            is   \n",
              "739                                            is   \n",
              "740                                 is similar to   \n",
              "741                                            is   \n",
              "742                                            is   \n",
              "743                                            is   \n",
              "744                                            is   \n",
              "745                                       is with   \n",
              "746                                            is   \n",
              "747                                 is similar to   \n",
              "748                                 is similar to   \n",
              "749                                            is   \n",
              "750                                            is   \n",
              "751                             Due is similar to   \n",
              "752                             Due is similar to   \n",
              "753                                            is   \n",
              "754                                 is similar to   \n",
              "755                                            is   \n",
              "756                                            is   \n",
              "757                                            is   \n",
              "758                             Due is similar to   \n",
              "759                                            is   \n",
              "760                             Due is similar to   \n",
              "761                                            is   \n",
              "762                             Due is similar to   \n",
              "763                                            is   \n",
              "764                                            is   \n",
              "765                                            is   \n",
              "766                             Due is similar to   \n",
              "767                             Due is similar to   \n",
              "768                                            is   \n",
              "769                                            is   \n",
              "770                                            is   \n",
              "771                                            is   \n",
              "772                                            is   \n",
              "773                             Due is similar to   \n",
              "774                                            is   \n",
              "775                             Due is similar to   \n",
              "776                                 is similar to   \n",
              "777                                 is similar to   \n",
              "778                             Due is similar to   \n",
              "779                                            is   \n",
              "780                             Due is similar to   \n",
              "781                                            is   \n",
              "782                                            is   \n",
              "783                                            is   \n",
              "784                             Due is similar to   \n",
              "785                                            is   \n",
              "786                                 is similar to   \n",
              "787                                 is similar to   \n",
              "788                                            is   \n",
              "789                                 is similar to   \n",
              "790                                            is   \n",
              "791                                            is   \n",
              "792                                            is   \n",
              "793                                            is   \n",
              "794                                            is   \n",
              "795                             Due is similar to   \n",
              "796                                            is   \n",
              "797                                            is   \n",
              "798                                            is   \n",
              "799                                 is similar to   \n",
              "800                                            is   \n",
              "801                                            is   \n",
              "802                             Due is similar to   \n",
              "803                                            is   \n",
              "804                                 is similar to   \n",
              "805                                            is   \n",
              "806                                            is   \n",
              "807                                 is similar to   \n",
              "808                                            is   \n",
              "809                                            is   \n",
              "810                             Due is similar to   \n",
              "811                                            is   \n",
              "812                                            is   \n",
              "813                                            is   \n",
              "814                                 is similar to   \n",
              "815                                            is   \n",
              "816                                 is similar to   \n",
              "817                                 is similar to   \n",
              "818                             Due is similar to   \n",
              "819                                            is   \n",
              "820                                 is similar to   \n",
              "821                                            is   \n",
              "822                                            is   \n",
              "823                                            is   \n",
              "824                                 is similar to   \n",
              "825                             Due is similar to   \n",
              "826                                      contains   \n",
              "827                                         is in   \n",
              "828                               Similarly allow   \n",
              "829                                         is in   \n",
              "830                                         allow   \n",
              "831                                       prevent   \n",
              "832                                       prevent   \n",
              "833                                         is in   \n",
              "834                                       prevent   \n",
              "835                                       prevent   \n",
              "836                                     implement   \n",
              "837                                     implement   \n",
              "838                                     implement   \n",
              "839                                       is with   \n",
              "840                               are same across   \n",
              "841                               are same across   \n",
              "842                                       FFN use   \n",
              "843                                           use   \n",
              "844                               are same across   \n",
              "845                       FFN use parameters from   \n",
              "846                                           use   \n",
              "847                                           are   \n",
              "848                       FFN use parameters from   \n",
              "849                           use parameters from   \n",
              "850                                           are   \n",
              "851                           use parameters from   \n",
              "852                               are same across   \n",
              "853                                       FFN use   \n",
              "854                                       is with   \n",
              "855                                           has   \n",
              "856                                           has   \n",
              "857                                            is   \n",
              "858                       convert input tokens to   \n",
              "859                      convert output tokens to   \n",
              "860                      convert output tokens to   \n",
              "861                                       convert   \n",
              "862                                           use   \n",
              "863                                       convert   \n",
              "864                                           use   \n",
              "865                       convert input tokens to   \n",
              "866                                           use   \n",
              "867                                      also use   \n",
              "868                                           use   \n",
              "869                                      also use   \n",
              "870                                         share   \n",
              "871                                         share   \n",
              "872                                         share   \n",
              "873                                         share   \n",
              "874                                         share   \n",
              "875                                         share   \n",
              "876                                         share   \n",
              "877                        share weight matrix In   \n",
              "878                                         share   \n",
              "879                                         share   \n",
              "880                                         share   \n",
              "881                                         share   \n",
              "882                                         share   \n",
              "883                           multiply weights In   \n",
              "884                                      multiply   \n",
              "885                           multiply weights by   \n",
              "886                                   must inject   \n",
              "887                                   must inject   \n",
              "888                                          make   \n",
              "889                                   must inject   \n",
              "890                                   must inject   \n",
              "891                                   must inject   \n",
              "892                                   must inject   \n",
              "893                                   must inject   \n",
              "894                                        use of   \n",
              "895                                   must inject   \n",
              "896                                        use of   \n",
              "897                                   must inject   \n",
              "898                                   must inject   \n",
              "899                                   must inject   \n",
              "900                                   must inject   \n",
              "901                                   must inject   \n",
              "902                                          make   \n",
              "903                                          make   \n",
              "904                                         is in   \n",
              "905                                   must inject   \n",
              "906                                   must inject   \n",
              "907                                   must inject   \n",
              "908                                   must inject   \n",
              "909                                   must inject   \n",
              "910                              add encodings to   \n",
              "911                              add encodings To   \n",
              "912                                           add   \n",
              "913                              add encodings at   \n",
              "914                                           add   \n",
              "915                                       size in   \n",
              "916                                       size in   \n",
              "917                                         is in   \n",
              "918                                            is   \n",
              "919                                       size of   \n",
              "920                                            is   \n",
              "921                                            is   \n",
              "922                                            is   \n",
              "923                                          have   \n",
              "924                                          have   \n",
              "925                                          have   \n",
              "926                                          have   \n",
              "927                                           can   \n",
              "928                         have dimension dmodel   \n",
              "929                                          have   \n",
              "930                                          have   \n",
              "931                                          have   \n",
              "932                                          have   \n",
              "933                         have dimension dmodel   \n",
              "934                                            is   \n",
              "935                                           use   \n",
              "936                                           use   \n",
              "937                                            is   \n",
              "938                                            is   \n",
              "939                         use sine functions In   \n",
              "940                                           use   \n",
              "941                                          form   \n",
              "942                                          form   \n",
              "943                                           can   \n",
              "944                                         allow   \n",
              "945                                  easily learn   \n",
              "946                                         chose   \n",
              "947                                         allow   \n",
              "948                                           can   \n",
              "949                                         learn   \n",
              "950                                         allow   \n",
              "951                                           can   \n",
              "952                                           can   \n",
              "953                                         learn   \n",
              "954                                         learn   \n",
              "955                                  easily learn   \n",
              "956                                           can   \n",
              "957                                chose function   \n",
              "958                                  easily learn   \n",
              "959                                           see   \n",
              "960                             also experimented   \n",
              "961                                  experimented   \n",
              "962                                           see   \n",
              "963                                extrapolate to   \n",
              "964                                         chose   \n",
              "965                                extrapolate to   \n",
              "966                                     may allow   \n",
              "967                                     may allow   \n",
              "968                                     may allow   \n",
              "969                                     may allow   \n",
              "970                                     may allow   \n",
              "971                                         chose   \n",
              "972                                extrapolate to   \n",
              "973                                 chose version   \n",
              "974                                     may allow   \n",
              "975                                extrapolate to   \n",
              "976                                extrapolate to   \n",
              "977                                     may allow   \n",
              "978                                       compare   \n",
              "979                            compare aspects to   \n",
              "980                                       zn with   \n",
              "981                            compare aspects to   \n",
              "982                            compare aspects to   \n",
              "983                                       compare   \n",
              "984                               In Attention is   \n",
              "985                            compare aspects to   \n",
              "986                                         is in   \n",
              "987                               In Attention is   \n",
              "988                                       compare   \n",
              "989                                    zn such as   \n",
              "990                                       compare   \n",
              "991                                 such as zn is   \n",
              "992                                         is In   \n",
              "993                            various aspects of   \n",
              "994                                       is with   \n",
              "995                            compare aspects to   \n",
              "996                                         xn to   \n",
              "997                                    aspects of   \n",
              "998                            compare aspects to   \n",
              "999                                    Motivating   \n",
              "1000                                   Motivating   \n",
              "1001                                     consider   \n",
              "1002                                           is   \n",
              "1003        is total computational complexity per   \n",
              "1004                      is total complexity per   \n",
              "1005                            is complexity per   \n",
              "1006                                           is   \n",
              "1007                                           is   \n",
              "1008              is computational complexity per   \n",
              "1009                                           is   \n",
              "1010                                           is   \n",
              "1011                                 is amount of   \n",
              "1012                                    is amount   \n",
              "1013                                           is   \n",
              "1014                                        is in   \n",
              "1015                                           is   \n",
              "1016                                           is   \n",
              "1017                                           is   \n",
              "1018                                           is   \n",
              "1019                          is key challenge in   \n",
              "1020                                        is in   \n",
              "1021                          is key challenge in   \n",
              "1022                              is challenge in   \n",
              "1023                                           is   \n",
              "1024                              is challenge in   \n",
              "1025                                  traverse in   \n",
              "1026                                         have   \n",
              "1027                                  traverse in   \n",
              "1028                                         have   \n",
              "1029                                         have   \n",
              "1030                                         have   \n",
              "1031                                        learn   \n",
              "1032                                        learn   \n",
              "1033                      connects positions with   \n",
              "1034                      connects positions with   \n",
              "1035                      connects positions with   \n",
              "1036                      connects positions with   \n",
              "1037                      connects positions with   \n",
              "1038                                     connects   \n",
              "1039                      connects positions with   \n",
              "1040                      connects positions with   \n",
              "1041                      connects positions with   \n",
              "1042                              are faster than   \n",
              "1043                                          are   \n",
              "1044                                          are   \n",
              "1045                                      is with   \n",
              "1046                                           is   \n",
              "1047                                are faster In   \n",
              "1048                                          are   \n",
              "1049                                          are   \n",
              "1050                                          are   \n",
              "1051                              is smaller than   \n",
              "1052                                are faster In   \n",
              "1053                                are faster In   \n",
              "1054                                           is   \n",
              "1055                                          are   \n",
              "1056                                           is   \n",
              "1057                              are faster than   \n",
              "1058                                          are   \n",
              "1059                                      improve   \n",
              "1060                                  considering   \n",
              "1061                                  considering   \n",
              "1062                  considering neighborhood in   \n",
              "1063                                  considering   \n",
              "1064                  considering neighborhood in   \n",
              "1065                                        could   \n",
              "1066                                      improve   \n",
              "1067                                      improve   \n",
              "1068                                  considering   \n",
              "1069                                      improve   \n",
              "1070                  considering neighborhood in   \n",
              "1071                  considering neighborhood in   \n",
              "1072                                  investigate   \n",
              "1073              investigate approach further in   \n",
              "1074                          investigate further   \n",
              "1075                      investigate approach in   \n",
              "1076                                      is with   \n",
              "1077                                   increasing   \n",
              "1078                                    length of   \n",
              "1079                                   increasing   \n",
              "1080                                        is in   \n",
              "1081                                     requires   \n",
              "1082                                    length of   \n",
              "1083                                     requires   \n",
              "1084                                   increasing   \n",
              "1085                                     requires   \n",
              "1086                                          are   \n",
              "1087         are generally expensive however than   \n",
              "1088                                          are   \n",
              "1089           are generally expensive however by   \n",
              "1090            are generally more expensive than   \n",
              "1091                                          are   \n",
              "1092                                          are   \n",
              "1093                                          are   \n",
              "1094      are generally more expensive however by   \n",
              "1095                                          are   \n",
              "1096      are generally more expensive however by   \n",
              "1097                                          are   \n",
              "1098                                          are   \n",
              "1099                are more expensive however by   \n",
              "1100                   are generally expensive by   \n",
              "1101                                          are   \n",
              "1102           are generally expensive however by   \n",
              "1103                                          are   \n",
              "1104                                          are   \n",
              "1105                                          are   \n",
              "1106                     are expensive however by   \n",
              "1107                                          are   \n",
              "1108           are generally expensive however by   \n",
              "1109                                          are   \n",
              "1110                           are expensive than   \n",
              "1111                                          are   \n",
              "1112                                          are   \n",
              "1113                                          are   \n",
              "1114                                          are   \n",
              "1115                   are expensive however than   \n",
              "1116              are generally more expensive by   \n",
              "1117         are generally expensive however than   \n",
              "1118                                          are   \n",
              "1119                                          are   \n",
              "1120                                          are   \n",
              "1121                                          are   \n",
              "1122                                          are   \n",
              "1123                 are generally expensive than   \n",
              "1124                                          are   \n",
              "1125                 are generally expensive than   \n",
              "1126                                          are   \n",
              "1127                                          are   \n",
              "1128            are generally more expensive than   \n",
              "1129                                          are   \n",
              "1130                 are generally expensive than   \n",
              "1131                     are expensive however by   \n",
              "1132                                          are   \n",
              "1133                                          are   \n",
              "1134           are generally expensive however by   \n",
              "1135              are generally more expensive by   \n",
              "1136                                          are   \n",
              "1137                                          are   \n",
              "1138            are generally more expensive than   \n",
              "1139                                          are   \n",
              "1140                                          are   \n",
              "1141                   are expensive however than   \n",
              "1142                             are expensive by   \n",
              "1143                                          are   \n",
              "1144                 are generally expensive than   \n",
              "1145                             are expensive by   \n",
              "1146                                          are   \n",
              "1147                                          are   \n",
              "1148                                          are   \n",
              "1149                                          are   \n",
              "1150                are more expensive however by   \n",
              "1151                                          are   \n",
              "1152                                          are   \n",
              "1153                are more expensive however by   \n",
              "1154                                          are   \n",
              "1155                      are more expensive than   \n",
              "1156         are generally expensive however than   \n",
              "1157      are generally more expensive however by   \n",
              "1158                                          are   \n",
              "1159            are generally more expensive than   \n",
              "1160                                          are   \n",
              "1161                                          are   \n",
              "1162                     are expensive however by   \n",
              "1163                                          are   \n",
              "1164                                          are   \n",
              "1165                                          are   \n",
              "1166    are generally more expensive however than   \n",
              "1167                                          are   \n",
              "1168                             are expensive by   \n",
              "1169                                          are   \n",
              "1170      are generally more expensive however by   \n",
              "1171      are generally more expensive however by   \n",
              "1172                           are expensive than   \n",
              "1173              are more expensive however than   \n",
              "1174                        are more expensive by   \n",
              "1175                                          are   \n",
              "1176                                          are   \n",
              "1177                                          are   \n",
              "1178                                          are   \n",
              "1179                   are generally expensive by   \n",
              "1180                                          are   \n",
              "1181                           are expensive than   \n",
              "1182                                          are   \n",
              "1183                           are expensive than   \n",
              "1184                                          are   \n",
              "1185                     are expensive however by   \n",
              "1186                                          are   \n",
              "1187                                          are   \n",
              "1188                                          are   \n",
              "1189                      are more expensive than   \n",
              "1190                                          are   \n",
              "1191                                          are   \n",
              "1192                      are more expensive than   \n",
              "1193                   are generally expensive by   \n",
              "1194                                          are   \n",
              "1195                                          are   \n",
              "1196              are generally more expensive by   \n",
              "1197                             are expensive by   \n",
              "1198                                          are   \n",
              "1199                                          are   \n",
              "1200                   are generally expensive by   \n",
              "1201    are generally more expensive however than   \n",
              "1202                                          are   \n",
              "1203      are generally more expensive however by   \n",
              "1204                                          are   \n",
              "1205                                          are   \n",
              "1206                   are generally expensive by   \n",
              "1207                                          are   \n",
              "1208                                          are   \n",
              "1209                                          are   \n",
              "1210                     are expensive however by   \n",
              "1211                                          are   \n",
              "1212                                          are   \n",
              "1213                      are more expensive than   \n",
              "1214                                          are   \n",
              "1215                                          are   \n",
              "1216         are generally expensive however than   \n",
              "1217           are generally expensive however by   \n",
              "1218                        are more expensive by   \n",
              "1219                                          are   \n",
              "1220                   are expensive however than   \n",
              "1221                   are generally expensive by   \n",
              "1222                                          are   \n",
              "1223              are generally more expensive by   \n",
              "1224    are generally more expensive however than   \n",
              "1225                                          are   \n",
              "1226           are generally expensive however by   \n",
              "1227                                          are   \n",
              "1228                                          are   \n",
              "1229                        are more expensive by   \n",
              "1230                                          are   \n",
              "1231              are generally more expensive by   \n",
              "1232                                          are   \n",
              "1233                                          are   \n",
              "1234                                          are   \n",
              "1235                                          are   \n",
              "1236                are more expensive however by   \n",
              "1237                                          are   \n",
              "1238                                          are   \n",
              "1239                                          are   \n",
              "1240                                          are   \n",
              "1241                                          are   \n",
              "1242                                          are   \n",
              "1243                                          are   \n",
              "1244                     are expensive however by   \n",
              "1245              are more expensive however than   \n",
              "1246                                          are   \n",
              "1247                                          are   \n",
              "1248                                          are   \n",
              "1249                                          are   \n",
              "1250                        are more expensive by   \n",
              "1251                                          are   \n",
              "1252                                          are   \n",
              "1253                        are more expensive by   \n",
              "1254                are more expensive however by   \n",
              "1255                                          are   \n",
              "1256                             are expensive by   \n",
              "1257                                          are   \n",
              "1258              are more expensive however than   \n",
              "1259                        are more expensive by   \n",
              "1260                                          are   \n",
              "1261                                          are   \n",
              "1262              are generally more expensive by   \n",
              "1263                             are expensive by   \n",
              "1264                                          are   \n",
              "1265                                          are   \n",
              "1266                are more expensive however by   \n",
              "1267              are more expensive however than   \n",
              "1268                                          are   \n",
              "1269                                          are   \n",
              "1270                                          are   \n",
              "1271                                          are   \n",
              "1272                                          are   \n",
              "1273                                          are   \n",
              "1274                   are expensive however than   \n",
              "1275    are generally more expensive however than   \n",
              "1276                                          are   \n",
              "1277                                          are   \n",
              "1278                                  is equal to   \n",
              "1279                                           is   \n",
              "1280                                           is   \n",
              "1281                                           is   \n",
              "1282                                           is   \n",
              "1283                                           is   \n",
              "1284                                           is   \n",
              "1285                                           is   \n",
              "1286                                is equal with   \n",
              "1287                                is equal with   \n",
              "1288                                           is   \n",
              "1289                        however is equal with   \n",
              "1290                          however is equal to   \n",
              "1291                                           is   \n",
              "1292                                           is   \n",
              "1293                                  is equal to   \n",
              "1294                        however is equal with   \n",
              "1295                          however is equal to   \n",
              "1296                                  could yield   \n",
              "1297                                  could yield   \n",
              "1298                                      inspect   \n",
              "1299         inspect attention distributions from   \n",
              "1300                                        is in   \n",
              "1301                                      discuss   \n",
              "1302                                      discuss   \n",
              "1303                                      exhibit   \n",
              "1304                                   related to   \n",
              "1305                                      exhibit   \n",
              "1306                                      exhibit   \n",
              "1307                                      exhibit   \n",
              "1308                                   related to   \n",
              "1309                                      exhibit   \n",
              "1310                                   related to   \n",
              "1311                                   related to   \n",
              "1312                                      exhibit   \n",
              "1313                                   trained on   \n",
              "1314                                   trained on   \n",
              "1315                                   trained on   \n",
              "1316                                   trained on   \n",
              "1317                                   trained on   \n",
              "1318                                   trained on   \n",
              "1319                                   trained on   \n",
              "1320                                   trained on   \n",
              "1321                                   trained on   \n",
              "1322                                   trained on   \n",
              "1323                                   trained on   \n",
              "1324                                   trained on   \n",
              "1325                                   trained on   \n",
              "1326                                   trained on   \n",
              "1327                                   trained on   \n",
              "1328                                   trained on   \n",
              "1329                                   trained on   \n",
              "1330                                   trained on   \n",
              "1331                                   trained on   \n",
              "1332                                   trained on   \n",
              "1333                                   trained on   \n",
              "1334                                   trained on   \n",
              "1335                                   trained on   \n",
              "1336                                   trained on   \n",
              "1337                                        using   \n",
              "1338                                         were   \n",
              "1339                                         used   \n",
              "1340                                     used For   \n",
              "1341                                         used   \n",
              "1342                                         used   \n",
              "1343                                         used   \n",
              "1344                                         used   \n",
              "1345                                         used   \n",
              "1346                                         used   \n",
              "1347                                         used   \n",
              "1348                                         used   \n",
              "1349                                         used   \n",
              "1350                                         used   \n",
              "1351                                         used   \n",
              "1352                                         used   \n",
              "1353                                         used   \n",
              "1354                                     used For   \n",
              "1355                                         used   \n",
              "1356                                         used   \n",
              "1357                                    used into   \n",
              "1358                                         used   \n",
              "1359                                         used   \n",
              "1360                     were batched together by   \n",
              "1361                              were batched by   \n",
              "1362                              were batched by   \n",
              "1363                                         were   \n",
              "1364                     were batched together by   \n",
              "1365                                         were   \n",
              "1366                                    contained   \n",
              "1367                                    contained   \n",
              "1368                                      trained   \n",
              "1369                                         took   \n",
              "1370                         described throughout   \n",
              "1371                                        using   \n",
              "1372                                      trained   \n",
              "1373                                 described on   \n",
              "1374                                          was   \n",
              "1375                                 described on   \n",
              "1376                                 described on   \n",
              "1377                                          was   \n",
              "1378                                 described on   \n",
              "1379                                          was   \n",
              "1380                                         were   \n",
              "1381                             were trained for   \n",
              "1382                             were trained for   \n",
              "1383                             were trained for   \n",
              "1384                             were trained for   \n",
              "1385                                         were   \n",
              "1386                                         used   \n",
              "1387                                     training   \n",
              "1388                                     training   \n",
              "1389                                       varied   \n",
              "1390         decreasing thereafter proportionally   \n",
              "1391                    decreasing proportionally   \n",
              "1392                                   decreasing   \n",
              "1393                                         used   \n",
              "1394                                       employ   \n",
              "1395                                       employ   \n",
              "1396                                        apply   \n",
              "1397                                        is in   \n",
              "1398                             apply dropout In   \n",
              "1399                                        apply   \n",
              "1400                             apply dropout to   \n",
              "1401                             apply dropout to   \n",
              "1402                             apply dropout to   \n",
              "1403                             apply dropout to   \n",
              "1404                                          use   \n",
              "1405                                          use   \n",
              "1406                                 use rate For   \n",
              "1407                                     employed   \n",
              "1408                                     Training   \n",
              "1409                                     Training   \n",
              "1410                                     employed   \n",
              "1411                                       learns   \n",
              "1412                                       learns   \n",
              "1413                            to Translation is   \n",
              "1414                            On Translation is   \n",
              "1415                                           is   \n",
              "1416                                 is listed in   \n",
              "1417                                 is listed in   \n",
              "1418                                 is listed in   \n",
              "1419                                 is listed in   \n",
              "1420                                      took on   \n",
              "1421                                 took at_time   \n",
              "1422                       achieves BLEU score On   \n",
              "1423                       achieves BLEU score On   \n",
              "1424                                     achieves   \n",
              "1425                       achieves BLEU score On   \n",
              "1426                                     achieves   \n",
              "1427                                     achieves   \n",
              "1428                       achieves BLEU score On   \n",
              "1429                                     achieves   \n",
              "1430                       achieves BLEU score On   \n",
              "1431                       achieves BLEU score On   \n",
              "1432                               used model For   \n",
              "1433                                         used   \n",
              "1434                                     used For   \n",
              "1435                                         used   \n",
              "1436                                         used   \n",
              "1437                                         used   \n",
              "1438                                     averaged   \n",
              "1439                                     averaged   \n",
              "1440                                    used with   \n",
              "1441                                         used   \n",
              "1442                        used beam search with   \n",
              "1443                                         were   \n",
              "1444                            were chosen after   \n",
              "1445                            were chosen after   \n",
              "1446                                   summarizes   \n",
              "1447                                     compares   \n",
              "1448                   compares training costs to   \n",
              "1449                                     compares   \n",
              "1450                                   summarizes   \n",
              "1451                   compares training costs to   \n",
              "1452                                     compares   \n",
              "1453                   compares training costs to   \n",
              "1454                                     compares   \n",
              "1455                   compares training costs to   \n",
              "1456                   compares training costs to   \n",
              "1457                   compares training costs to   \n",
              "1458                   compares training costs to   \n",
              "1459                   compares training costs to   \n",
              "1460                                     estimate   \n",
              "1461                                     estimate   \n",
              "1462                                     estimate   \n",
              "1463                                     estimate   \n",
              "1464                                     estimate   \n",
              "1465                                     estimate   \n",
              "1466                                     estimate   \n",
              "1467                                     estimate   \n",
              "1468                                     estimate   \n",
              "1469                                     estimate   \n",
              "1470                                     estimate   \n",
              "1471                                     estimate   \n",
              "1472                                    measuring   \n",
              "1473                                    measuring   \n",
              "1474                                    change on   \n",
              "1475                                    measuring   \n",
              "1476                                    measuring   \n",
              "1477                                    measuring   \n",
              "1478                                    measuring   \n",
              "1479                                    measuring   \n",
              "1480                                    measuring   \n",
              "1481                                    measuring   \n",
              "1482                                    change to   \n",
              "1483                                        is in   \n",
              "1484                                    varied in   \n",
              "1485                                    measuring   \n",
              "1486                                    change in   \n",
              "1487                                    measuring   \n",
              "1488                                    measuring   \n",
              "1489                                    measuring   \n",
              "1490                                    measuring   \n",
              "1491                                    measuring   \n",
              "1492                                       varied   \n",
              "1493                                    measuring   \n",
              "1494                                    measuring   \n",
              "1495                                    varied in   \n",
              "1496                                    change to   \n",
              "1497                                    measuring   \n",
              "1498                                    measuring   \n",
              "1499                                    change to   \n",
              "1500                                    change to   \n",
              "1501                                    measuring   \n",
              "1502                                 described in   \n",
              "1503                                 described in   \n",
              "1504                                      present   \n",
              "1505                                        is in   \n",
              "1506                                      present   \n",
              "1507                                      vary In   \n",
              "1508                               keeping amount   \n",
              "1509                               keeping amount   \n",
              "1510                               vary number In   \n",
              "1511                                         vary   \n",
              "1512                                         vary   \n",
              "1513                                         vary   \n",
              "1514                        vary attention key In   \n",
              "1515                                      keeping   \n",
              "1516                                      keeping   \n",
              "1517                                is worse than   \n",
              "1518                                           is   \n",
              "1519                                is worse than   \n",
              "1520                                           is   \n",
              "1521                                           is   \n",
              "1522                          also drops off with   \n",
              "1523                                           is   \n",
              "1524                               drops off with   \n",
              "1525                                     is worse   \n",
              "1526                          also drops off with   \n",
              "1527                                is worse than   \n",
              "1528                                           is   \n",
              "1529                               drops off with   \n",
              "1530                          also drops off with   \n",
              "1531                                is worse than   \n",
              "1532                                           is   \n",
              "1533                               drops off with   \n",
              "1534                                     is worse   \n",
              "1535                                     used for   \n",
              "1536                        used respectively for   \n",
              "1537                                         used   \n",
              "1538                              used values for   \n",
              "1539                 used values respectively for   \n",
              "1540                            used respectively   \n",
              "1541                             are identical to   \n",
              "1542                                          are   \n",
              "1543                             are identical to   \n",
              "1544                                          are   \n",
              "1545                             are identical to   \n",
              "1546                             are identical to   \n",
              "1547                                       are on   \n",
              "1548                                       are on   \n",
              "1549                                       are on   \n",
              "1550                                      are per   \n",
              "1551                                      are per   \n",
              "1552                                     reducing   \n",
              "1553                                     reducing   \n",
              "1554                                          dev   \n",
              "1555                                     reducing   \n",
              "1556                                     reducing   \n",
              "1557                           further observe in   \n",
              "1558                                   observe in   \n",
              "1559                           observe results to   \n",
              "1560                                      observe   \n",
              "1561                                      replace   \n",
              "1562                                      observe   \n",
              "1563                                      observe   \n",
              "1564                                      replace   \n",
              "1565                                      replace   \n",
              "1566                                      replace   \n",
              "1567                                    replacing   \n",
              "1568                                    replacing   \n",
              "1569                                    replacing   \n",
              "1570                            based entirely on   \n",
              "1571                                        is In   \n",
              "1572                            based entirely on   \n",
              "1573                                    replacing   \n",
              "1574                                       headed   \n",
              "1575                                     based on   \n",
              "1576                                     based on   \n",
              "1577                                          can   \n",
              "1578                                          can   \n",
              "1579                                          can   \n",
              "1580                                          can   \n",
              "1581                                          can   \n",
              "1582                                          can   \n",
              "1583                                          can   \n",
              "1584                                          can   \n",
              "1585                                          can   \n",
              "1586                                          can   \n",
              "1587                                  outperforms   \n",
              "1588                                  outperforms   \n",
              "1589                                  outperforms   \n",
              "1590                                  outperforms   \n",
              "1591                            are excited about   \n",
              "1592                                        apply   \n",
              "1593                            are excited about   \n",
              "1594                                          are   \n",
              "1595                                    involving   \n",
              "1596                        extend Transformer to   \n",
              "1597                                    involving   \n",
              "1598                                    involving   \n",
              "1599                                       extend   \n",
              "1600                         large inputs such as   \n",
              "1601                               inputs such as   \n",
              "1602                              is available at   \n",
              "1603                                        train   \n",
              "1604                                           is   \n",
              "1605                                          are   \n",
              "1606                             are grateful for   \n",
              "1607                             are grateful for   \n",
              "1608                                          are   \n",
              "1609                              are grateful to   \n",
              "1610                                          are   \n",
              "\n",
              "                                                                                                        object  \\\n",
              "0                                                                                                          All   \n",
              "1                                                                                                      encoder   \n",
              "2                                                                                                      decoder   \n",
              "3                                                                                                     you Need   \n",
              "4                                                                                                    Attention   \n",
              "5                                                                                                      encoder   \n",
              "6                                                                                          attention mechanism   \n",
              "7                                                                                          attention mechanism   \n",
              "8                                                                                          attention mechanism   \n",
              "9                                                                                                      encoder   \n",
              "10                                                                                         attention mechanism   \n",
              "11                                                                                                     encoder   \n",
              "12                                                                                         attention mechanism   \n",
              "13                                                                                                     decoder   \n",
              "14                                                                                                     encoder   \n",
              "15                                                                                                     decoder   \n",
              "16                                                                                         attention mechanism   \n",
              "17                                                                                         attention mechanism   \n",
              "18                                                                                         attention mechanism   \n",
              "19                                                                                                     decoder   \n",
              "20                                                                                                     decoder   \n",
              "21                        network architecture based solely on attention mechanisms dispensing with recurrence   \n",
              "22                                  new network architecture based on attention mechanisms dispensing entirely   \n",
              "23                                               new network architecture based solely on attention mechanisms   \n",
              "24                 simple network architecture based solely on attention mechanisms dispensing with recurrence   \n",
              "25                                                   simple network architecture based on attention mechanisms   \n",
              "26                                                          network architecture based on attention mechanisms   \n",
              "27                      network architecture based on attention mechanisms dispensing with recurrence entirely   \n",
              "28                                                      new network architecture based on attention mechanisms   \n",
              "29                                                                                        network architecture   \n",
              "30                                                                                                 Transformer   \n",
              "31                        simple network architecture based on attention mechanisms dispensing with recurrence   \n",
              "32                                    new network architecture based solely on attention mechanisms dispensing   \n",
              "33                           new simple network architecture based on attention mechanisms dispensing entirely   \n",
              "34           new network architecture based solely on attention mechanisms dispensing with recurrence entirely   \n",
              "35                                               network architecture based on attention mechanisms dispensing   \n",
              "36                  new network architecture based on attention mechanisms dispensing with recurrence entirely   \n",
              "37                                                                             new simple network architecture   \n",
              "38               network architecture based solely on attention mechanisms dispensing with recurrence entirely   \n",
              "39                        simple network architecture based solely on attention mechanisms dispensing entirely   \n",
              "40                           new network architecture based solely on attention mechanisms dispensing entirely   \n",
              "41                                 simple network architecture based solely on attention mechanisms dispensing   \n",
              "42                                      network architecture based on attention mechanisms dispensing entirely   \n",
              "43                    new simple network architecture based solely on attention mechanisms dispensing entirely   \n",
              "44                                                                                 simple network architecture   \n",
              "45                                                   network architecture based solely on attention mechanisms   \n",
              "46             new simple network architecture based solely on attention mechanisms dispensing with recurrence   \n",
              "47    new simple network architecture based solely on attention mechanisms dispensing with recurrence entirely   \n",
              "48                               network architecture based solely on attention mechanisms dispensing entirely   \n",
              "49                                        network architecture based solely on attention mechanisms dispensing   \n",
              "50                                    new simple network architecture based on attention mechanisms dispensing   \n",
              "51                               simple network architecture based on attention mechanisms dispensing entirely   \n",
              "52        simple network architecture based solely on attention mechanisms dispensing with recurrence entirely   \n",
              "53               simple network architecture based on attention mechanisms dispensing with recurrence entirely   \n",
              "54           new simple network architecture based on attention mechanisms dispensing with recurrence entirely   \n",
              "55                           new network architecture based on attention mechanisms dispensing with recurrence   \n",
              "56                                               new simple network architecture based on attention mechanisms   \n",
              "57                                        simple network architecture based on attention mechanisms dispensing   \n",
              "58                                           new network architecture based on attention mechanisms dispensing   \n",
              "59                    new simple network architecture based on attention mechanisms dispensing with recurrence   \n",
              "60                    new network architecture based solely on attention mechanisms dispensing with recurrence   \n",
              "61                                            simple network architecture based solely on attention mechanisms   \n",
              "62                                                                                    new network architecture   \n",
              "63                                        new simple network architecture based solely on attention mechanisms   \n",
              "64                               network architecture based on attention mechanisms dispensing with recurrence   \n",
              "65                             new simple network architecture based solely on attention mechanisms dispensing   \n",
              "66                                                                   existing best results including ensembles   \n",
              "67                                                                                      results by over 2 BLEU   \n",
              "68                                                                                 best results by over 2 BLEU   \n",
              "69                                                    existing best results including ensembles by over 2 BLEU   \n",
              "70                                                                             existing results by over 2 BLEU   \n",
              "71                                                                        existing best results by over 2 BLEU   \n",
              "72                                                                            best results including ensembles   \n",
              "73                                                                  results including ensembles by over 2 BLEU   \n",
              "74                                                                        existing results including ensembles   \n",
              "75                                                     28.4 BLEU on WMT 2014 Englishto German translation task   \n",
              "76                                                                                                     results   \n",
              "77                                                                                       existing best results   \n",
              "78                                                             best results including ensembles by over 2 BLEU   \n",
              "79                                                                                                   28.4 BLEU   \n",
              "80                                                                                 results including ensembles   \n",
              "81                                                                                                best results   \n",
              "82                                                                                            existing results   \n",
              "83                                                         existing results including ensembles by over 2 BLEU   \n",
              "84                                                                  small fraction of training costs of models   \n",
              "85                                                                   new model state of art BLEU score of 41.0   \n",
              "86                                                                                      new single model state   \n",
              "87                                                                        single model state of art BLEU score   \n",
              "88                                                                       model state of art BLEU score of 41.0   \n",
              "89                                                            new single model state of art BLEU score of 41.0   \n",
              "90                                                                                          single model state   \n",
              "91                                                                                                    training   \n",
              "92                                                                                             new model state   \n",
              "93                                                  small fraction of training costs of models from literature   \n",
              "94                                                                                                 model state   \n",
              "95                                                                               model state of art BLEU score   \n",
              "96                                                                single model state of art BLEU score of 41.0   \n",
              "97                                                                                  fraction of training costs   \n",
              "98                                                                                            WMT 2014 English   \n",
              "99                                                                        WMT 2014 English to translation task   \n",
              "100                                                            small fraction of training costs of best models   \n",
              "101                                                                                                   fraction   \n",
              "102                                                  fraction of training costs of best models from literature   \n",
              "103                                            small fraction of training costs of best models from literature   \n",
              "104                                                                           small fraction of training costs   \n",
              "105                                                                          new model state of art BLEU score   \n",
              "106                                                                                             small fraction   \n",
              "107                                                       fraction of training costs of models from literature   \n",
              "108                                                                 fraction of training costs from literature   \n",
              "109                                                                       WMT 2014 English to translation task   \n",
              "110                                                                WMT 2014 English to French translation task   \n",
              "111                                                                  fraction of training costs of best models   \n",
              "112                                                                       fraction of training costs of models   \n",
              "113                                                                WMT 2014 English to French translation task   \n",
              "114                                                           small fraction of training costs from literature   \n",
              "115                                                                                                   training   \n",
              "116                                                                                           WMT 2014 English   \n",
              "117                                                                   new single model state of art BLEU score   \n",
              "118                                                                                                 particular   \n",
              "119                                                                    boundaries of recurrent language models   \n",
              "120                                                                                                 boundaries   \n",
              "121                                                                    boundaries of recurrent language models   \n",
              "122                                                                              boundaries of language models   \n",
              "123                                                                              boundaries of language models   \n",
              "124                                                                                                 boundaries   \n",
              "125                                                                                                     random   \n",
              "126                                                                                                     random   \n",
              "127                                                                                                       RNNs   \n",
              "128                                                                                             self attention   \n",
              "129                                                                                             Illia designed   \n",
              "130                                                                     other person involved in nearly detail   \n",
              "131                                                                           person involved in nearly detail   \n",
              "132                                                                                                     person   \n",
              "133                                                                                                     detail   \n",
              "134                                                                                              nearly detail   \n",
              "135                                                                                       multi head attention   \n",
              "136                                                                                             head attention   \n",
              "137                                                                                  person involved in detail   \n",
              "138                                                                               scaled dot product attention   \n",
              "139                                                                                                     detail   \n",
              "140                                                                     parameter free position representation   \n",
              "141                                                                                      dot product attention   \n",
              "142                                                                            other person involved in detail   \n",
              "143                                                                          parameter position representation   \n",
              "144                                                                                              nearly detail   \n",
              "145                                                                                               other person   \n",
              "146                                                                                      our original codebase   \n",
              "147                                                                                       novel model variants   \n",
              "148                                                                                             model variants   \n",
              "149                                                                                       novel model variants   \n",
              "150                                                                                             model variants   \n",
              "151                                                                                                       days   \n",
              "152                                                                                       our earlier codebase   \n",
              "153                                                                                               our codebase   \n",
              "154                                                                                                       days   \n",
              "155                                                                                               our research   \n",
              "156                                                                             various parts of tensor2tensor   \n",
              "157                                                                                        countless long days   \n",
              "158                                                                                               our research   \n",
              "159                                                                                                  long days   \n",
              "160                                                                                              various parts   \n",
              "161                                                                                     parts of tensor2tensor   \n",
              "162                                                                                             countless days   \n",
              "163                                                                                             countless days   \n",
              "164                                                                                                  long days   \n",
              "165                                                                                        countless long days   \n",
              "166                                                                                                      parts   \n",
              "167                                                                                            Google Research   \n",
              "168                                                                                                 Long Beach   \n",
              "169                                                                                                         CA   \n",
              "170                                                                                                       2017   \n",
              "171                                                                                                 Long Beach   \n",
              "172                                                                                                       2017   \n",
              "173                                                                                                         CA   \n",
              "174                                                      computation along symbol positions of input sequences   \n",
              "175                                                      computation along symbol positions of input sequences   \n",
              "176                                                      computation along symbol positions of input sequences   \n",
              "177                                                                         computation along symbol positions   \n",
              "178                                                                                                computation   \n",
              "179                                                                                                computation   \n",
              "180                                                                                                computation   \n",
              "181                                                      computation along symbol positions of input sequences   \n",
              "182                                                                         computation along symbol positions   \n",
              "183                                                                         computation along symbol positions   \n",
              "184                                                                                                computation   \n",
              "185                                                                         computation along symbol positions   \n",
              "186                                                                                                      steps   \n",
              "187                                                                                  steps in computation time   \n",
              "188                                                                                                  positions   \n",
              "189                                                                            function of previous state ht 1   \n",
              "190                                                                                                   function   \n",
              "191                                                                                           computation time   \n",
              "192                                                                                     function of state ht 1   \n",
              "193                                                                     function of previous hidden state ht 1   \n",
              "194                                                                              function of hidden state ht 1   \n",
              "195                                                                                                   batching   \n",
              "196                                                                   parallelization within training examples   \n",
              "197                                                                                            parallelization   \n",
              "198                                                                                   memory constraints limit   \n",
              "199                                                                                                   examples   \n",
              "200                                                                   parallelization within training examples   \n",
              "201                                                                                   memory constraints limit   \n",
              "202                                                                                            parallelization   \n",
              "203                                                                                   batching across examples   \n",
              "204                                                                                                     latter   \n",
              "205                                                                                                     latter   \n",
              "206                                                                                 improvements in efficiency   \n",
              "207                                                                                                     latter   \n",
              "208                                                                                                     latter   \n",
              "209                                                       significant improvements in computational efficiency   \n",
              "210                                                       significant improvements in computational efficiency   \n",
              "211                                                                                    factorization tricks 18   \n",
              "212                                                                                          model performance   \n",
              "213                                                                                   significant improvements   \n",
              "214                                                                                          model performance   \n",
              "215                                                                     significant improvements in efficiency   \n",
              "216                                                                   improvements in computational efficiency   \n",
              "217                                                                                    factorization tricks 18   \n",
              "218                                                                     significant improvements in efficiency   \n",
              "219                                                                                   significant improvements   \n",
              "220                                                                                          model performance   \n",
              "221                                                                                          model performance   \n",
              "222                                                                   improvements in computational efficiency   \n",
              "223                                                                                   computational efficiency   \n",
              "224                                                                                               improvements   \n",
              "225                                                                                               improvements   \n",
              "226                                                                                 improvements in efficiency   \n",
              "227                                                                                    allowing without regard   \n",
              "228                                                                                                   modeling   \n",
              "229                                                                         sequence modeling in various tasks   \n",
              "230                                                                      compelling sequence modeling in tasks   \n",
              "231                                                                                              various tasks   \n",
              "232                                                                         part of sequence modeling in tasks   \n",
              "233                                                                                                     regard   \n",
              "234                                                                                   regard to their distance   \n",
              "235                                             integral part of compelling sequence modeling in various tasks   \n",
              "236                                                                                          sequence modeling   \n",
              "237                                                                      compelling sequence modeling in tasks   \n",
              "238                                                      part of compelling sequence modeling in various tasks   \n",
              "239                                                                                              integral part   \n",
              "240                                                                regard to their distance in input sequences   \n",
              "241                                                                regard to their distance in input sequences   \n",
              "242                                                                  allowing without regard to their distance   \n",
              "243                                                                                   modeling of dependencies   \n",
              "244                                                              integral part of compelling sequence modeling   \n",
              "245                                                                                                       part   \n",
              "246                                                                integral part of sequence modeling in tasks   \n",
              "247                                                                                   regard to their distance   \n",
              "248                                                                               compelling sequence modeling   \n",
              "249                                                                                            input sequences   \n",
              "250                                                                                 sequence modeling in tasks   \n",
              "251                                                                         integral part of sequence modeling   \n",
              "252                                                                       part of compelling sequence modeling   \n",
              "253                                                                               compelling sequence modeling   \n",
              "254                                                                                                   allowing   \n",
              "255                                                                 part of sequence modeling in various tasks   \n",
              "256                                                     integral part of compelling sequence modeling in tasks   \n",
              "257                                                                                          sequence modeling   \n",
              "258                                                        integral part of sequence modeling in various tasks   \n",
              "259                                                                         sequence modeling in various tasks   \n",
              "260                                                              compelling sequence modeling in various tasks   \n",
              "261                                                              compelling sequence modeling in various tasks   \n",
              "262                                                                                                     regard   \n",
              "263                                               allowing without regard to their distance in input sequences   \n",
              "264                                                                                  part of sequence modeling   \n",
              "265                                                              part of compelling sequence modeling in tasks   \n",
              "266                                                                                 sequence modeling in tasks   \n",
              "267                                                                                               however used   \n",
              "268                                                                         conjunction with recurrent network   \n",
              "269                                                                                                conjunction   \n",
              "270                                                                                   conjunction with network   \n",
              "271                                                                                   conjunction with network   \n",
              "272                                                                         conjunction with recurrent network   \n",
              "273                                                                                                conjunction   \n",
              "274                                                                                                conjunction   \n",
              "275                                                                                                conjunction   \n",
              "276                                                                                          recurrent network   \n",
              "277                                                                                                       used   \n",
              "278                                                                         conjunction with recurrent network   \n",
              "279                                                                                               however used   \n",
              "280                                                                                   conjunction with network   \n",
              "281                                                                         conjunction with recurrent network   \n",
              "282                                                                                   conjunction with network   \n",
              "283                                                                                                       used   \n",
              "284                                                                                                       work   \n",
              "285                                                                                                    relying   \n",
              "286                                                                                                Transformer   \n",
              "287                                                                                            instead relying   \n",
              "288                                                                                           relying entirely   \n",
              "289                                                                                         model architecture   \n",
              "290                                                                                   instead relying entirely   \n",
              "291                                                                                                       work   \n",
              "292                                                                                        translation quality   \n",
              "293                                                                                                    trained   \n",
              "294                                                                      trained for as little as twelve hours   \n",
              "295                                                                                           new state of art   \n",
              "296                                                                                            parallelization   \n",
              "297                                                                                                      state   \n",
              "298                                                   trained for as little as twelve hours on eight P100 GPUs   \n",
              "299                                                                                               state of art   \n",
              "300                                                                                       more parallelization   \n",
              "301                                                                                                  new state   \n",
              "302                                                                         significantly more parallelization   \n",
              "303                                                                                                   networks   \n",
              "304                                                                                       basic building block   \n",
              "305                                                                                     convolutional networks   \n",
              "306                                                                              convolutional neural networks   \n",
              "307                                                                                             building block   \n",
              "308                                                                                            neural networks   \n",
              "309                                                                                                     models   \n",
              "310                                                                                           linearly ConvS2S   \n",
              "311                                                                                                   distance   \n",
              "312                                                                                 distance between positions   \n",
              "313                                                                                                    ConvS2S   \n",
              "314                                                                  dependencies between distant positions 11   \n",
              "315                                                                                               positions 11   \n",
              "316                                                                                               dependencies   \n",
              "317                                                                          dependencies between positions 11   \n",
              "318                                                                                       distant positions 11   \n",
              "319                                                                                        different positions   \n",
              "320                                                                                      positions of sequence   \n",
              "321                                                                                        attention mechanism   \n",
              "322                                                                               positions of single sequence   \n",
              "323                                                                     different positions of single sequence   \n",
              "324                                                                            different positions of sequence   \n",
              "325                                                                                                  positions   \n",
              "326                                                                                      has used successfully   \n",
              "327                                                                                                   has used   \n",
              "328                                                first transduction model relying entirely on self attention   \n",
              "329                                                                                 transduction model relying   \n",
              "330                                                                                   first transduction model   \n",
              "331                                                               transduction model relying on self attention   \n",
              "332                                                      transduction model relying entirely on self attention   \n",
              "333                                                                           first transduction model relying   \n",
              "334                                                         first transduction model relying on self attention   \n",
              "335                                                                  first transduction model relying entirely   \n",
              "336                                                                                         transduction model   \n",
              "337                                                                        transduction model relying entirely   \n",
              "338                                                                                             its advantages   \n",
              "339                                                                                                Transformer   \n",
              "340                                                                                         following sections   \n",
              "341                                                                                                   sections   \n",
              "342                                                                                             self attention   \n",
              "343                                                                input sequence of symbol representations x1   \n",
              "344                                                               xn to sequence of continuous representations   \n",
              "345                                                                          xn to sequence of representations   \n",
              "346                                                                                             xn to sequence   \n",
              "347                                                                                             input sequence   \n",
              "348                                                                input sequence of symbol representations x1   \n",
              "349                                                                          xn to sequence of representations   \n",
              "350                                                                                                         xn   \n",
              "351                                                                                                         xn   \n",
              "352                                                                                             input sequence   \n",
              "353                                                                                             xn to sequence   \n",
              "354                                                               xn to sequence of continuous representations   \n",
              "355                                                                                  ym of symbols one element   \n",
              "356                                                                          ym of symbols one element at time   \n",
              "357                                                                                                          z   \n",
              "358                                                                                        symbols one element   \n",
              "359                                                                                         output sequence y1   \n",
              "360                                                                                                       time   \n",
              "361                                                                                                         ym   \n",
              "362                                                                                                 ym at time   \n",
              "363                                                                                           additional input   \n",
              "364                                                                                                    symbols   \n",
              "365                                                                                          generated symbols   \n",
              "366                                                                               previously generated symbols   \n",
              "367                                                                                                      input   \n",
              "368                                                                                       left halves Figure 1   \n",
              "369                                                                                                     halves   \n",
              "370                                                                                            halves Figure 1   \n",
              "371                                                                                                       wise   \n",
              "372                                                                                             self attention   \n",
              "373                                                                                             self attention   \n",
              "374                                                                                                       wise   \n",
              "375                                                                                       overall architecture   \n",
              "376                                                                                               architecture   \n",
              "377                                                                                     stacked self attention   \n",
              "378                                                                                               architecture   \n",
              "379                                                                                                left halves   \n",
              "380                                                                                                 point wise   \n",
              "381                                                                                                     halves   \n",
              "382                                                                                       left halves Figure 1   \n",
              "383                                                                                     stacked self attention   \n",
              "384                                                                         fully connected layers for encoder   \n",
              "385                                                                                                 point wise   \n",
              "386                                                                                                left halves   \n",
              "387                                                                                            halves Figure 1   \n",
              "388                                                                                       overall architecture   \n",
              "389                                                                                             two sub layers   \n",
              "390                                                                                                     simple   \n",
              "391                                                                                            simple Figure 1   \n",
              "392                                                                              head self attention mechanism   \n",
              "393                                                                                                  position2   \n",
              "394                                                                                                      multi   \n",
              "395                                                                                         position2 Figure 1   \n",
              "396                                                                        multi head self attention mechanism   \n",
              "397                                                                                                   Figure 1   \n",
              "398                                                                                  simple position2 Figure 1   \n",
              "399                                                                                     residual connection 10   \n",
              "400                                                                                      layer normalization 1   \n",
              "401                                                          residual connection 10 around each two sub layers   \n",
              "402                                                                connection 10 around each of two sub layers   \n",
              "403                                                       residual connection 10 around each of two sub layers   \n",
              "404                                                                                              connection 10   \n",
              "405                                                                   connection 10 around each two sub layers   \n",
              "406                                                                                             where function   \n",
              "407                                                                                                  sub layer   \n",
              "408                                                             where function implemented by sub layer itself   \n",
              "409                                                                                 where function implemented   \n",
              "410                                                                   function implemented by sub layer itself   \n",
              "411                                                                          function implemented by sub layer   \n",
              "412                                                                                           sub layer itself   \n",
              "413                                                                                       function implemented   \n",
              "414                                                                                                   function   \n",
              "415                                                                    where function implemented by sub layer   \n",
              "416                                                                                        stack of N 6 layers   \n",
              "417                                                                              stack of N 6 identical layers   \n",
              "418                                                                                              also composed   \n",
              "419                                                                                                      stack   \n",
              "420                                                                                                      stack   \n",
              "421                                                                                                      stack   \n",
              "422                                                                              stack of N 6 identical layers   \n",
              "423                                                                                                   composed   \n",
              "424                                                                                        stack of N 6 layers   \n",
              "425                                                                                              encoder layer   \n",
              "426                                                             residual connections around each of sub layers   \n",
              "427                                                                                                connections   \n",
              "428                                                                      connections around each of sub layers   \n",
              "429                                                             residual connections around each of sub layers   \n",
              "430                                                                residual connections around each sub layers   \n",
              "431                                                                         connections around each sub layers   \n",
              "432                                                                                        layer normalization   \n",
              "433                                                                         connections around each sub layers   \n",
              "434                                                                residual connections around each sub layers   \n",
              "435                                                                                       residual connections   \n",
              "436                                                                                                connections   \n",
              "437                                                                                       residual connections   \n",
              "438                                                                      connections around each of sub layers   \n",
              "439                                                                                     attending to positions   \n",
              "440                                                                                                  attending   \n",
              "441                                                                                                  positions   \n",
              "442                                                                                                    decoder   \n",
              "443                                                                          attending to subsequent positions   \n",
              "444                                                            outputs at positions less than i. 3.2 Attention   \n",
              "445                                                                                                    outputs   \n",
              "446                                                                                                   position   \n",
              "447                                                                                       outputs at positions   \n",
              "448                                                                                               one position   \n",
              "449                                                                                                       fact   \n",
              "450                                                                                  outputs at positions less   \n",
              "451                                                                                     weighted sum of values   \n",
              "452                                                                   where computed by compatibility function   \n",
              "453                                                                                              sum of values   \n",
              "454                                                          where computed by compatibility function of query   \n",
              "455                                                                                     compatibility function   \n",
              "456                                                                                                   computed   \n",
              "457                                                     compatibility function of query with corresponding key   \n",
              "458                                                                                                      value   \n",
              "459                                                 where computed by compatibility function of query with key   \n",
              "460                                                                   compatibility function of query with key   \n",
              "461                                                                                             where computed   \n",
              "462                                            where computed by compatibility function with corresponding key   \n",
              "463                                   where computed by compatibility function of query with corresponding key   \n",
              "464                                                                                               weighted sum   \n",
              "465                                                              compatibility function with corresponding key   \n",
              "466                                                                            compatibility function of query   \n",
              "467                                                                                          corresponding key   \n",
              "468                                                                            compatibility function with key   \n",
              "469                                                                                                        sum   \n",
              "470                                                                                                   computed   \n",
              "471                                                          where computed by compatibility function with key   \n",
              "472                                                                             Dot Product Attention Figure 2   \n",
              "473                                                                             Dot Product Attention Figure 2   \n",
              "474                                                                                    queries of dimension dk   \n",
              "475                                                                                               dimension dv   \n",
              "476                                                                                                    queries   \n",
              "477                                                                                   several attention layers   \n",
              "478                                                                           several attention layers running   \n",
              "479                                                                       attention layers running in parallel   \n",
              "480                                                                                   attention layers running   \n",
              "481                                                               several attention layers running in parallel   \n",
              "482                                                                                           attention layers   \n",
              "483                                                                                                       keys   \n",
              "484                                                                          set packed together into matrix Q   \n",
              "485                                                                                                 set packed   \n",
              "486                                                                        set of queries packed into matrix Q   \n",
              "487                                                              set of queries simultaneously packed together   \n",
              "488                                                               set of queries packed together into matrix Q   \n",
              "489                                                           set simultaneously packed together into matrix Q   \n",
              "490                                                                                                        set   \n",
              "491                                                                    set simultaneously packed into matrix Q   \n",
              "492                                                                       set of queries simultaneously packed   \n",
              "493                                                                                        set packed together   \n",
              "494                                                                              set of queries simultaneously   \n",
              "495                                                                             set of queries packed together   \n",
              "496                                                         set of queries simultaneously packed into matrix Q   \n",
              "497                                                                                      set of queries packed   \n",
              "498                                                                                  set simultaneously packed   \n",
              "499                                                                                                   practice   \n",
              "500                                                                         set simultaneously packed together   \n",
              "501                                                                                         attention function   \n",
              "502                                                                                   set packed into matrix Q   \n",
              "503                                                set of queries simultaneously packed together into matrix Q   \n",
              "504                                                                                         set simultaneously   \n",
              "505                                                                                             set of queries   \n",
              "506                                                                                       also packed together   \n",
              "507                                                                                                     packed   \n",
              "508                                                                            packed together into matrices K   \n",
              "509                                                                                also packed into matrices K   \n",
              "510                                                                                            packed together   \n",
              "511                                                                                     packed into matrices K   \n",
              "512                                                                                                also packed   \n",
              "513                                                                       also packed together into matrices K   \n",
              "514                                                                                       V softmax QKT dk V 1   \n",
              "515                                                                                                          K   \n",
              "516                                                                                      dot product attention   \n",
              "517                                                                       dot product multiplicative attention   \n",
              "518                                                                                                     matrix   \n",
              "519                                                                                                          Q   \n",
              "520                                                                                          matrix of outputs   \n",
              "521                                                                                                       feed   \n",
              "522                                                                                     compatibility function   \n",
              "523                                                                                     compatibility function   \n",
              "524                                                                                        single hidden layer   \n",
              "525                                                                                            can implemented   \n",
              "526                                                                                                    similar   \n",
              "527                                                                highly optimized matrix multiplication code   \n",
              "528                                                                                     theoretical complexity   \n",
              "529                                                                                                    similar   \n",
              "530                                                                       optimized matrix multiplication code   \n",
              "531                                                                          similar in theoretical complexity   \n",
              "532                                                                                 matrix multiplication code   \n",
              "533                                                                                                much faster   \n",
              "534                                                                                                     faster   \n",
              "535                                                                                                    similar   \n",
              "536                                                                                            can implemented   \n",
              "537                                                                          similar in theoretical complexity   \n",
              "538                                                                                            can implemented   \n",
              "539                                                                                               small values   \n",
              "540                                                              for small values mechanisms perform similarly   \n",
              "541                                                              for small values mechanisms perform similarly   \n",
              "542                                                                    for values mechanisms perform similarly   \n",
              "543                                                                                 scaling for values of dk 3   \n",
              "544                                                                        for small values mechanisms perform   \n",
              "545                                                              for small values of dk two mechanisms perform   \n",
              "546                                                                    for values of dk two mechanisms perform   \n",
              "547                                                                               mechanisms perform similarly   \n",
              "548                                                                                                    scaling   \n",
              "549                                                                                                     values   \n",
              "550                                                                                  scaling for larger values   \n",
              "551                                                                                               small values   \n",
              "552                                                                for values two mechanisms perform similarly   \n",
              "553                                                                    for small values two mechanisms perform   \n",
              "554                                                                        for small values mechanisms perform   \n",
              "555                                                                  for small values of dk mechanisms perform   \n",
              "556                                                          for values of dk two mechanisms perform similarly   \n",
              "557                                                              for small values of dk two mechanisms perform   \n",
              "558                                                                                         small values of dk   \n",
              "559                                                                                              larger values   \n",
              "560                                                                          for values two mechanisms perform   \n",
              "561                                                                                      dot product attention   \n",
              "562                                                                                             values of dk 3   \n",
              "563                                                                              for values mechanisms perform   \n",
              "564                                                                                      dot product attention   \n",
              "565                                                                          for values two mechanisms perform   \n",
              "566                                                                           two mechanisms perform similarly   \n",
              "567                                                              for values of dk mechanisms perform similarly   \n",
              "568                                                                                         mechanisms perform   \n",
              "569                                                        for small values of dk mechanisms perform similarly   \n",
              "570                                                          for small values two mechanisms perform similarly   \n",
              "571                                                                                             values of dk 3   \n",
              "572                                                                        for values of dk mechanisms perform   \n",
              "573                                                                              for values mechanisms perform   \n",
              "574                                                                                         small values of dk   \n",
              "575                                                                                               values of dk   \n",
              "576                                                    for small values of dk two mechanisms perform similarly   \n",
              "577                                                                    for values mechanisms perform similarly   \n",
              "578                                                                                     two mechanisms perform   \n",
              "579                                                                                              larger values   \n",
              "580                                                                                         scaling for values   \n",
              "581                                                                                                     values   \n",
              "582                                                                                         small values of dk   \n",
              "583                                                                          scaling for larger values of dk 3   \n",
              "584                                                                                 scaling for values of dk 3   \n",
              "585                                                                           two mechanisms perform similarly   \n",
              "586                                                                                                     values   \n",
              "587                                                                          scaling for larger values of dk 3   \n",
              "588                                                                    for small values two mechanisms perform   \n",
              "589                                                                                         scaling for values   \n",
              "590                                                                                               values of dk   \n",
              "591                                                                  for small values of dk mechanisms perform   \n",
              "592                                                                    for values of dk two mechanisms perform   \n",
              "593                                                                                  scaling for larger values   \n",
              "594                                                                                      larger values of dk 3   \n",
              "595                                                                                                     values   \n",
              "596                                                                               mechanisms perform similarly   \n",
              "597                                                                                               small values   \n",
              "598                                                                                                    scaling   \n",
              "599                                                                                               values of dk   \n",
              "600                                                          for values of dk two mechanisms perform similarly   \n",
              "601                                                                                         small values of dk   \n",
              "602                                                                                     two mechanisms perform   \n",
              "603                                                        for small values of dk mechanisms perform similarly   \n",
              "604                                                          for small values two mechanisms perform similarly   \n",
              "605                                                                                                     values   \n",
              "606                                                                                      larger values of dk 3   \n",
              "607                                                                        for values of dk mechanisms perform   \n",
              "608                                                    for small values of dk two mechanisms perform similarly   \n",
              "609                                                                for values two mechanisms perform similarly   \n",
              "610                                                                                         mechanisms perform   \n",
              "611                                                                                               values of dk   \n",
              "612                                                                                                     values   \n",
              "613                                                                                               small values   \n",
              "614                                                              for values of dk mechanisms perform similarly   \n",
              "615                                                                                               values of dk   \n",
              "616                                                                                                  magnitude   \n",
              "617                                                                                                    regions   \n",
              "618                                                                                               large values   \n",
              "619                                                                                                gradients 4   \n",
              "620                                                                                                     values   \n",
              "621                                                                                           softmax function   \n",
              "622                                                                                extremely small gradients 4   \n",
              "623                                                                                          small gradients 4   \n",
              "624                                                                                                      large   \n",
              "625                                                                                         large values of dk   \n",
              "626                                                                                                       1 dk   \n",
              "627                                                                                               dot products   \n",
              "628                                                                                                     effect   \n",
              "629                                                                                         linear projections   \n",
              "630                                                                                                projections   \n",
              "631                                                                                                    queries   \n",
              "632                                                                                                  different   \n",
              "633                                                                                             values h times   \n",
              "634                                                                                                    queries   \n",
              "635                                                                                                  different   \n",
              "636                                                                                                projections   \n",
              "637                                                                                         linear projections   \n",
              "638                                                                                                         dk   \n",
              "639                                                                                                  different   \n",
              "640                                                                                                projections   \n",
              "641                                                                                                  different   \n",
              "642                                                                                                         dk   \n",
              "643                                                                                                       keys   \n",
              "644                                                                                         linear projections   \n",
              "645                                                                                             values h times   \n",
              "646                                                                                                       keys   \n",
              "647                                                                                         linear projections   \n",
              "648                                                                                                         dk   \n",
              "649                                                                                                  different   \n",
              "650                                                                                                  different   \n",
              "651                                                                                                         dk   \n",
              "652                                                                                                projections   \n",
              "653                                                                               dv dimensional output values   \n",
              "654                                                                                           dv output values   \n",
              "655                                                                                         attention function   \n",
              "656                                                                                                   parallel   \n",
              "657                                                                                                   Figure 2   \n",
              "658                                                                         information at different positions   \n",
              "659                                           information from different representation subspaces at positions   \n",
              "660                                                                                                      model   \n",
              "661                                                     information from representation subspaces at positions   \n",
              "662                                                                                   information at positions   \n",
              "663                                                                                                      model   \n",
              "664                                                     information from representation subspaces at positions   \n",
              "665                                                                  information from representation subspaces   \n",
              "666                                                        information from different representation subspaces   \n",
              "667                                 information from different representation subspaces at different positions   \n",
              "668                                           information from different representation subspaces at positions   \n",
              "669                                 information from different representation subspaces at different positions   \n",
              "670                                                                  information from representation subspaces   \n",
              "671                                           information from representation subspaces at different positions   \n",
              "672                                                                                                information   \n",
              "673                                           information from representation subspaces at different positions   \n",
              "674                                                                         information at different positions   \n",
              "675                                                                                   information at positions   \n",
              "676                                                                                                information   \n",
              "677                                                        information from different representation subspaces   \n",
              "678                                                                                      single attention head   \n",
              "679                                                                                             attention head   \n",
              "680                                                                               independent random variables   \n",
              "681                                                                                                  variables   \n",
              "682                                                                                                      large   \n",
              "683                                                                               independent random variables   \n",
              "684                                                                                      independent variables   \n",
              "685                                                                                                     mean 0   \n",
              "686                                                                                                     assume   \n",
              "687                                                                                                  variables   \n",
              "688                                                                                           random variables   \n",
              "689                                                                                      independent variables   \n",
              "690                                                                                           random variables   \n",
              "691                                                                                            q k dk i 1 qiki   \n",
              "692                                                                                                       work   \n",
              "693                                                                              h 8 parallel attention layers   \n",
              "694                                                                                       h 8 attention layers   \n",
              "695                                                                                                      heads   \n",
              "696                                                                                          dk dv dmodel h 64   \n",
              "697                                                                                                       that   \n",
              "698                                                            that of head attention with full dimensionality   \n",
              "699                                                           Due to reduced dimension of head similar to that   \n",
              "700                                                                 that of head attention with dimensionality   \n",
              "701                                                                              that of single head attention   \n",
              "702                                                                   Due to reduced dimension of head similar   \n",
              "703                Due to reduced dimension of head similar to that of head attention with full dimensionality   \n",
              "704                                                            that of head attention with full dimensionality   \n",
              "705                                                                                                Due similar   \n",
              "706                             Due to dimension of head similar to that of head attention with dimensionality   \n",
              "707                                                     that of single head attention with full dimensionality   \n",
              "708                                                     that of single head attention with full dimensionality   \n",
              "709         Due to reduced dimension of head similar to that of single head attention with full dimensionality   \n",
              "710                                                                 that of head attention with dimensionality   \n",
              "711                                                                   Due to reduced dimension of head similar   \n",
              "712                                                                 that of head attention with dimensionality   \n",
              "713                                                 Due to dimension of head similar to that of head attention   \n",
              "714                                                 Due to dimension of head similar to that of head attention   \n",
              "715                                         Due to reduced dimension of head similar to that of head attention   \n",
              "716                                                                              that of single head attention   \n",
              "717                                                          that of single head attention with dimensionality   \n",
              "718                                                          that of single head attention with dimensionality   \n",
              "719                                                                                     that of head attention   \n",
              "720                Due to reduced dimension of head similar to that of head attention with full dimensionality   \n",
              "721                                                                           Due to dimension of head similar   \n",
              "722                        Due to dimension of head similar to that of head attention with full dimensionality   \n",
              "723                                                     that of single head attention with full dimensionality   \n",
              "724                                                     that of single head attention with full dimensionality   \n",
              "725                                                                              that of single head attention   \n",
              "726                                                                                                       that   \n",
              "727                                                                 that of head attention with dimensionality   \n",
              "728                     Due to reduced dimension of head similar to that of head attention with dimensionality   \n",
              "729                                                                                                       that   \n",
              "730                                                                 that of head attention with dimensionality   \n",
              "731                                                                                     that of head attention   \n",
              "732                                                                           Due to dimension of head similar   \n",
              "733                                                            that of head attention with full dimensionality   \n",
              "734                      Due to dimension of head similar to that of single head attention with dimensionality   \n",
              "735                                                           Due to reduced dimension of head similar to that   \n",
              "736                                                           Due to reduced dimension of head similar to that   \n",
              "737         Due to reduced dimension of head similar to that of single head attention with full dimensionality   \n",
              "738                                          Due to dimension of head similar to that of single head attention   \n",
              "739                                  Due to reduced dimension of head similar to that of single head attention   \n",
              "740                                                          that of single head attention with dimensionality   \n",
              "741                                          Due to dimension of head similar to that of single head attention   \n",
              "742         Due to reduced dimension of head similar to that of single head attention with full dimensionality   \n",
              "743              Due to reduced dimension of head similar to that of single head attention with dimensionality   \n",
              "744                                          Due to dimension of head similar to that of single head attention   \n",
              "745                                                                                        full dimensionality   \n",
              "746                                                                                                Due similar   \n",
              "747                                                                              that of single head attention   \n",
              "748                                                            that of head attention with full dimensionality   \n",
              "749                                  Due to reduced dimension of head similar to that of single head attention   \n",
              "750                        Due to dimension of head similar to that of head attention with full dimensionality   \n",
              "751                                                                 that of head attention with dimensionality   \n",
              "752                                                                                     that of head attention   \n",
              "753                                                           Due to reduced dimension of head similar to that   \n",
              "754                                                                              that of single head attention   \n",
              "755                 Due to dimension of head similar to that of single head attention with full dimensionality   \n",
              "756                                                                           Due to dimension of head similar   \n",
              "757                             Due to dimension of head similar to that of head attention with dimensionality   \n",
              "758                                                          that of single head attention with dimensionality   \n",
              "759              Due to reduced dimension of head similar to that of single head attention with dimensionality   \n",
              "760                                                                                                       that   \n",
              "761                                                                   Due to reduced dimension of head similar   \n",
              "762                                                                 that of head attention with dimensionality   \n",
              "763                                                                   Due to reduced dimension of head similar   \n",
              "764              Due to reduced dimension of head similar to that of single head attention with dimensionality   \n",
              "765                                                 Due to dimension of head similar to that of head attention   \n",
              "766                                                            that of head attention with full dimensionality   \n",
              "767                                                     that of single head attention with full dimensionality   \n",
              "768                             Due to dimension of head similar to that of head attention with dimensionality   \n",
              "769              Due to reduced dimension of head similar to that of single head attention with dimensionality   \n",
              "770                 Due to dimension of head similar to that of single head attention with full dimensionality   \n",
              "771                                          Due to dimension of head similar to that of single head attention   \n",
              "772                                                                                                    similar   \n",
              "773                                                          that of single head attention with dimensionality   \n",
              "774                                                                                                Due similar   \n",
              "775                                                                 that of head attention with dimensionality   \n",
              "776                                                                              that of single head attention   \n",
              "777                                                            that of head attention with full dimensionality   \n",
              "778                                                                              that of single head attention   \n",
              "779                                                                                                    similar   \n",
              "780                                                                                     that of head attention   \n",
              "781                     Due to reduced dimension of head similar to that of head attention with dimensionality   \n",
              "782                Due to reduced dimension of head similar to that of head attention with full dimensionality   \n",
              "783                                                                                                    similar   \n",
              "784                                                                                                       that   \n",
              "785                                                                                                    similar   \n",
              "786                                                                                     that of head attention   \n",
              "787                                                                                     that of head attention   \n",
              "788                 Due to dimension of head similar to that of single head attention with full dimensionality   \n",
              "789                                                                                                       that   \n",
              "790                                         Due to reduced dimension of head similar to that of head attention   \n",
              "791                                         Due to reduced dimension of head similar to that of head attention   \n",
              "792                                  Due to reduced dimension of head similar to that of single head attention   \n",
              "793                Due to reduced dimension of head similar to that of head attention with full dimensionality   \n",
              "794                 Due to dimension of head similar to that of single head attention with full dimensionality   \n",
              "795                                                                                                       that   \n",
              "796                                                                           Due to dimension of head similar   \n",
              "797                                                                   Due to dimension of head similar to that   \n",
              "798                      Due to dimension of head similar to that of single head attention with dimensionality   \n",
              "799                                                                                     that of head attention   \n",
              "800                     Due to reduced dimension of head similar to that of head attention with dimensionality   \n",
              "801                                                                   Due to dimension of head similar to that   \n",
              "802                                                                              that of single head attention   \n",
              "803                     Due to reduced dimension of head similar to that of head attention with dimensionality   \n",
              "804                                                            that of head attention with full dimensionality   \n",
              "805                             Due to dimension of head similar to that of head attention with dimensionality   \n",
              "806                                                                   Due to dimension of head similar to that   \n",
              "807                                                     that of single head attention with full dimensionality   \n",
              "808                        Due to dimension of head similar to that of head attention with full dimensionality   \n",
              "809                      Due to dimension of head similar to that of single head attention with dimensionality   \n",
              "810                                                     that of single head attention with full dimensionality   \n",
              "811         Due to reduced dimension of head similar to that of single head attention with full dimensionality   \n",
              "812                        Due to dimension of head similar to that of head attention with full dimensionality   \n",
              "813                      Due to dimension of head similar to that of single head attention with dimensionality   \n",
              "814                                                     that of single head attention with full dimensionality   \n",
              "815                                  Due to reduced dimension of head similar to that of single head attention   \n",
              "816                                                                                     that of head attention   \n",
              "817                                                            that of head attention with full dimensionality   \n",
              "818                                                          that of single head attention with dimensionality   \n",
              "819                                                                   Due to dimension of head similar to that   \n",
              "820                                                          that of single head attention with dimensionality   \n",
              "821                                         Due to reduced dimension of head similar to that of head attention   \n",
              "822                                                 Due to dimension of head similar to that of head attention   \n",
              "823                                                                                                Due similar   \n",
              "824                                                          that of single head attention with dimensionality   \n",
              "825                                                                                                       that   \n",
              "826                                                                                      self attention layers   \n",
              "827                                                                                                    encoder   \n",
              "828                                                                                        position in decoder   \n",
              "829                                                                                                    decoder   \n",
              "830                                                                                        position in decoder   \n",
              "831                                                                                  leftward information flow   \n",
              "832                                                                       leftward information flow in decoder   \n",
              "833                                                                                                    decoder   \n",
              "834                                                                                information flow in decoder   \n",
              "835                                                                                           information flow   \n",
              "836                                                                            inside of dot product attention   \n",
              "837                                                                     inside of scaled dot product attention   \n",
              "838                                                                                                     inside   \n",
              "839                                                                                         ReLU activation in   \n",
              "840                                                                                        different positions   \n",
              "841                                                                                                  positions   \n",
              "842                                                                                       different parameters   \n",
              "843                                                                                       different parameters   \n",
              "844                                                                                                  positions   \n",
              "845                                                                                                      layer   \n",
              "846                                                                                                 parameters   \n",
              "847                                                                                                       same   \n",
              "848                                                                                             layer to layer   \n",
              "849                                                                                                      layer   \n",
              "850                                                                                                       same   \n",
              "851                                                                                             layer to layer   \n",
              "852                                                                                        different positions   \n",
              "853                                                                                                 parameters   \n",
              "854                                                                                              kernel size 1   \n",
              "855                                                                                    dimensionality dff 2048   \n",
              "856                                                                                    dimensionality dff 2048   \n",
              "857                                                                                                 dmodel 512   \n",
              "858                                                                                vectors of dimension dmodel   \n",
              "859                                                                                                    vectors   \n",
              "860                                                                                vectors of dimension dmodel   \n",
              "861                                                                                              output tokens   \n",
              "862                                                                                         learned embeddings   \n",
              "863                                                                                               input tokens   \n",
              "864                                                                                                 embeddings   \n",
              "865                                                                                                    vectors   \n",
              "866                                                                                                  the usual   \n",
              "867                                                                                                  the usual   \n",
              "868                                                                                                        the   \n",
              "869                                                                                                        the   \n",
              "870                                                    same weight matrix between two embedding layers similar   \n",
              "871                                                         weight matrix between two embedding layers similar   \n",
              "872                                                                                      weight matrix similar   \n",
              "873                                              same weight matrix between two embedding layers similar to 24   \n",
              "874                                                                                weight matrix similar to 24   \n",
              "875                                                                                 same weight matrix similar   \n",
              "876                                                                                         same weight matrix   \n",
              "877                                                                                                  our model   \n",
              "878                                                            same weight matrix between two embedding layers   \n",
              "879                                                                           same weight matrix similar to 24   \n",
              "880                                                                                              weight matrix   \n",
              "881                                                                 weight matrix between two embedding layers   \n",
              "882                                                   weight matrix between two embedding layers similar to 24   \n",
              "883                                                                                           embedding layers   \n",
              "884                                                                                                    weights   \n",
              "885                                                                                                     dmodel   \n",
              "886                                                                     information about position in sequence   \n",
              "887                                                            information about relative position in sequence   \n",
              "888                                                                                                        use   \n",
              "889                                                              information about relative position of tokens   \n",
              "890                                                                     information about position in sequence   \n",
              "891                                                            information about relative position in sequence   \n",
              "892                                                              information about relative position of tokens   \n",
              "893                                                  information about relative position of tokens in sequence   \n",
              "894                                                                                                      order   \n",
              "895                                                                                                information   \n",
              "896                                                                                          order of sequence   \n",
              "897                                                                        information about relative position   \n",
              "898                                                                                                information   \n",
              "899                                                           information about position of tokens in sequence   \n",
              "900                                                           information about position of tokens in sequence   \n",
              "901                                                                        information about relative position   \n",
              "902                                                                                               use of order   \n",
              "903                                                                                   use of order of sequence   \n",
              "904                                                                                                   sequence   \n",
              "905                                                                       information about position of tokens   \n",
              "906                                                                                 information about position   \n",
              "907                                                                       information about position of tokens   \n",
              "908                                                  information about relative position of tokens in sequence   \n",
              "909                                                                                 information about position   \n",
              "910                                                                                           input embeddings   \n",
              "911                                                                                                        end   \n",
              "912                                                                                                  encodings   \n",
              "913                                                                                                  5 Table 1   \n",
              "914                                                                                       positional encodings   \n",
              "915                                                                                  restricted self attention   \n",
              "916                                                                                             self attention   \n",
              "917                                                                                  restricted self attention   \n",
              "918                                                                                kernel size of convolutions   \n",
              "919                                                                                               neighborhood   \n",
              "920                                                                                                kernel size   \n",
              "921                                                                                   representation dimension   \n",
              "922                                                                                            sequence length   \n",
              "923                                                                                           dimension dmodel   \n",
              "924                                                                        same dimension dmodel as embeddings   \n",
              "925                                                                                      same dimension dmodel   \n",
              "926                                                                                      same dimension dmodel   \n",
              "927                                                                                                 can summed   \n",
              "928                                                                                                 can summed   \n",
              "929                                                                        same dimension dmodel as embeddings   \n",
              "930                                                                                           dimension dmodel   \n",
              "931                                                                             dimension dmodel as embeddings   \n",
              "932                                                                             dimension dmodel as embeddings   \n",
              "933                                                                                                 can summed   \n",
              "934                                                                                                  dimension   \n",
              "935                                                                              sine functions of frequencies   \n",
              "936                                                                    sine functions of different frequencies   \n",
              "937                                                                                                   position   \n",
              "938                                                                                             where position   \n",
              "939                                                                                                       work   \n",
              "940                                                                                             sine functions   \n",
              "941                                                                                                progression   \n",
              "942                                                                                      geometric progression   \n",
              "943                                              since for fixed offset k can represented as function of PEpos   \n",
              "944                                                                                                      learn   \n",
              "945                                                                                                     attend   \n",
              "946                                                                                                   function   \n",
              "947                                                                                               easily learn   \n",
              "948                                                       since for fixed offset k can represented as function   \n",
              "949                                                                                                     attend   \n",
              "950                                                                                                      model   \n",
              "951                                       since for fixed offset k can represented as linear function of PEpos   \n",
              "952                                                since for fixed offset k can represented as linear function   \n",
              "953                                                                               attend by relative positions   \n",
              "954                                                                                        attend by positions   \n",
              "955                                                                                        attend by positions   \n",
              "956                                                                   since for fixed offset k can represented   \n",
              "957                                                                                            we hypothesized   \n",
              "958                                                                               attend by relative positions   \n",
              "959                                                                                              Table 3 row E   \n",
              "960                                                                                                      using   \n",
              "961                                                                                                      using   \n",
              "962                                                                                              Table 3 row E   \n",
              "963                                                                          sequence lengths longer than ones   \n",
              "964                                                                                         sinusoidal version   \n",
              "965                                                              sequence lengths longer than ones encountered   \n",
              "966                                                                     extrapolate to sequence lengths longer   \n",
              "967                                                                            extrapolate to sequence lengths   \n",
              "968                                                                                                extrapolate   \n",
              "969                                                                                                      model   \n",
              "970                                                           extrapolate to sequence lengths longer than ones   \n",
              "971                                                                                                    version   \n",
              "972                                                                                    sequence lengths longer   \n",
              "973                                                                                               it may allow   \n",
              "974                               extrapolate to sequence lengths longer than ones encountered during training   \n",
              "975                                                                                           sequence lengths   \n",
              "976                                              sequence lengths longer than ones encountered during training   \n",
              "977                                               extrapolate to sequence lengths longer than ones encountered   \n",
              "978                                                                           aspects of self attention layers   \n",
              "979                                                                                      recurrent layers used   \n",
              "980                                                                                                         xi   \n",
              "981                                                                                                     layers   \n",
              "982                                                                                                layers used   \n",
              "983                                                                   various aspects of self attention layers   \n",
              "984                                                                               zn with xi such hidden layer   \n",
              "985                                                                             recurrent layers commonly used   \n",
              "986                                                                      typical sequence transduction encoder   \n",
              "987                                                                          xn to sequence of equal length z1   \n",
              "988                                                                                                    aspects   \n",
              "989                                                      hidden layer in typical sequence transduction encoder   \n",
              "990                                                                                            various aspects   \n",
              "991                                                                                                      zi Rd   \n",
              "992                                                                                                    section   \n",
              "993                                                                                      self attention layers   \n",
              "994                                                                                                         xi   \n",
              "995                                                                                           recurrent layers   \n",
              "996                                                                                sequence of equal length z1   \n",
              "997                                                                                      self attention layers   \n",
              "998                                                                                       layers commonly used   \n",
              "999                                                                                  our use of self attention   \n",
              "1000                                                                                                   our use   \n",
              "1001                                                                                          three desiderata   \n",
              "1002                                                                                  computational complexity   \n",
              "1003                                                                                                     layer   \n",
              "1004                                                                                                     layer   \n",
              "1005                                                                                                     layer   \n",
              "1006                                                                                          total complexity   \n",
              "1007                                                                            total computational complexity   \n",
              "1008                                                                                                     layer   \n",
              "1009                                                                                                complexity   \n",
              "1010                                                                                                    amount   \n",
              "1011                                                                                               computation   \n",
              "1012                                                                                          can parallelized   \n",
              "1013                                                                                               path length   \n",
              "1014                                                                                                   network   \n",
              "1015                                                         path length between range dependencies in network   \n",
              "1016                                                               path length between long range dependencies   \n",
              "1017                                                                    path length between range dependencies   \n",
              "1018                                                    path length between long range dependencies in network   \n",
              "1019                                                                               sequence transduction tasks   \n",
              "1020                                                                          many sequence transduction tasks   \n",
              "1021                                                                          many sequence transduction tasks   \n",
              "1022                                                                          many sequence transduction tasks   \n",
              "1023                                                                                                       key   \n",
              "1024                                                                               sequence transduction tasks   \n",
              "1025                                                                                                   network   \n",
              "1026                                                                                                  traverse   \n",
              "1027                                                                                                   network   \n",
              "1028                                                                                       traverse in network   \n",
              "1029                                                                                       traverse in network   \n",
              "1030                                                                                                  traverse   \n",
              "1031                                                                                long range dependencies 11   \n",
              "1032                                                                                     range dependencies 11   \n",
              "1033                                                       constant number of sequentially executed operations   \n",
              "1034                                                                             number of executed operations   \n",
              "1035                                                                                           constant number   \n",
              "1036                                                                                      number of operations   \n",
              "1037                                                                                                    number   \n",
              "1038                                                                                                 positions   \n",
              "1039                                                                             constant number of operations   \n",
              "1040                                                                    constant number of executed operations   \n",
              "1041                                                                number of sequentially executed operations   \n",
              "1042                                                                                          recurrent layers   \n",
              "1043                                                                               In terms faster than layers   \n",
              "1044                                                   In terms of computational complexity faster than layers   \n",
              "1045           sentence representations used by state of art models in machine translations such word piece 31   \n",
              "1046                                                                                              when smaller   \n",
              "1047                                                                                       terms of complexity   \n",
              "1048                                                                     In terms faster than recurrent layers   \n",
              "1049                                         In terms of computational complexity faster than recurrent layers   \n",
              "1050                                                                                                    faster   \n",
              "1051                                                                           representation dimensionality d   \n",
              "1052                                                                                                     terms   \n",
              "1053                                                                         terms of computational complexity   \n",
              "1054                                                                                                   smaller   \n",
              "1055                                                                 In terms of complexity faster than layers   \n",
              "1056                                                         when smaller than representation dimensionality d   \n",
              "1057                                                                                                    layers   \n",
              "1058                                                       In terms of complexity faster than recurrent layers   \n",
              "1059                                                                                     performance for tasks   \n",
              "1060                                                                                              neighborhood   \n",
              "1061                                                                                    neighborhood of size r   \n",
              "1062                                               6 input sequence centered around respective output position   \n",
              "1063                                                                                         only neighborhood   \n",
              "1064                                                                                          6 input sequence   \n",
              "1065                                                                                          could restricted   \n",
              "1066                                                                       computational performance for tasks   \n",
              "1067                                                                                               performance   \n",
              "1068                                                                               only neighborhood of size r   \n",
              "1069                                                                                 computational performance   \n",
              "1070                                                                                 6 input sequence centered   \n",
              "1071                                                          6 input sequence centered around output position   \n",
              "1072                                                                                                  approach   \n",
              "1073                                                                                               future work   \n",
              "1074                                                                                                  approach   \n",
              "1075                                                                                               future work   \n",
              "1076                                                                                          kernel width k n   \n",
              "1077                                                                                                    length   \n",
              "1078                                                            longest paths between two positions in network   \n",
              "1079                                                  length of longest paths between two positions in network   \n",
              "1080                                                                           case of dilated convolutions 15   \n",
              "1081                                                                       stack of O n k convolutional layers   \n",
              "1082                                                                    paths between two positions in network   \n",
              "1083                                                                                     stack of O n k layers   \n",
              "1084                                                          length of paths between two positions in network   \n",
              "1085                                                                                                     stack   \n",
              "1086                        generally expensive than recurrent layers by factor of k. Separable convolutions 6   \n",
              "1087                                                                                                    layers   \n",
              "1088                                  expensive than recurrent layers by factor of k. Separable convolutions 6   \n",
              "1089                                                                               factor of k. convolutions 6   \n",
              "1090                                                                                                    layers   \n",
              "1091                                                                                            more expensive   \n",
              "1092                                  expensive than recurrent layers by factor of k. Separable convolutions 6   \n",
              "1093                                                                 expensive than recurrent layers by factor   \n",
              "1094                                                                               factor of k. convolutions 6   \n",
              "1095                                              expensive than layers by factor of k. convolutions 6 however   \n",
              "1096                                                                                                    factor   \n",
              "1097                             generally more expensive than layers by factor of k. Separable convolutions 6   \n",
              "1098                                            expensive than layers by factor of k. Separable convolutions 6   \n",
              "1099                                                                                                    factor   \n",
              "1100                                                                                                    factor   \n",
              "1101                                       generally more expensive than layers by factor of k. convolutions 6   \n",
              "1102                                                                                                    factor   \n",
              "1103                                                 more expensive than layers by factor of k. convolutions 6   \n",
              "1104                                       more expensive than recurrent layers by factor of k. convolutions 6   \n",
              "1105                                                                          generally more expensive however   \n",
              "1106                                                                     factor of k. Separable convolutions 6   \n",
              "1107                                          generally more expensive than recurrent layers by factor however   \n",
              "1108                                                                     factor of k. Separable convolutions 6   \n",
              "1109                                                    generally more expensive than layers by factor however   \n",
              "1110                                                                                                    layers   \n",
              "1111                        generally expensive than recurrent layers by factor of k. Separable convolutions 6   \n",
              "1112                                                    more expensive than recurrent layers by factor however   \n",
              "1113                     generally more expensive than layers by factor of k. Separable convolutions 6 however   \n",
              "1114                             more expensive than recurrent layers by factor of k. Separable convolutions 6   \n",
              "1115                                                                                                    layers   \n",
              "1116                                                                     factor of k. Separable convolutions 6   \n",
              "1117                                                                                          recurrent layers   \n",
              "1118                                                                                         expensive however   \n",
              "1119                                                       generally expensive than recurrent layers by factor   \n",
              "1120                                                         generally expensive than layers by factor however   \n",
              "1121                     more expensive than recurrent layers by factor of k. Separable convolutions 6 however   \n",
              "1122                                         more expensive than layers by factor of k. convolutions 6 however   \n",
              "1123                                                                                          recurrent layers   \n",
              "1124                          expensive than recurrent layers by factor of k. Separable convolutions 6 however   \n",
              "1125                                                                                          recurrent layers   \n",
              "1126                                                                                  generally more expensive   \n",
              "1127                                                            generally more expensive than layers by factor   \n",
              "1128                                                                                                    layers   \n",
              "1129                                                                   expensive than layers by factor however   \n",
              "1130                                                                                                    layers   \n",
              "1131                                                                                                    factor   \n",
              "1132                                          generally more expensive than recurrent layers by factor however   \n",
              "1133                          generally expensive than layers by factor of k. Separable convolutions 6 however   \n",
              "1134                                                                     factor of k. Separable convolutions 6   \n",
              "1135                                                                                                    factor   \n",
              "1136                             generally more expensive than recurrent layers by factor of k. convolutions 6   \n",
              "1137                                                  generally more expensive than recurrent layers by factor   \n",
              "1138                                                                                          recurrent layers   \n",
              "1139                                                              more expensive than layers by factor however   \n",
              "1140                     generally more expensive than recurrent layers by factor of k. convolutions 6 however   \n",
              "1141                                                                                          recurrent layers   \n",
              "1142                                                                     factor of k. Separable convolutions 6   \n",
              "1143                               more expensive than layers by factor of k. Separable convolutions 6 however   \n",
              "1144                                                                                                    layers   \n",
              "1145                                                                               factor of k. convolutions 6   \n",
              "1146                                                                   expensive than layers by factor however   \n",
              "1147                     generally more expensive than recurrent layers by factor of k. convolutions 6 however   \n",
              "1148                                              expensive than layers by factor of k. convolutions 6 however   \n",
              "1149                                                                                       generally expensive   \n",
              "1150                                                                     factor of k. Separable convolutions 6   \n",
              "1151                                                                                                 expensive   \n",
              "1152                                                                                  generally more expensive   \n",
              "1153                                                                               factor of k. convolutions 6   \n",
              "1154                                    expensive than layers by factor of k. Separable convolutions 6 however   \n",
              "1155                                                                                          recurrent layers   \n",
              "1156                                                                                          recurrent layers   \n",
              "1157                                                                     factor of k. Separable convolutions 6   \n",
              "1158                             generally more expensive than recurrent layers by factor of k. convolutions 6   \n",
              "1159                                                                                          recurrent layers   \n",
              "1160                                                         generally expensive than layers by factor however   \n",
              "1161                                  generally expensive than recurrent layers by factor of k. convolutions 6   \n",
              "1162                                                                               factor of k. convolutions 6   \n",
              "1163                     more expensive than recurrent layers by factor of k. Separable convolutions 6 however   \n",
              "1164                generally expensive than recurrent layers by factor of k. Separable convolutions 6 however   \n",
              "1165                                                      expensive than layers by factor of k. convolutions 6   \n",
              "1166                                                                                          recurrent layers   \n",
              "1167                                    generally expensive than layers by factor of k. convolutions 6 however   \n",
              "1168                                                                                                    factor   \n",
              "1169                                                              more expensive than layers by factor however   \n",
              "1170                                                                               factor of k. convolutions 6   \n",
              "1171                                                                     factor of k. Separable convolutions 6   \n",
              "1172                                                                                          recurrent layers   \n",
              "1173                                                                                                    layers   \n",
              "1174                                                                               factor of k. convolutions 6   \n",
              "1175                                       more expensive than layers by factor of k. Separable convolutions 6   \n",
              "1176                                    generally expensive than layers by factor of k. convolutions 6 however   \n",
              "1177                                  generally expensive than layers by factor of k. Separable convolutions 6   \n",
              "1178                                  generally expensive than layers by factor of k. Separable convolutions 6   \n",
              "1179                                                                               factor of k. convolutions 6   \n",
              "1180                                    expensive than recurrent layers by factor of k. convolutions 6 however   \n",
              "1181                                                                                          recurrent layers   \n",
              "1182                               generally more expensive than layers by factor of k. convolutions 6 however   \n",
              "1183                                                                                                    layers   \n",
              "1184                                                         expensive than recurrent layers by factor however   \n",
              "1185                                                                               factor of k. convolutions 6   \n",
              "1186                                                         expensive than recurrent layers by factor however   \n",
              "1187                                                                           expensive than layers by factor   \n",
              "1188                                                            more expensive than recurrent layers by factor   \n",
              "1189                                                                                                    layers   \n",
              "1190                          generally expensive than layers by factor of k. Separable convolutions 6 however   \n",
              "1191                                                 more expensive than layers by factor of k. convolutions 6   \n",
              "1192                                                                                                    layers   \n",
              "1193                                                                     factor of k. Separable convolutions 6   \n",
              "1194                                                                      more expensive than layers by factor   \n",
              "1195                          expensive than recurrent layers by factor of k. Separable convolutions 6 however   \n",
              "1196                                                                     factor of k. Separable convolutions 6   \n",
              "1197                                                                                                    factor   \n",
              "1198                                               generally expensive than recurrent layers by factor however   \n",
              "1199                                                                           expensive than layers by factor   \n",
              "1200                                                                               factor of k. convolutions 6   \n",
              "1201                                                                                                    layers   \n",
              "1202                                                       generally expensive than recurrent layers by factor   \n",
              "1203                                                                                                    factor   \n",
              "1204                                                                                            more expensive   \n",
              "1205                          generally expensive than recurrent layers by factor of k. convolutions 6 however   \n",
              "1206                                                                                                    factor   \n",
              "1207                                                                          generally more expensive however   \n",
              "1208                                                            generally more expensive than layers by factor   \n",
              "1209           generally more expensive than recurrent layers by factor of k. Separable convolutions 6 however   \n",
              "1210                                                                     factor of k. Separable convolutions 6   \n",
              "1211                                         more expensive than layers by factor of k. convolutions 6 however   \n",
              "1212                                                                                       generally expensive   \n",
              "1213                                                                                          recurrent layers   \n",
              "1214                                                                               generally expensive however   \n",
              "1215                                            generally expensive than layers by factor of k. convolutions 6   \n",
              "1216                                                                                                    layers   \n",
              "1217                                                                               factor of k. convolutions 6   \n",
              "1218                                                                               factor of k. convolutions 6   \n",
              "1219                               generally more expensive than layers by factor of k. convolutions 6 however   \n",
              "1220                                                                                                    layers   \n",
              "1221                                                                     factor of k. Separable convolutions 6   \n",
              "1222                                                                                    more expensive however   \n",
              "1223                                                                                                    factor   \n",
              "1224                                                                                          recurrent layers   \n",
              "1225                               more expensive than layers by factor of k. Separable convolutions 6 however   \n",
              "1226                                                                                                    factor   \n",
              "1227                     generally more expensive than layers by factor of k. Separable convolutions 6 however   \n",
              "1228                                            expensive than recurrent layers by factor of k. convolutions 6   \n",
              "1229                                                                     factor of k. Separable convolutions 6   \n",
              "1230                   generally more expensive than recurrent layers by factor of k. Separable convolutions 6   \n",
              "1231                                                                               factor of k. convolutions 6   \n",
              "1232                                                                               generally expensive however   \n",
              "1233                                                  generally more expensive than recurrent layers by factor   \n",
              "1234                               more expensive than recurrent layers by factor of k. convolutions 6 however   \n",
              "1235                                            generally expensive than layers by factor of k. convolutions 6   \n",
              "1236                                                                     factor of k. Separable convolutions 6   \n",
              "1237                                                      expensive than layers by factor of k. convolutions 6   \n",
              "1238                             generally more expensive than layers by factor of k. Separable convolutions 6   \n",
              "1239                generally expensive than recurrent layers by factor of k. Separable convolutions 6 however   \n",
              "1240                                                                      more expensive than layers by factor   \n",
              "1241                                  generally expensive than recurrent layers by factor of k. convolutions 6   \n",
              "1242                          generally expensive than recurrent layers by factor of k. convolutions 6 however   \n",
              "1243                                                                                         expensive however   \n",
              "1244                                                                                                    factor   \n",
              "1245                                                                                          recurrent layers   \n",
              "1246           generally more expensive than recurrent layers by factor of k. Separable convolutions 6 however   \n",
              "1247                                       more expensive than layers by factor of k. Separable convolutions 6   \n",
              "1248                                            expensive than layers by factor of k. Separable convolutions 6   \n",
              "1249                                            expensive than recurrent layers by factor of k. convolutions 6   \n",
              "1250                                                                                                    factor   \n",
              "1251                                                                                                 expensive   \n",
              "1252                               more expensive than recurrent layers by factor of k. convolutions 6 however   \n",
              "1253                                                                     factor of k. Separable convolutions 6   \n",
              "1254                                                                               factor of k. convolutions 6   \n",
              "1255                                                                 generally expensive than layers by factor   \n",
              "1256                                                                               factor of k. convolutions 6   \n",
              "1257                                                                 expensive than recurrent layers by factor   \n",
              "1258                                                                                          recurrent layers   \n",
              "1259                                                                                                    factor   \n",
              "1260                                                    more expensive than recurrent layers by factor however   \n",
              "1261                                                    generally more expensive than layers by factor however   \n",
              "1262                                                                               factor of k. convolutions 6   \n",
              "1263                                                                     factor of k. Separable convolutions 6   \n",
              "1264                             more expensive than recurrent layers by factor of k. Separable convolutions 6   \n",
              "1265                                                            more expensive than recurrent layers by factor   \n",
              "1266                                                                                                    factor   \n",
              "1267                                                                                                    layers   \n",
              "1268                                       more expensive than recurrent layers by factor of k. convolutions 6   \n",
              "1269                                                                 generally expensive than layers by factor   \n",
              "1270                                    expensive than recurrent layers by factor of k. convolutions 6 however   \n",
              "1271                   generally more expensive than recurrent layers by factor of k. Separable convolutions 6   \n",
              "1272                                               generally expensive than recurrent layers by factor however   \n",
              "1273                                       generally more expensive than layers by factor of k. convolutions 6   \n",
              "1274                                                                                          recurrent layers   \n",
              "1275                                                                                                    layers   \n",
              "1276                                                                                    more expensive however   \n",
              "1277                                    expensive than layers by factor of k. Separable convolutions 6 however   \n",
              "1278                                                                       combination of self attention layer   \n",
              "1279                                                                Even with k n however equal to combination   \n",
              "1280                                                                                                     equal   \n",
              "1281                                                Even with k n equal to combination of self attention layer   \n",
              "1282                                                                             with k n equal to combination   \n",
              "1283                                             with k n however equal to combination of self attention layer   \n",
              "1284                                        Even with k n however equal to combination of self attention layer   \n",
              "1285                                                     with k n equal to combination of self attention layer   \n",
              "1286                                                                                                       k n   \n",
              "1287                                                                                                  Even k n   \n",
              "1288                                                                     with k n however equal to combination   \n",
              "1289                                                                                                  Even k n   \n",
              "1290                                                                       combination of self attention layer   \n",
              "1291                                                                                             however equal   \n",
              "1292                                                                        Even with k n equal to combination   \n",
              "1293                                                                                               combination   \n",
              "1294                                                                                                       k n   \n",
              "1295                                                                                               combination   \n",
              "1296                                                                                 more interpretable models   \n",
              "1297                                                                                      interpretable models   \n",
              "1298                                                                                   attention distributions   \n",
              "1299                                                                                                our models   \n",
              "1300                                                                                                  appendix   \n",
              "1301                                                                                                  examples   \n",
              "1302                                                                                      examples in appendix   \n",
              "1303                                                                                                  behavior   \n",
              "1304                                                                                                 structure   \n",
              "1305                                                                   behavior related to syntactic structure   \n",
              "1306                                                                             behavior related to structure   \n",
              "1307                                                                                          behavior related   \n",
              "1308                                                                          syntactic structure of sentences   \n",
              "1309                                                      behavior related to syntactic structure of sentences   \n",
              "1310                                                                                    structure of sentences   \n",
              "1311                                                                                       syntactic structure   \n",
              "1312                                                                behavior related to structure of sentences   \n",
              "1313                          standard WMT 2014 English dataset consisting of about 4.5 million sentence pairs   \n",
              "1314                            WMT 2014 English German dataset consisting of about 4.5 million sentence pairs   \n",
              "1315                                                                          standard WMT 2014 German dataset   \n",
              "1316                                                              standard WMT 2014 English dataset consisting   \n",
              "1317                                                                       WMT 2014 English dataset consisting   \n",
              "1318                                                                      standard WMT 2014 dataset consisting   \n",
              "1319                                   WMT 2014 English dataset consisting of about 4.5 million sentence pairs   \n",
              "1320                                  standard WMT 2014 dataset consisting of about 4.5 million sentence pairs   \n",
              "1321                                                                WMT 2014 English German dataset consisting   \n",
              "1322                                                               standard WMT 2014 German dataset consisting   \n",
              "1323                                                                                  WMT 2014 English dataset   \n",
              "1324                                                                               WMT 2014 dataset consisting   \n",
              "1325                                           WMT 2014 dataset consisting of about 4.5 million sentence pairs   \n",
              "1326                                                                                   WMT 2014 German dataset   \n",
              "1327                                                                           WMT 2014 English German dataset   \n",
              "1328                                                                                          WMT 2014 dataset   \n",
              "1329                                    WMT 2014 German dataset consisting of about 4.5 million sentence pairs   \n",
              "1330                                                                        WMT 2014 German dataset consisting   \n",
              "1331                                                                         standard WMT 2014 English dataset   \n",
              "1332                   standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs   \n",
              "1333                           standard WMT 2014 German dataset consisting of about 4.5 million sentence pairs   \n",
              "1334                                                                                 standard WMT 2014 dataset   \n",
              "1335                                                       standard WMT 2014 English German dataset consisting   \n",
              "1336                                                                  standard WMT 2014 English German dataset   \n",
              "1337                                                                                                 byte pair   \n",
              "1338                                                                                                   encoded   \n",
              "1339                                                                larger WMT 2014 English dataset consisting   \n",
              "1340                                                                                                    French   \n",
              "1341                                                             significantly larger WMT 2014 English dataset   \n",
              "1342                                                                                  WMT 2014 English dataset   \n",
              "1343                                                         larger WMT 2014 English French dataset consisting   \n",
              "1344                                 significantly larger WMT 2014 English dataset consisting of 36M sentences   \n",
              "1345                                               larger WMT 2014 English dataset consisting of 36M sentences   \n",
              "1346                                                                       WMT 2014 English dataset consisting   \n",
              "1347                                                                           larger WMT 2014 English dataset   \n",
              "1348                                                  significantly larger WMT 2014 English dataset consisting   \n",
              "1349                                                                    larger WMT 2014 English French dataset   \n",
              "1350                                                                           WMT 2014 English French dataset   \n",
              "1351                                           significantly larger WMT 2014 English French dataset consisting   \n",
              "1352                                                                WMT 2014 English French dataset consisting   \n",
              "1353                                               WMT 2014 English French dataset consisting of 36M sentences   \n",
              "1354                                                                                            English French   \n",
              "1355                                        larger WMT 2014 English French dataset consisting of 36M sentences   \n",
              "1356                          significantly larger WMT 2014 English French dataset consisting of 36M sentences   \n",
              "1357                                                                            32000 word piece vocabulary 31   \n",
              "1358                                                      WMT 2014 English dataset consisting of 36M sentences   \n",
              "1359                                                      significantly larger WMT 2014 English French dataset   \n",
              "1360                                                                               approximate sequence length   \n",
              "1361                                                                               approximate sequence length   \n",
              "1362                                                                                           sequence length   \n",
              "1363                                                                                          batched together   \n",
              "1364                                                                                           sequence length   \n",
              "1365                                                                                                   batched   \n",
              "1366                                                                                                       set   \n",
              "1367                                                                                     set of sentence pairs   \n",
              "1368                                                                                                our models   \n",
              "1369                                                                                         about 0.4 seconds   \n",
              "1370                                                                                                     paper   \n",
              "1371                                                                                           hyperparameters   \n",
              "1372                                                                                               base models   \n",
              "1373                                                                                                      line   \n",
              "1374                                                                                               1.0 seconds   \n",
              "1375                                                                                               bottom line   \n",
              "1376                                                                                    bottom line of table 3   \n",
              "1377                                                                                For our models 1.0 seconds   \n",
              "1378                                                                                           line of table 3   \n",
              "1379                                                                            For our big models 1.0 seconds   \n",
              "1380                                                                                                   trained   \n",
              "1381                                                                                             300,000 steps   \n",
              "1382                                                                                    300,000 steps 3.5 days   \n",
              "1383                                                                                             300,000 steps   \n",
              "1384                                                                                    300,000 steps 3.5 days   \n",
              "1385                                                                                                   trained   \n",
              "1386                                                                                         Adam optimizer 17   \n",
              "1387                                                                                                     steps   \n",
              "1388                                                                                                     steps   \n",
              "1389                                                                                             learning rate   \n",
              "1390                                                                                                        it   \n",
              "1391                                                                                                        it   \n",
              "1392                                                                                                        it   \n",
              "1393                                                                                         warmup steps 4000   \n",
              "1394                                                                             three types of regularization   \n",
              "1395                                                                                               three types   \n",
              "1396                                                                                                dropout 27   \n",
              "1397                                                                                                   encoder   \n",
              "1398                                                                                                  addition   \n",
              "1399                                                                                                   dropout   \n",
              "1400                                                                                                      sums   \n",
              "1401                                                                                        sums of embeddings   \n",
              "1402                                                                             sums of embeddings in encoder   \n",
              "1403                                                                                           sums in encoder   \n",
              "1404                                                                                         rate of Pdrop 0.1   \n",
              "1405                                                                                                      rate   \n",
              "1406                                                                                                base model   \n",
              "1407                                                                        label smoothing of value ls 0.1 30   \n",
              "1408                                                                                             Cost FLOPs EN   \n",
              "1409                                                                                                Cost FLOPs   \n",
              "1410                                                                                           label smoothing   \n",
              "1411                                                                                                    unsure   \n",
              "1412                                                                                               more unsure   \n",
              "1413                                                          big transformer model Transformer big in Table 2   \n",
              "1414                                                          big transformer model Transformer big in Table 2   \n",
              "1415                                                                                                    listed   \n",
              "1416                                                                                           line of Table 3   \n",
              "1417                                                                                    bottom line of Table 3   \n",
              "1418                                                                                               bottom line   \n",
              "1419                                                                                                      line   \n",
              "1420                                                                                               8 P100 GPUs   \n",
              "1421                                                                                                  3.5 days   \n",
              "1422                                                                                          WMT 2014 English   \n",
              "1423                                                                      WMT 2014 English to translation task   \n",
              "1424                                                                                                BLEU score   \n",
              "1425                                                               WMT 2014 English to French translation task   \n",
              "1426                                                                                                BLEU score   \n",
              "1427                                                                                        BLEU score of 41.0   \n",
              "1428                                                                                          WMT 2014 English   \n",
              "1429                                                                                        BLEU score of 41.0   \n",
              "1430                                                               WMT 2014 English to French translation task   \n",
              "1431                                                                      WMT 2014 English to translation task   \n",
              "1432                                                                                               base models   \n",
              "1433                                                                                              single model   \n",
              "1434                                                                                               base models   \n",
              "1435                                                                                            model obtained   \n",
              "1436                                                                                                     model   \n",
              "1437                                                                                     single model obtained   \n",
              "1438                                                                                       last 20 checkpoints   \n",
              "1439                                                                                            20 checkpoints   \n",
              "1440                                                                                                 beam size   \n",
              "1441                                                                                               beam search   \n",
              "1442                                                                                                 beam size   \n",
              "1443                                                                                                    chosen   \n",
              "1444                                                                        experimentation on development set   \n",
              "1445                                                                                           experimentation   \n",
              "1446                                                                                               our results   \n",
              "1447                                                                                   our translation quality   \n",
              "1448                                                                                       model architectures   \n",
              "1449                                                                                   our translation quality   \n",
              "1450                                                                                               our results   \n",
              "1451                                                                                 other model architectures   \n",
              "1452                                                                                            training costs   \n",
              "1453                                                                                       model architectures   \n",
              "1454                                                                                            training costs   \n",
              "1455                                                                 other model architectures from literature   \n",
              "1456                                                                 other model architectures from literature   \n",
              "1457                                                                                 other model architectures   \n",
              "1458                                                                       model architectures from literature   \n",
              "1459                                                                       model architectures from literature   \n",
              "1460                                                                                                    number   \n",
              "1461                                                                                                    number   \n",
              "1462                                                                                     estimate of precision   \n",
              "1463                                                                    estimate of sustained single precision   \n",
              "1464                                                                                               number used   \n",
              "1465                                                                              estimate of single precision   \n",
              "1466                                                                           number of point operations used   \n",
              "1467                                                                                number of point operations   \n",
              "1468                                                                                                  estimate   \n",
              "1469                                                                           estimate of sustained precision   \n",
              "1470                                                                       number of floating point operations   \n",
              "1471                                                                  number of floating point operations used   \n",
              "1472                                                                      change in performance to translation   \n",
              "1473                                                       change on English to translation on development set   \n",
              "1474                                                                                                   English   \n",
              "1475                                                               change in performance to German translation   \n",
              "1476                                                                                                    change   \n",
              "1477                                                   change in performance to translation on development set   \n",
              "1478                                                change on English to German translation on development set   \n",
              "1479                                        change in performance on English to translation on development set   \n",
              "1480                                                                  change to translation on development set   \n",
              "1481                                                                          change in performance on English   \n",
              "1482                                                                     German translation on development set   \n",
              "1483                                                                                               performance   \n",
              "1484                                                                                            different ways   \n",
              "1485                                                                              change to German translation   \n",
              "1486                                                                                               performance   \n",
              "1487                                                                                     change to translation   \n",
              "1488                                                           change to German translation on development set   \n",
              "1489                                                                                     change in performance   \n",
              "1490                                            change in performance to German translation on development set   \n",
              "1491                                 change in performance on English to German translation on development set   \n",
              "1492                                                                                            our base model   \n",
              "1493                                                           change in performance on English to translation   \n",
              "1494                                                                          change on English to translation   \n",
              "1495                                                                                                      ways   \n",
              "1496                                                                            translation on development set   \n",
              "1497                                                                   change on English to German translation   \n",
              "1498                                                    change in performance on English to German translation   \n",
              "1499                                                                                               translation   \n",
              "1500                                                                                        German translation   \n",
              "1501                                                                                         change on English   \n",
              "1502                                                                                                   section   \n",
              "1503                                                                                          previous section   \n",
              "1504                                                                                        results in Table 3   \n",
              "1505                                                                                                   Table 3   \n",
              "1506                                                                                                   results   \n",
              "1507                                                                                                     Table   \n",
              "1508                                                                                                 described   \n",
              "1509                                                                                described in Section 3.2.2   \n",
              "1510                                                                                                     Table   \n",
              "1511                                                                                             attention key   \n",
              "1512                                                                                                    number   \n",
              "1513                                                                                 number of attention heads   \n",
              "1514                                                                                                     Table   \n",
              "1515                                                                            amount of computation constant   \n",
              "1516                                                                                                    amount   \n",
              "1517                                                                                                   setting   \n",
              "1518                                                                          0.9 BLEU worse than best setting   \n",
              "1519                                                                                              best setting   \n",
              "1520                                                                               0.9 BLEU worse than setting   \n",
              "1521                                                                                                     worse   \n",
              "1522                                                                                                     heads   \n",
              "1523                                                                                                     worse   \n",
              "1524                                                                                                many heads   \n",
              "1525                                                                                                  0.9 BLEU   \n",
              "1526                                                                                                many heads   \n",
              "1527                                                                                              best setting   \n",
              "1528                                                                               0.9 BLEU worse than setting   \n",
              "1529                                                                                            too many heads   \n",
              "1530                                                                                            too many heads   \n",
              "1531                                                                                                   setting   \n",
              "1532                                                                          0.9 BLEU worse than best setting   \n",
              "1533                                                                                                     heads   \n",
              "1534                                                                                                  0.9 BLEU   \n",
              "1535                                                                                                       K80   \n",
              "1536                                                                                                       K80   \n",
              "1537                                                                                                    values   \n",
              "1538                                                                                                       K80   \n",
              "1539                                                                                                       K80   \n",
              "1540                                                                                                    values   \n",
              "1541                                                                                       those of base model   \n",
              "1542                                                                                                 identical   \n",
              "1543                                                                                                     those   \n",
              "1544                                                                                                 identical   \n",
              "1545                                                                                       those of base model   \n",
              "1546                                                                                                     those   \n",
              "1547                                                                                                   English   \n",
              "1548                                                                    English to translation development set   \n",
              "1549                                                             English to German translation development set   \n",
              "1550                                                                                                 wordpiece   \n",
              "1551                                                                                                 wordpiece   \n",
              "1552                                                                                         attention size dk   \n",
              "1553                                                                                     attention key size dk   \n",
              "1554                                                                                              dev 106 base   \n",
              "1555                                                                                         attention size dk   \n",
              "1556                                                                                     attention key size dk   \n",
              "1557                                                                                                    rows C   \n",
              "1558                                                                                                    rows C   \n",
              "1559                                                                                                base model   \n",
              "1560                                                                                                   results   \n",
              "1561                                                                                   our sinusoidal encoding   \n",
              "1562                                                                                         identical results   \n",
              "1563                                                                                  nearly identical results   \n",
              "1564                                                                                   our positional encoding   \n",
              "1565                                                                                              our encoding   \n",
              "1566                                                                        our sinusoidal positional encoding   \n",
              "1567                                recurrent layers commonly used in encoder decoder architectures with multi   \n",
              "1568                                          layers commonly used in encoder decoder architectures with multi   \n",
              "1569                                     layers most commonly used in encoder decoder architectures with multi   \n",
              "1570                                                                                                 attention   \n",
              "1571                                                                                                      work   \n",
              "1572                                                                                                 attention   \n",
              "1573                           recurrent layers most commonly used in encoder decoder architectures with multi   \n",
              "1574                                                                                            self attention   \n",
              "1575                                                                                                 attention   \n",
              "1576                                                                                                 attention   \n",
              "1577                                                                  For translation tasks can trained faster   \n",
              "1578                                                                                               can trained   \n",
              "1579                                                                     can trained faster than architectures   \n",
              "1580                                                                                        can trained faster   \n",
              "1581                                                                         For translation tasks can trained   \n",
              "1582                                 For translation tasks can trained significantly faster than architectures   \n",
              "1583                                                                          can trained significantly faster   \n",
              "1584                                                    For translation tasks can trained significantly faster   \n",
              "1585                                                       can trained significantly faster than architectures   \n",
              "1586                                               For translation tasks can trained faster than architectures   \n",
              "1587                                                                        even previously reported ensembles   \n",
              "1588                                                                             previously reported ensembles   \n",
              "1589                                                                        even previously reported ensembles   \n",
              "1590                                                                             previously reported ensembles   \n",
              "1591                                                                                       future of attention   \n",
              "1592                                                                                                      them   \n",
              "1593                                                                                                    future   \n",
              "1594                                                                                                   excited   \n",
              "1595                                                                          input modalities other than text   \n",
              "1596                                                                                                  problems   \n",
              "1597                                                                                          input modalities   \n",
              "1598                                                                                    input modalities other   \n",
              "1599                                                                                               Transformer   \n",
              "1600                                                                                                    images   \n",
              "1601                                                                                                    images   \n",
              "1602                                                                                                     https   \n",
              "1603                                                                                                our models   \n",
              "1604                                                                                                 available   \n",
              "1605                                                  grateful to Nal Kalchbrenner for their fruitful comments   \n",
              "1606                                                                                            their comments   \n",
              "1607                                                                                   their fruitful comments   \n",
              "1608                                                                                                  grateful   \n",
              "1609                                                                                          Nal Kalchbrenner   \n",
              "1610                                                           grateful to Nal Kalchbrenner for their comments   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    sentence  \n",
              "0                                  Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .  \n",
              "1                                  Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .  \n",
              "2                                  Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .  \n",
              "3                                  Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .  \n",
              "4                                  Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .  \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "10                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "11                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "12                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "13                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "14                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "15                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "21                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "22                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "23                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "26                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "27                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "28                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "29                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "30                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "36                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "37                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "38                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "39                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "40                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "41                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "42                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "43                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "44                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "45                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "46                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "47                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "48                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "49                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "50                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "53                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "54                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "55                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "56                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "57                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "58                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "59                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "60                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "61                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "62                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "63                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "65                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "66                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "67                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "68                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "75                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "79                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "80                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "81                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "82                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "83                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "84                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "85                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "86                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "87                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "88                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "89                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "90                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "91                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "92                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "93                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "94                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "95                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "96                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "97                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "98                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "99                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "100                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "101                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "102                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "103                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "104                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "105                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "106                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "107                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "108                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "109                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "110                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "111                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "112                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "113                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "114                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "115                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "116                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "117                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "118                                                                                                                                                                                                                                                                                                                                     1 Introduction Recurrent neural networks , long short term memory 12 and gated recurrent 7 neural networks in particular , have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation 29 , 2 , 5 .  \n",
              "119                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .  \n",
              "120                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .  \n",
              "121                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .  \n",
              "122                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .  \n",
              "123                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .  \n",
              "124                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .  \n",
              "125                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Listing order is random .  \n",
              "126                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Listing order is random .  \n",
              "127                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Jakob proposed replacing RNNs with self attention and started the effort to evaluate this idea .  \n",
              "128                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Jakob proposed replacing RNNs with self attention and started the effort to evaluate this idea .  \n",
              "129                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Ashish , with Illia , designed and implemented the first Transformer models and has been crucially involved in every aspect of this work .  \n",
              "130                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "131                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "132                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "133                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "134                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "135                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "136                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "137                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "138                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "139                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "140                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "141                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "142                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "143                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "144                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "145                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Niki designed , implemented , tuned and evaluated countless model variants in our original codebase and tensor2tensor .  \n",
              "147                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .  \n",
              "148                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .  \n",
              "149                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .  \n",
              "150                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .  \n",
              "151                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "152                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "153                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "154                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "155                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "156                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "157                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "158                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "159                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "160                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "161                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "162                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "163                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "164                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "165                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "166                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "167                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Work performed while at Google Research .  \n",
              "168                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .  \n",
              "169                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .  \n",
              "170                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .  \n",
              "171                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .  \n",
              "172                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .  \n",
              "173                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .  \n",
              "174                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "175                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "176                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "177                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "178                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "179                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "180                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "181                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "182                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "183                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "185                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "186                                                                                                                                                                                                                                                                                                                                                                                                                                                      Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .  \n",
              "187                                                                                                                                                                                                                                                                                                                                                                                                                                                      Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .  \n",
              "188                                                                                                                                                                                                                                                                                                                                                                                                                                                      Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .  \n",
              "189                                                                                                                                                                                                                                                                                                                                                                                                                                                      Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .  \n",
              "190                                                                                                                                                                                                                                                                                                                                                                                                                                                      Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .  \n",
              "191                                                                                                                                                                                                                                                                                                                                                                                                                                                      Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .  \n",
              "192                                                                                                                                                                                                                                                                                                                                                                                                                                                      Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .  \n",
              "193                                                                                                                                                                                                                                                                                                                                                                                                                                                      Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .  \n",
              "194                                                                                                                                                                                                                                                                                                                                                                                                                                                      Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .  \n",
              "195                                                                                                                                                                                                                                                                                                                                                                                                                                        This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .  \n",
              "196                                                                                                                                                                                                                                                                                                                                                                                                                                        This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .  \n",
              "197                                                                                                                                                                                                                                                                                                                                                                                                                                        This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .  \n",
              "198                                                                                                                                                                                                                                                                                                                                                                                                                                        This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .  \n",
              "199                                                                                                                                                                                                                                                                                                                                                                                                                                        This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .  \n",
              "200                                                                                                                                                                                                                                                                                                                                                                                                                                        This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .  \n",
              "201                                                                                                                                                                                                                                                                                                                                                                                                                                        This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .  \n",
              "202                                                                                                                                                                                                                                                                                                                                                                                                                                        This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .  \n",
              "203                                                                                                                                                                                                                                                                                                                                                                                                                                        This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .  \n",
              "204                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "205                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "206                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "207                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "208                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "209                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "210                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "211                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "212                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "213                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "214                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "215                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "216                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "217                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "218                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "219                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "220                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "221                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "222                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "223                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "224                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "225                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "226                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "227                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "228                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "229                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "230                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "231                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "232                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "233                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "234                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "235                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "236                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "237                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "238                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "239                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "240                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "241                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "242                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "243                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "244                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "245                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "246                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "247                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "248                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "249                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "250                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "251                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "252                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "253                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "254                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "255                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "256                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "257                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "258                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "259                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "260                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "261                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "262                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "263                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "264                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "265                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "266                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "267                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "268                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "269                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "270                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "271                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "272                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "273                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "274                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "275                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "276                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "277                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "278                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "279                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "280                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "281                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "282                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "283                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "284                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .  \n",
              "285                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .  \n",
              "286                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .  \n",
              "287                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .  \n",
              "288                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .  \n",
              "289                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .  \n",
              "290                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .  \n",
              "291                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .  \n",
              "292                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "293                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "294                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "295                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "296                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "297                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "298                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "299                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "300                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "301                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "302                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "303                                                                                                                                                                                                                                                                                                                                 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .  \n",
              "304                                                                                                                                                                                                                                                                                                                                 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .  \n",
              "305                                                                                                                                                                                                                                                                                                                                 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .  \n",
              "306                                                                                                                                                                                                                                                                                                                                 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .  \n",
              "307                                                                                                                                                                                                                                                                                                                                 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .  \n",
              "308                                                                                                                                                                                                                                                                                                                                 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .  \n",
              "309                                                                                                                                                                                                                                                                                                                                                                                                                      In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .  \n",
              "310                                                                                                                                                                                                                                                                                                                                                                                                                      In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .  \n",
              "311                                                                                                                                                                                                                                                                                                                                                                                                                      In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .  \n",
              "312                                                                                                                                                                                                                                                                                                                                                                                                                      In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .  \n",
              "313                                                                                                                                                                                                                                                                                                                                                                                                                      In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .  \n",
              "314                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        This makes it more difficult to learn dependencies between distant positions 11 .  \n",
              "315                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        This makes it more difficult to learn dependencies between distant positions 11 .  \n",
              "316                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        This makes it more difficult to learn dependencies between distant positions 11 .  \n",
              "317                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        This makes it more difficult to learn dependencies between distant positions 11 .  \n",
              "318                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        This makes it more difficult to learn dependencies between distant positions 11 .  \n",
              "319                                                                                                                                                                                                                                                                                                                                                                                                                                                     Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .  \n",
              "320                                                                                                                                                                                                                                                                                                                                                                                                                                                     Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .  \n",
              "321                                                                                                                                                                                                                                                                                                                                                                                                                                                     Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .  \n",
              "322                                                                                                                                                                                                                                                                                                                                                                                                                                                     Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .  \n",
              "323                                                                                                                                                                                                                                                                                                                                                                                                                                                     Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .  \n",
              "324                                                                                                                                                                                                                                                                                                                                                                                                                                                     Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .  \n",
              "325                                                                                                                                                                                                                                                                                                                                                                                                                                                     Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .  \n",
              "326                                                                                                                                                                                                                                                                                                                                                                                                               Self attention has been used successfully in a variety of tasks including reading comprehension , abstractive summarization , textual entailment and learning task independent sentence representations 4 , 22 , 23 , 19 .  \n",
              "327                                                                                                                                                                                                                                                                                                                                                                                                               Self attention has been used successfully in a variety of tasks including reading comprehension , abstractive summarization , textual entailment and learning task independent sentence representations 4 , 22 , 23 , 19 .  \n",
              "328                                                                                                                                                                                                                                                                                                                                                                                                       To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .  \n",
              "329                                                                                                                                                                                                                                                                                                                                                                                                       To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .  \n",
              "330                                                                                                                                                                                                                                                                                                                                                                                                       To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .  \n",
              "331                                                                                                                                                                                                                                                                                                                                                                                                       To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .  \n",
              "332                                                                                                                                                                                                                                                                                                                                                                                                       To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .  \n",
              "333                                                                                                                                                                                                                                                                                                                                                                                                       To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .  \n",
              "334                                                                                                                                                                                                                                                                                                                                                                                                       To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .  \n",
              "335                                                                                                                                                                                                                                                                                                                                                                                                       To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .  \n",
              "336                                                                                                                                                                                                                                                                                                                                                                                                       To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .  \n",
              "337                                                                                                                                                                                                                                                                                                                                                                                                       To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .  \n",
              "338                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .  \n",
              "339                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .  \n",
              "340                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .  \n",
              "341                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .  \n",
              "342                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .  \n",
              "343                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "344                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "345                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "346                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "347                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "348                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "349                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "350                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "351                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "352                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "353                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "354                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "355                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .  \n",
              "356                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .  \n",
              "357                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .  \n",
              "358                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .  \n",
              "359                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .  \n",
              "360                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .  \n",
              "361                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .  \n",
              "362                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .  \n",
              "363                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .  \n",
              "364                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .  \n",
              "365                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .  \n",
              "366                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .  \n",
              "367                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .  \n",
              "368                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "369                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "370                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "371                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "372                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "373                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "374                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "375                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "376                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "377                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "378                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "379                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "380                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "381                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "382                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "383                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "384                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "385                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "386                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "387                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "388                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "389                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Each layer has two sub layers .  \n",
              "390                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .  \n",
              "391                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .  \n",
              "392                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .  \n",
              "393                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .  \n",
              "394                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .  \n",
              "395                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .  \n",
              "396                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .  \n",
              "397                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .  \n",
              "398                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .  \n",
              "399                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .  \n",
              "400                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .  \n",
              "401                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .  \n",
              "402                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .  \n",
              "403                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .  \n",
              "404                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .  \n",
              "405                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .  \n",
              "406                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .  \n",
              "407                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .  \n",
              "408                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .  \n",
              "409                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .  \n",
              "410                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .  \n",
              "411                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .  \n",
              "412                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .  \n",
              "413                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .  \n",
              "414                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .  \n",
              "415                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .  \n",
              "416                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Decoder : The decoder is also composed of a stack of N 6 identical layers .  \n",
              "417                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Decoder : The decoder is also composed of a stack of N 6 identical layers .  \n",
              "418                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Decoder : The decoder is also composed of a stack of N 6 identical layers .  \n",
              "419                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Decoder : The decoder is also composed of a stack of N 6 identical layers .  \n",
              "420                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Decoder : The decoder is also composed of a stack of N 6 identical layers .  \n",
              "421                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Decoder : The decoder is also composed of a stack of N 6 identical layers .  \n",
              "422                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Decoder : The decoder is also composed of a stack of N 6 identical layers .  \n",
              "423                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Decoder : The decoder is also composed of a stack of N 6 identical layers .  \n",
              "424                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Decoder : The decoder is also composed of a stack of N 6 identical layers .  \n",
              "425                                                                                                                                                                                                                                                                                                                                                                                                                                                             In addition to the two sub layers in each encoder layer , the decoder inserts a third sub layer , which performs multi head attention over the output of the encoder stack .  \n",
              "426                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "427                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "428                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "429                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "430                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "431                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "432                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "433                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "434                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "435                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "436                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "437                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "438                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "439                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .  \n",
              "440                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .  \n",
              "441                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .  \n",
              "442                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .  \n",
              "443                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .  \n",
              "444                                                                                                                                                                                                                                                  This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .  \n",
              "445                                                                                                                                                                                                                                                  This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .  \n",
              "446                                                                                                                                                                                                                                                  This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .  \n",
              "447                                                                                                                                                                                                                                                  This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .  \n",
              "448                                                                                                                                                                                                                                                  This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .  \n",
              "449                                                                                                                                                                                                                                                  This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .  \n",
              "450                                                                                                                                                                                                                                                  This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .  \n",
              "451                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "452                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "453                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "454                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "455                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "456                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "457                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "458                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "459                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "460                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "461                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "462                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "463                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "464                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "465                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "466                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "467                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "468                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "469                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "470                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "471                                                                                                                                                                                                                                                                                                                                                                                                                                                       The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "472                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              3.2.1 Scaled Dot Product Attention We call our particular attention Scaled Dot Product Attention Figure 2 .  \n",
              "473                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              3.2.1 Scaled Dot Product Attention We call our particular attention Scaled Dot Product Attention Figure 2 .  \n",
              "474                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The input consists of queries and keys of dimension dk , and values of dimension dv .  \n",
              "475                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The input consists of queries and keys of dimension dk , and values of dimension dv .  \n",
              "476                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The input consists of queries and keys of dimension dk , and values of dimension dv .  \n",
              "477                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    right Multi Head Attention consists of several attention layers running in parallel .  \n",
              "478                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    right Multi Head Attention consists of several attention layers running in parallel .  \n",
              "479                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    right Multi Head Attention consists of several attention layers running in parallel .  \n",
              "480                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    right Multi Head Attention consists of several attention layers running in parallel .  \n",
              "481                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    right Multi Head Attention consists of several attention layers running in parallel .  \n",
              "482                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    right Multi Head Attention consists of several attention layers running in parallel .  \n",
              "483                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             query with all keys , divide each by dk , and apply a softmax function to obtain the weights on the values .  \n",
              "484                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "485                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "486                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "487                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "488                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "489                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "490                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "491                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "492                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "493                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "494                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "495                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "496                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "497                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "498                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "499                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "500                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "501                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "502                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "503                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "504                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "505                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "506                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The keys and values are also packed together into matrices K and V .  \n",
              "507                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The keys and values are also packed together into matrices K and V .  \n",
              "508                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The keys and values are also packed together into matrices K and V .  \n",
              "509                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The keys and values are also packed together into matrices K and V .  \n",
              "510                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The keys and values are also packed together into matrices K and V .  \n",
              "511                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The keys and values are also packed together into matrices K and V .  \n",
              "512                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The keys and values are also packed together into matrices K and V .  \n",
              "513                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The keys and values are also packed together into matrices K and V .  \n",
              "514                                                                                                                                                                                                                                                                                                                                                                                                                                        We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .  \n",
              "515                                                                                                                                                                                                                                                                                                                                                                                                                                        We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .  \n",
              "516                                                                                                                                                                                                                                                                                                                                                                                                                                        We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .  \n",
              "517                                                                                                                                                                                                                                                                                                                                                                                                                                        We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .  \n",
              "518                                                                                                                                                                                                                                                                                                                                                                                                                                        We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .  \n",
              "519                                                                                                                                                                                                                                                                                                                                                                                                                                        We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .  \n",
              "520                                                                                                                                                                                                                                                                                                                                                                                                                                        We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .  \n",
              "521                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Additive attention computes the compatibility function using a feed forward network with a single hidden layer .  \n",
              "522                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Additive attention computes the compatibility function using a feed forward network with a single hidden layer .  \n",
              "523                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Additive attention computes the compatibility function using a feed forward network with a single hidden layer .  \n",
              "524                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Additive attention computes the compatibility function using a feed forward network with a single hidden layer .  \n",
              "525                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "526                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "527                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "528                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "529                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "530                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "531                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "532                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "533                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "534                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "535                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "536                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "537                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "538                                                                                                                                                                                                                                                                                                                                                                                                                        While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "539                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "540                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "541                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "542                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "543                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "544                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "545                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "546                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "547                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "548                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "549                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "550                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "551                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "552                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "553                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "554                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "555                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "556                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "557                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "558                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "559                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "560                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "561                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "562                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "563                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "564                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "565                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "566                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "567                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "568                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "569                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "570                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "571                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "572                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "573                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "574                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "575                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "576                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "577                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "578                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "579                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "580                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "581                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "582                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "583                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "584                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "585                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "586                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "587                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "588                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "589                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "590                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "591                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "592                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "593                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "594                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "595                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "596                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "597                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "598                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "599                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "600                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "601                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "602                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "603                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "604                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "605                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "606                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "607                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "608                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "609                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "610                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "611                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "612                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "613                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "614                                                                                                                                                                                                                                                                                                                                                                                                                                                                     While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "615                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "616                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "617                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "618                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "619                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "620                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "621                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "622                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "623                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "624                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "625                                                                                                                                                                                                                                                                                                                                                                                                                                                                 We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "626                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          To counteract this effect , we scale the dot products by 1 dk .  \n",
              "627                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          To counteract this effect , we scale the dot products by 1 dk .  \n",
              "628                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          To counteract this effect , we scale the dot products by 1 dk .  \n",
              "629                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "630                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "631                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "632                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "633                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "634                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "635                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "636                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "637                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "638                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "639                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "640                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "641                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "642                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "643                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "644                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "645                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "646                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "647                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "648                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "649                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "650                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "651                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "652                                                                                                                                                                                                                                                                                                                              3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "653                                                                                                                                                                                                                                                                                                                                                                                                                                                                            On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .  \n",
              "654                                                                                                                                                                                                                                                                                                                                                                                                                                                                            On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .  \n",
              "655                                                                                                                                                                                                                                                                                                                                                                                                                                                                            On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .  \n",
              "656                                                                                                                                                                                                                                                                                                                                                                                                                                                                            On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .  \n",
              "657                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              These are concatenated and once again projected , resulting in the final values , as depicted in Figure 2 .  \n",
              "658                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "659                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "660                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "661                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "662                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "663                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "664                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "665                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "666                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "667                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "668                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "669                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "670                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "671                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "672                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "673                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "674                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "675                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "676                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "677                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .  \n",
              "678                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 With a single attention head , averaging inhibits this .  \n",
              "679                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 With a single attention head , averaging inhibits this .  \n",
              "680                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "681                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "682                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "683                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "684                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "685                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "686                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "687                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "688                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "689                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "690                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "691                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Then their dot product , q k dk i 1 qiki , has mean 0 and variance dk .  \n",
              "692                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we employ h 8 parallel attention layers , or heads .  \n",
              "693                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we employ h 8 parallel attention layers , or heads .  \n",
              "694                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we employ h 8 parallel attention layers , or heads .  \n",
              "695                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we employ h 8 parallel attention layers , or heads .  \n",
              "696                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For each of these we use dk dv dmodel h 64 .  \n",
              "697                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "698                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "699                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "700                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "701                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "702                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "703                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "704                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "705                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "706                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "707                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "708                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "709                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "710                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "711                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "712                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "713                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "714                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "715                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "716                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "717                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "718                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "719                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "720                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "721                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "722                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "723                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "724                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "725                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "726                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "727                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "728                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "729                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "730                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "731                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "732                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "733                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "734                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "735                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "736                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "737                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "738                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "739                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "740                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "741                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "742                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "743                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "744                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "745                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "746                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "747                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "748                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "749                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "750                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "751                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "752                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "753                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "754                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "755                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "756                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "757                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "758                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "759                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "760                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "761                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "762                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "763                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "764                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "765                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "766                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "767                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "768                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "769                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "770                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "771                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "772                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "773                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "774                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "775                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "776                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "777                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "778                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "779                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "780                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "781                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "782                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "783                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "784                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "785                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "786                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "787                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "788                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "789                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "790                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "791                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "792                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "793                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "794                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "795                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "796                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "797                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "798                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "799                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "800                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "801                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "802                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "803                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "804                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "805                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "806                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "807                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "808                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "809                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "810                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "811                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "812                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "813                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "814                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "815                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "816                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "817                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "818                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "819                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "820                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "821                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "822                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "823                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "824                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "825                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "826                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The encoder contains self attention layers .  \n",
              "827                                                                                                                                                                                                                                                                                                                                                                                                                                                                               In a self attention layer all of the keys , values and queries come from the same place , in this case , the output of the previous layer in the encoder .  \n",
              "828                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .  \n",
              "829                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .  \n",
              "830                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .  \n",
              "831                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We need to prevent leftward information flow in the decoder to preserve the auto regressive property .  \n",
              "832                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We need to prevent leftward information flow in the decoder to preserve the auto regressive property .  \n",
              "833                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We need to prevent leftward information flow in the decoder to preserve the auto regressive property .  \n",
              "834                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We need to prevent leftward information flow in the decoder to preserve the auto regressive property .  \n",
              "835                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We need to prevent leftward information flow in the decoder to preserve the auto regressive property .  \n",
              "836                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We implement this inside of scaled dot product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections .  \n",
              "837                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We implement this inside of scaled dot product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections .  \n",
              "838                                                                                                                                                                                                                                                                                                                                                                                                                                                                      We implement this inside of scaled dot product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections .  \n",
              "839                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          This consists of two linear transformations with a ReLU activation in between .  \n",
              "840                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "841                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "842                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "843                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "844                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "845                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "846                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "847                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "848                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "849                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "850                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "851                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "852                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "853                                                                                                                                                                                                                                                                                                                                                                                                                                                                              FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "854                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Another way of describing this is as two convolutions with kernel size 1 .  \n",
              "855                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .  \n",
              "856                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .  \n",
              "857                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .  \n",
              "858                                                                                                                                                                                                                                                                                                                                                                                                                                                    3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .  \n",
              "859                                                                                                                                                                                                                                                                                                                                                                                                                                                    3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .  \n",
              "860                                                                                                                                                                                                                                                                                                                                                                                                                                                    3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .  \n",
              "861                                                                                                                                                                                                                                                                                                                                                                                                                                                    3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .  \n",
              "862                                                                                                                                                                                                                                                                                                                                                                                                                                                    3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .  \n",
              "863                                                                                                                                                                                                                                                                                                                                                                                                                                                    3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .  \n",
              "864                                                                                                                                                                                                                                                                                                                                                                                                                                                    3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .  \n",
              "865                                                                                                                                                                                                                                                                                                                                                                                                                                                    3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .  \n",
              "866                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .  \n",
              "867                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .  \n",
              "868                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .  \n",
              "869                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .  \n",
              "870                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "871                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "872                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "873                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "874                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "875                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "876                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "877                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "878                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "879                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "880                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "881                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "882                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "883                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In the embedding layers , we multiply those weights by dmodel .  \n",
              "884                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In the embedding layers , we multiply those weights by dmodel .  \n",
              "885                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          In the embedding layers , we multiply those weights by dmodel .  \n",
              "886                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "887                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "888                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "889                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "890                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "891                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "892                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "893                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "894                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "895                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "896                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "897                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "898                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "899                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "900                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "901                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "902                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "903                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "904                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "905                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "906                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "907                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "908                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "909                                                                                                                                                                                                                                                                                                                                                                            3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "910                                                                                                                                                                                                                                                                                                                                                                                                                                 To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .  \n",
              "911                                                                                                                                                                                                                                                                                                                                                                                                                                 To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .  \n",
              "912                                                                                                                                                                                                                                                                                                                                                                                                                                 To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .  \n",
              "913                                                                                                                                                                                                                                                                                                                                                                                                                                 To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .  \n",
              "914                                                                                                                                                                                                                                                                                                                                                                                                                                 To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .  \n",
              "915                                                                                                                                                                                                                                                                                                                                                                                                                                                                    n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "916                                                                                                                                                                                                                                                                                                                                                                                                                                                                    n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "917                                                                                                                                                                                                                                                                                                                                                                                                                                                                    n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "918                                                                                                                                                                                                                                                                                                                                                                                                                                                                    n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "919                                                                                                                                                                                                                                                                                                                                                                                                                                                                    n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "920                                                                                                                                                                                                                                                                                                                                                                                                                                                                    n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "921                                                                                                                                                                                                                                                                                                                                                                                                                                                                    n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "922                                                                                                                                                                                                                                                                                                                                                                                                                                                                    n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "923                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "924                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "925                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "926                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "927                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "928                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "929                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "930                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "931                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "932                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "933                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "934                                                                                                                                                                                                                                                                                                                                                                                                                                    In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .  \n",
              "935                                                                                                                                                                                                                                                                                                                                                                                                                                    In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .  \n",
              "936                                                                                                                                                                                                                                                                                                                                                                                                                                    In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .  \n",
              "937                                                                                                                                                                                                                                                                                                                                                                                                                                    In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .  \n",
              "938                                                                                                                                                                                                                                                                                                                                                                                                                                    In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .  \n",
              "939                                                                                                                                                                                                                                                                                                                                                                                                                                    In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .  \n",
              "940                                                                                                                                                                                                                                                                                                                                                                                                                                    In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .  \n",
              "941                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The wavelengths form a geometric progression from 2 to 10000 2 .  \n",
              "942                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The wavelengths form a geometric progression from 2 to 10000 2 .  \n",
              "943                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "944                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "945                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "946                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "947                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "948                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "949                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "950                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "951                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "952                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "953                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "954                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "955                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "956                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "957                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "958                                                                                                                                                                                                                                                                                                                                                                                                                      We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "959                                                                                                                                                                                                                                                                                                                                                                                                                                                                          We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .  \n",
              "960                                                                                                                                                                                                                                                                                                                                                                                                                                                                          We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .  \n",
              "961                                                                                                                                                                                                                                                                                                                                                                                                                                                                          We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .  \n",
              "962                                                                                                                                                                                                                                                                                                                                                                                                                                                                          We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .  \n",
              "963                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "964                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "965                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "966                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "967                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "968                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "969                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "970                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "971                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "972                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "973                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "974                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "975                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "976                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "977                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "978                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "979                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "980                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "981                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "982                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "983                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "984                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "985                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "986                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "987                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "988                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "989                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "990                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "991                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "992                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "993                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "994                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "995                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "996                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "997                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "998                                                                                                                                                                                                                                                   4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "999                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Motivating our use of self attention we consider three desiderata .  \n",
              "1000                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Motivating our use of self attention we consider three desiderata .  \n",
              "1001                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Motivating our use of self attention we consider three desiderata .  \n",
              "1002                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   One is the total computational complexity per layer .  \n",
              "1003                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   One is the total computational complexity per layer .  \n",
              "1004                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   One is the total computational complexity per layer .  \n",
              "1005                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   One is the total computational complexity per layer .  \n",
              "1006                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   One is the total computational complexity per layer .  \n",
              "1007                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   One is the total computational complexity per layer .  \n",
              "1008                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   One is the total computational complexity per layer .  \n",
              "1009                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   One is the total computational complexity per layer .  \n",
              "1010                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .  \n",
              "1011                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .  \n",
              "1012                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .  \n",
              "1013                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The third is the path length between long range dependencies in the network .  \n",
              "1014                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The third is the path length between long range dependencies in the network .  \n",
              "1015                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The third is the path length between long range dependencies in the network .  \n",
              "1016                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The third is the path length between long range dependencies in the network .  \n",
              "1017                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The third is the path length between long range dependencies in the network .  \n",
              "1018                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The third is the path length between long range dependencies in the network .  \n",
              "1019                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Learning long range dependencies is a key challenge in many sequence transduction tasks .  \n",
              "1020                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Learning long range dependencies is a key challenge in many sequence transduction tasks .  \n",
              "1021                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Learning long range dependencies is a key challenge in many sequence transduction tasks .  \n",
              "1022                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Learning long range dependencies is a key challenge in many sequence transduction tasks .  \n",
              "1023                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Learning long range dependencies is a key challenge in many sequence transduction tasks .  \n",
              "1024                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Learning long range dependencies is a key challenge in many sequence transduction tasks .  \n",
              "1025                                                                                                                                                                                                                                                                                                                                                                                                                                                                               One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .  \n",
              "1026                                                                                                                                                                                                                                                                                                                                                                                                                                                                               One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .  \n",
              "1027                                                                                                                                                                                                                                                                                                                                                                                                                                                                               One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .  \n",
              "1028                                                                                                                                                                                                                                                                                                                                                                                                                                                                               One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .  \n",
              "1029                                                                                                                                                                                                                                                                                                                                                                                                                                                                               One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .  \n",
              "1030                                                                                                                                                                                                                                                                                                                                                                                                                                                                               One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .  \n",
              "1031                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long range dependencies 11 .  \n",
              "1032                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long range dependencies 11 .  \n",
              "1033                                                                                                                                                                                                                                                                                                                                                                                                                                         As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .  \n",
              "1034                                                                                                                                                                                                                                                                                                                                                                                                                                         As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .  \n",
              "1035                                                                                                                                                                                                                                                                                                                                                                                                                                         As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .  \n",
              "1036                                                                                                                                                                                                                                                                                                                                                                                                                                         As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .  \n",
              "1037                                                                                                                                                                                                                                                                                                                                                                                                                                         As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .  \n",
              "1038                                                                                                                                                                                                                                                                                                                                                                                                                                         As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .  \n",
              "1039                                                                                                                                                                                                                                                                                                                                                                                                                                         As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .  \n",
              "1040                                                                                                                                                                                                                                                                                                                                                                                                                                         As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .  \n",
              "1041                                                                                                                                                                                                                                                                                                                                                                                                                                         As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .  \n",
              "1042                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1043                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1044                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1045                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1046                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1047                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1048                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1049                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1050                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1051                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1052                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1053                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1054                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1055                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1056                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1057                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1058                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "1059                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1060                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1061                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1062                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1063                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1064                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1065                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1066                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1067                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1068                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1069                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1070                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1071                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "1072                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We plan to investigate this approach further in future work .  \n",
              "1073                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We plan to investigate this approach further in future work .  \n",
              "1074                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We plan to investigate this approach further in future work .  \n",
              "1075                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We plan to investigate this approach further in future work .  \n",
              "1076                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           A single convolutional layer with kernel width k n does not connect all pairs of input and output positions .  \n",
              "1077                                                                                                                                                                                                                                                                                                                                                                                                  Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .  \n",
              "1078                                                                                                                                                                                                                                                                                                                                                                                                  Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .  \n",
              "1079                                                                                                                                                                                                                                                                                                                                                                                                  Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .  \n",
              "1080                                                                                                                                                                                                                                                                                                                                                                                                  Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .  \n",
              "1081                                                                                                                                                                                                                                                                                                                                                                                                  Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .  \n",
              "1082                                                                                                                                                                                                                                                                                                                                                                                                  Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .  \n",
              "1083                                                                                                                                                                                                                                                                                                                                                                                                  Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .  \n",
              "1084                                                                                                                                                                                                                                                                                                                                                                                                  Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .  \n",
              "1085                                                                                                                                                                                                                                                                                                                                                                                                  Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .  \n",
              "1086                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1087                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1088                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1089                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1090                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1091                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1092                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1093                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1094                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1095                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1096                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1097                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1098                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1099                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1100                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1101                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1102                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1103                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1104                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1105                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1106                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1107                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1108                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1109                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1110                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1111                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1112                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1113                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1114                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1115                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1116                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1117                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1118                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1119                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1120                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1121                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1122                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1123                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1124                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1125                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1126                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1127                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1128                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1129                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1130                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1131                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1132                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1133                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1134                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1135                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1136                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1137                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1138                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1139                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1140                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1141                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1142                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1143                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1144                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1145                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1146                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1147                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1148                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1149                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1150                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1151                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1152                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1153                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1154                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1155                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1156                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1157                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1158                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1159                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1160                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1161                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1162                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1163                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1164                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1165                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1166                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1167                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1168                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1169                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1170                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1171                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1172                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1173                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1174                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1175                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1176                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1177                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1178                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1179                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1180                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1181                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1182                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1183                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1184                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1185                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1186                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1187                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1188                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1189                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1190                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1191                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1192                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1193                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1194                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1195                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1196                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1197                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1198                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1199                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1200                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1201                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1202                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1203                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1204                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1205                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1206                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1207                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1208                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1209                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1210                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1211                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1212                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1213                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1214                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1215                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1216                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1217                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1218                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1219                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1220                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1221                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1222                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1223                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1224                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1225                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1226                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1227                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1228                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1229                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1230                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1231                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1232                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1233                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1234                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1235                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1236                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1237                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1238                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1239                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1240                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1241                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1242                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1243                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1244                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1245                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1246                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1247                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1248                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1249                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1250                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1251                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1252                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1253                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1254                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1255                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1256                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1257                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1258                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1259                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1260                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1261                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1262                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1263                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1264                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1265                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1266                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1267                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1268                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1269                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1270                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1271                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1272                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1273                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1274                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1275                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1276                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1277                                                                                                                                                                                                                                                                                                                                                                                                                                               Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .  \n",
              "1278                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1279                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1280                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1281                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1282                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1283                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1284                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1285                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1286                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1287                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1288                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1289                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1290                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1291                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1292                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1293                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1294                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1295                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "1296                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                As side benefit , self attention could yield more interpretable models .  \n",
              "1297                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                As side benefit , self attention could yield more interpretable models .  \n",
              "1298                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We inspect attention distributions from our models and present and discuss examples in the appendix .  \n",
              "1299                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We inspect attention distributions from our models and present and discuss examples in the appendix .  \n",
              "1300                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We inspect attention distributions from our models and present and discuss examples in the appendix .  \n",
              "1301                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We inspect attention distributions from our models and present and discuss examples in the appendix .  \n",
              "1302                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We inspect attention distributions from our models and present and discuss examples in the appendix .  \n",
              "1303                                                                                                                                                                                                                                                                                                                                                                                                                                                    Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .  \n",
              "1304                                                                                                                                                                                                                                                                                                                                                                                                                                                    Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .  \n",
              "1305                                                                                                                                                                                                                                                                                                                                                                                                                                                    Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .  \n",
              "1306                                                                                                                                                                                                                                                                                                                                                                                                                                                    Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .  \n",
              "1307                                                                                                                                                                                                                                                                                                                                                                                                                                                    Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .  \n",
              "1308                                                                                                                                                                                                                                                                                                                                                                                                                                                    Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .  \n",
              "1309                                                                                                                                                                                                                                                                                                                                                                                                                                                    Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .  \n",
              "1310                                                                                                                                                                                                                                                                                                                                                                                                                                                    Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .  \n",
              "1311                                                                                                                                                                                                                                                                                                                                                                                                                                                    Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .  \n",
              "1312                                                                                                                                                                                                                                                                                                                                                                                                                                                    Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .  \n",
              "1313                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1314                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1315                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1316                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1317                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1318                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1319                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1320                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1321                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1322                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1323                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1324                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1325                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1326                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1327                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1328                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1329                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1330                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1331                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1332                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1333                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1334                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1335                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1336                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "1337                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Sentences were encoded using byte pair encoding 3 , which has a shared sourcetarget vocabulary of about 37000 tokens .  \n",
              "1338                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Sentences were encoded using byte pair encoding 3 , which has a shared sourcetarget vocabulary of about 37000 tokens .  \n",
              "1339                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1340                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1341                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1342                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1343                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1344                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1345                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1346                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1347                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1348                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1349                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1350                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1351                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1352                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1353                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1354                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1355                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1356                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1357                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1358                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1359                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "1360                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Sentence pairs were batched together by approximate sequence length .  \n",
              "1361                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Sentence pairs were batched together by approximate sequence length .  \n",
              "1362                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Sentence pairs were batched together by approximate sequence length .  \n",
              "1363                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Sentence pairs were batched together by approximate sequence length .  \n",
              "1364                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Sentence pairs were batched together by approximate sequence length .  \n",
              "1365                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Sentence pairs were batched together by approximate sequence length .  \n",
              "1366                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .  \n",
              "1367                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .  \n",
              "1368                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs .  \n",
              "1369                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .  \n",
              "1370                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .  \n",
              "1371                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .  \n",
              "1372                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We trained the base models for a total of 100,000 steps or 12 hours .  \n",
              "1373                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .  \n",
              "1374                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .  \n",
              "1375                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .  \n",
              "1376                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .  \n",
              "1377                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .  \n",
              "1378                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .  \n",
              "1379                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .  \n",
              "1380                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The big models were trained for 300,000 steps 3.5 days .  \n",
              "1381                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The big models were trained for 300,000 steps 3.5 days .  \n",
              "1382                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The big models were trained for 300,000 steps 3.5 days .  \n",
              "1383                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The big models were trained for 300,000 steps 3.5 days .  \n",
              "1384                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The big models were trained for 300,000 steps 3.5 days .  \n",
              "1385                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The big models were trained for 300,000 steps 3.5 days .  \n",
              "1386                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.3 Optimizer We used the Adam optimizer 17 with 1 0.9 , 2 0.98 and 10 9 .  \n",
              "1387                                                                                                                                                                                                                                                                                 We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .  \n",
              "1388                                                                                                                                                                                                                                                                                 We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .  \n",
              "1389                                                                                                                                                                                                                                                                                 We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .  \n",
              "1390                                                                                                                                                                                                                                                                                 We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .  \n",
              "1391                                                                                                                                                                                                                                                                                 We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .  \n",
              "1392                                                                                                                                                                                                                                                                                 We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .  \n",
              "1393                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We used warmup steps 4000 .  \n",
              "1394                                                                                                                                                                                                                                                                                                                                                                                                                          5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .  \n",
              "1395                                                                                                                                                                                                                                                                                                                                                                                                                          5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .  \n",
              "1396                                                                                                                                                                                                                                                                                                                                                                                                                          5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .  \n",
              "1397                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .  \n",
              "1398                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .  \n",
              "1399                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .  \n",
              "1400                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .  \n",
              "1401                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .  \n",
              "1402                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .  \n",
              "1403                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .  \n",
              "1404                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       For the base model , we use a rate of Pdrop 0.1 .  \n",
              "1405                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       For the base model , we use a rate of Pdrop 0.1 .  \n",
              "1406                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       For the base model , we use a rate of Pdrop 0.1 .  \n",
              "1407                                                                                              Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .  \n",
              "1408                                                                                              Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .  \n",
              "1409                                                                                              Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .  \n",
              "1410                                                                                              Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .  \n",
              "1411                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This hurts perplexity , as the model learns to be more unsure , but improves accuracy and BLEU score .  \n",
              "1412                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This hurts perplexity , as the model learns to be more unsure , but improves accuracy and BLEU score .  \n",
              "1413                                                                                                                                                                                                                                                                                                                                        6 Results 6.1 Machine Translation On the WMT 2014 English to German translation task , the big transformer model Transformer big in Table 2 outperforms the best previously reported models including ensembles by more than 2.0 BLEU , establishing a new state of the art BLEU score of 28.4 .  \n",
              "1414                                                                                                                                                                                                                                                                                                                                        6 Results 6.1 Machine Translation On the WMT 2014 English to German translation task , the big transformer model Transformer big in Table 2 outperforms the best previously reported models including ensembles by more than 2.0 BLEU , establishing a new state of the art BLEU score of 28.4 .  \n",
              "1415                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The configuration of this model is listed in the bottom line of Table 3 .  \n",
              "1416                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The configuration of this model is listed in the bottom line of Table 3 .  \n",
              "1417                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The configuration of this model is listed in the bottom line of Table 3 .  \n",
              "1418                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The configuration of this model is listed in the bottom line of Table 3 .  \n",
              "1419                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The configuration of this model is listed in the bottom line of Table 3 .  \n",
              "1420                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Training took 3.5 days on 8 P100 GPUs .  \n",
              "1421                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Training took 3.5 days on 8 P100 GPUs .  \n",
              "1422                                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .  \n",
              "1423                                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .  \n",
              "1424                                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .  \n",
              "1425                                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .  \n",
              "1426                                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .  \n",
              "1427                                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .  \n",
              "1428                                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .  \n",
              "1429                                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .  \n",
              "1430                                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .  \n",
              "1431                                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .  \n",
              "1432                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .  \n",
              "1433                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .  \n",
              "1434                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .  \n",
              "1435                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .  \n",
              "1436                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .  \n",
              "1437                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .  \n",
              "1438                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For the big models , we averaged the last 20 checkpoints .  \n",
              "1439                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For the big models , we averaged the last 20 checkpoints .  \n",
              "1440                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We used beam search with a beam size of 4 and length penalty 0.6 31 .  \n",
              "1441                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We used beam search with a beam size of 4 and length penalty 0.6 31 .  \n",
              "1442                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We used beam search with a beam size of 4 and length penalty 0.6 31 .  \n",
              "1443                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        These hyperparameters were chosen after experimentation on the development set .  \n",
              "1444                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        These hyperparameters were chosen after experimentation on the development set .  \n",
              "1445                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        These hyperparameters were chosen after experimentation on the development set .  \n",
              "1446                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1447                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1448                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1449                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1450                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1451                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1452                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1453                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1454                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1455                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1456                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1457                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1458                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1459                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "1460                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "1461                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "1462                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "1463                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "1464                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "1465                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "1466                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "1467                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "1468                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "1469                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "1470                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "1471                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "1472                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1473                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1474                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1475                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1476                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1477                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1478                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1479                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1480                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1481                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1482                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1483                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1484                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1485                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1486                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1487                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1488                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1489                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1490                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1491                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1492                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1493                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1494                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1495                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1496                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1497                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1498                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1499                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1500                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1501                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "1502                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                We used beam search as described in the previous section , but no checkpoint averaging .  \n",
              "1503                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                We used beam search as described in the previous section , but no checkpoint averaging .  \n",
              "1504                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We present these results in Table 3 .  \n",
              "1505                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We present these results in Table 3 .  \n",
              "1506                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We present these results in Table 3 .  \n",
              "1507                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "1508                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "1509                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "1510                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "1511                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "1512                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "1513                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "1514                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "1515                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "1516                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "1517                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1518                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1519                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1520                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1521                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1522                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1523                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1524                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1525                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1526                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1527                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1528                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1529                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1530                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1531                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1532                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1533                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1534                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "1535                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .  \n",
              "1536                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .  \n",
              "1537                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .  \n",
              "1538                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .  \n",
              "1539                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .  \n",
              "1540                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .  \n",
              "1541                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Unlisted values are identical to those of the base model .  \n",
              "1542                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Unlisted values are identical to those of the base model .  \n",
              "1543                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Unlisted values are identical to those of the base model .  \n",
              "1544                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Unlisted values are identical to those of the base model .  \n",
              "1545                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Unlisted values are identical to those of the base model .  \n",
              "1546                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Unlisted values are identical to those of the base model .  \n",
              "1547                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   All metrics are on the English to German translation development set , newstest2013 .  \n",
              "1548                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   All metrics are on the English to German translation development set , newstest2013 .  \n",
              "1549                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   All metrics are on the English to German translation development set , newstest2013 .  \n",
              "1550                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Listed perplexities are per wordpiece , according to our byte pair encoding , and should not be compared to per word perplexities .  \n",
              "1551                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Listed perplexities are per wordpiece , according to our byte pair encoding , and should not be compared to per word perplexities .  \n",
              "1552   N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .  \n",
              "1553   N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .  \n",
              "1554   N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .  \n",
              "1555   N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .  \n",
              "1556   N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .  \n",
              "1557                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We further observe in rows C and D that , as expected , bigger models are better , and dropout is very helpful in avoiding over fitting .  \n",
              "1558                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We further observe in rows C and D that , as expected , bigger models are better , and dropout is very helpful in avoiding over fitting .  \n",
              "1559                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .  \n",
              "1560                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .  \n",
              "1561                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .  \n",
              "1562                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .  \n",
              "1563                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .  \n",
              "1564                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .  \n",
              "1565                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .  \n",
              "1566                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .  \n",
              "1567                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "1568                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "1569                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "1570                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "1571                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "1572                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "1573                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "1574                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "1575                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "1576                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "1577                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .  \n",
              "1578                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .  \n",
              "1579                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .  \n",
              "1580                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .  \n",
              "1581                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .  \n",
              "1582                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .  \n",
              "1583                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .  \n",
              "1584                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .  \n",
              "1585                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .  \n",
              "1586                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .  \n",
              "1587                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In the former task our best model outperforms even all previously reported ensembles .  \n",
              "1588                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In the former task our best model outperforms even all previously reported ensembles .  \n",
              "1589                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In the former task our best model outperforms even all previously reported ensembles .  \n",
              "1590                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In the former task our best model outperforms even all previously reported ensembles .  \n",
              "1591                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We are excited about the future of attention based models and plan to apply them to other tasks .  \n",
              "1592                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We are excited about the future of attention based models and plan to apply them to other tasks .  \n",
              "1593                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We are excited about the future of attention based models and plan to apply them to other tasks .  \n",
              "1594                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We are excited about the future of attention based models and plan to apply them to other tasks .  \n",
              "1595                                                                                                                                                                                                                                                                                                                                                                                        We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .  \n",
              "1596                                                                                                                                                                                                                                                                                                                                                                                        We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .  \n",
              "1597                                                                                                                                                                                                                                                                                                                                                                                        We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .  \n",
              "1598                                                                                                                                                                                                                                                                                                                                                                                        We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .  \n",
              "1599                                                                                                                                                                                                                                                                                                                                                                                        We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .  \n",
              "1600                                                                                                                                                                                                                                                                                                                                                                                        We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .  \n",
              "1601                                                                                                                                                                                                                                                                                                                                                                                        We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .  \n",
              "1602                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .  \n",
              "1603                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .  \n",
              "1604                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .  \n",
              "1605                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .  \n",
              "1606                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .  \n",
              "1607                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .  \n",
              "1608                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .  \n",
              "1609                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .  \n",
              "1610                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-86d94000-0247-48ab-80af-af85737b31ed\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject</th>\n",
              "      <th>relation</th>\n",
              "      <th>object</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Attention</td>\n",
              "      <td>is</td>\n",
              "      <td>All</td>\n",
              "      <td>Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>convolutional</td>\n",
              "      <td>include</td>\n",
              "      <td>encoder</td>\n",
              "      <td>Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>convolutional</td>\n",
              "      <td>include</td>\n",
              "      <td>decoder</td>\n",
              "      <td>Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Attention</td>\n",
              "      <td>is All</td>\n",
              "      <td>you Need</td>\n",
              "      <td>Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>you</td>\n",
              "      <td>Need</td>\n",
              "      <td>Attention</td>\n",
              "      <td>Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>performing models</td>\n",
              "      <td>connect</td>\n",
              "      <td>encoder</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>performing models</td>\n",
              "      <td>also connect decoder through</td>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>models</td>\n",
              "      <td>also connect encoder through</td>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>performing models</td>\n",
              "      <td>also connect encoder through</td>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>models</td>\n",
              "      <td>also connect</td>\n",
              "      <td>encoder</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>models</td>\n",
              "      <td>also connect decoder through</td>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>performing models</td>\n",
              "      <td>also connect</td>\n",
              "      <td>encoder</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>performing models</td>\n",
              "      <td>connect decoder through</td>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>performing models</td>\n",
              "      <td>connect</td>\n",
              "      <td>decoder</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>models</td>\n",
              "      <td>connect</td>\n",
              "      <td>encoder</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>models</td>\n",
              "      <td>also connect</td>\n",
              "      <td>decoder</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>models</td>\n",
              "      <td>connect encoder through</td>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>models</td>\n",
              "      <td>connect decoder through</td>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>performing models</td>\n",
              "      <td>connect encoder through</td>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>models</td>\n",
              "      <td>connect</td>\n",
              "      <td>decoder</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>performing models</td>\n",
              "      <td>also connect</td>\n",
              "      <td>decoder</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>network architecture based solely on attention mechanisms dispensing with recurrence</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new network architecture based on attention mechanisms dispensing entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new network architecture based solely on attention mechanisms</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>simple network architecture based solely on attention mechanisms dispensing with recurrence</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>simple network architecture based on attention mechanisms</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>network architecture based on attention mechanisms</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>network architecture based on attention mechanisms dispensing with recurrence entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new network architecture based on attention mechanisms</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>network architecture</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>Transformer</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>simple network architecture based on attention mechanisms dispensing with recurrence</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new network architecture based solely on attention mechanisms dispensing</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new simple network architecture based on attention mechanisms dispensing entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new network architecture based solely on attention mechanisms dispensing with recurrence entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>network architecture based on attention mechanisms dispensing</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new network architecture based on attention mechanisms dispensing with recurrence entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new simple network architecture</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>network architecture based solely on attention mechanisms dispensing with recurrence entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>simple network architecture based solely on attention mechanisms dispensing entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new network architecture based solely on attention mechanisms dispensing entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>simple network architecture based solely on attention mechanisms dispensing</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>network architecture based on attention mechanisms dispensing entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new simple network architecture based solely on attention mechanisms dispensing entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>simple network architecture</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>network architecture based solely on attention mechanisms</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new simple network architecture based solely on attention mechanisms dispensing with recurrence</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new simple network architecture based solely on attention mechanisms dispensing with recurrence entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>network architecture based solely on attention mechanisms dispensing entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>network architecture based solely on attention mechanisms dispensing</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new simple network architecture based on attention mechanisms dispensing</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>simple network architecture based on attention mechanisms dispensing entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>simple network architecture based solely on attention mechanisms dispensing with recurrence entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>simple network architecture based on attention mechanisms dispensing with recurrence entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new simple network architecture based on attention mechanisms dispensing with recurrence entirely</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new network architecture based on attention mechanisms dispensing with recurrence</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new simple network architecture based on attention mechanisms</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>simple network architecture based on attention mechanisms dispensing</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new network architecture based on attention mechanisms dispensing</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new simple network architecture based on attention mechanisms dispensing with recurrence</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new network architecture based solely on attention mechanisms dispensing with recurrence</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>simple network architecture based solely on attention mechanisms</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new network architecture</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new simple network architecture based solely on attention mechanisms</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>network architecture based on attention mechanisms dispensing with recurrence</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new simple network architecture based solely on attention mechanisms dispensing</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>existing best results including ensembles</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>results by over 2 BLEU</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>best results by over 2 BLEU</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>existing best results including ensembles by over 2 BLEU</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>existing results by over 2 BLEU</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>existing best results by over 2 BLEU</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>best results including ensembles</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>results including ensembles by over 2 BLEU</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>existing results including ensembles</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>Our model</td>\n",
              "      <td>achieves</td>\n",
              "      <td>28.4 BLEU on WMT 2014 Englishto German translation task</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>results</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>existing best results</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>best results including ensembles by over 2 BLEU</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>Our model</td>\n",
              "      <td>achieves</td>\n",
              "      <td>28.4 BLEU</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>results including ensembles</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>best results</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>existing results</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>existing results including ensembles by over 2 BLEU</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>small fraction of training costs of models</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>new model state of art BLEU score of 41.0</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>new single model state</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>single model state of art BLEU score</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>model state of art BLEU score of 41.0</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>new single model state of art BLEU score of 41.0</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>single model state</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes model state after</td>\n",
              "      <td>training</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>new model state</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>small fraction of training costs of models from literature</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>model state</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>model state of art BLEU score</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>single model state of art BLEU score of 41.0</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>fraction of training costs</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes model state On</td>\n",
              "      <td>WMT 2014 English</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes model state On</td>\n",
              "      <td>WMT 2014 English to translation task</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>small fraction of training costs of best models</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>fraction</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>fraction of training costs of best models from literature</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>small fraction of training costs of best models from literature</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>small fraction of training costs</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>new model state of art BLEU score</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>small fraction</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>fraction of training costs of models from literature</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>fraction of training costs from literature</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes fraction On</td>\n",
              "      <td>WMT 2014 English to translation task</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes model state On</td>\n",
              "      <td>WMT 2014 English to French translation task</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>fraction of training costs of best models</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>fraction of training costs of models</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes fraction On</td>\n",
              "      <td>WMT 2014 English to French translation task</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>small fraction of training costs from literature</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes fraction after</td>\n",
              "      <td>training</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes fraction On</td>\n",
              "      <td>WMT 2014 English</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>new single model state of art BLEU score</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>1 Introduction Recurrent neural networks</td>\n",
              "      <td>memory in</td>\n",
              "      <td>particular</td>\n",
              "      <td>1 Introduction Recurrent neural networks , long short term memory 12 and gated recurrent 7 neural networks in particular , have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation 29 , 2 , 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>Numerous efforts</td>\n",
              "      <td>push</td>\n",
              "      <td>boundaries of recurrent language models</td>\n",
              "      <td>Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>efforts</td>\n",
              "      <td>push</td>\n",
              "      <td>boundaries</td>\n",
              "      <td>Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>efforts</td>\n",
              "      <td>push</td>\n",
              "      <td>boundaries of recurrent language models</td>\n",
              "      <td>Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>efforts</td>\n",
              "      <td>push</td>\n",
              "      <td>boundaries of language models</td>\n",
              "      <td>Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>Numerous efforts</td>\n",
              "      <td>push</td>\n",
              "      <td>boundaries of language models</td>\n",
              "      <td>Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>Numerous efforts</td>\n",
              "      <td>push</td>\n",
              "      <td>boundaries</td>\n",
              "      <td>Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>order</td>\n",
              "      <td>is</td>\n",
              "      <td>random</td>\n",
              "      <td>Listing order is random .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>Listing order</td>\n",
              "      <td>is</td>\n",
              "      <td>random</td>\n",
              "      <td>Listing order is random .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>Jakob</td>\n",
              "      <td>replacing</td>\n",
              "      <td>RNNs</td>\n",
              "      <td>Jakob proposed replacing RNNs with self attention and started the effort to evaluate this idea .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>Jakob</td>\n",
              "      <td>replacing RNNs with</td>\n",
              "      <td>self attention</td>\n",
              "      <td>Jakob proposed replacing RNNs with self attention and started the effort to evaluate this idea .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>Ashish</td>\n",
              "      <td>is with</td>\n",
              "      <td>Illia designed</td>\n",
              "      <td>Ashish , with Illia , designed and implemented the first Transformer models and has been crucially involved in every aspect of this work .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>Noam</td>\n",
              "      <td>became</td>\n",
              "      <td>other person involved in nearly detail</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>Noam</td>\n",
              "      <td>became</td>\n",
              "      <td>person involved in nearly detail</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>Noam</td>\n",
              "      <td>became</td>\n",
              "      <td>person</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>other person</td>\n",
              "      <td>involved in</td>\n",
              "      <td>detail</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>person</td>\n",
              "      <td>involved in</td>\n",
              "      <td>nearly detail</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>Noam</td>\n",
              "      <td>proposed</td>\n",
              "      <td>multi head attention</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>Noam</td>\n",
              "      <td>proposed</td>\n",
              "      <td>head attention</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>Noam</td>\n",
              "      <td>became</td>\n",
              "      <td>person involved in detail</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>Noam</td>\n",
              "      <td>proposed</td>\n",
              "      <td>scaled dot product attention</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>person</td>\n",
              "      <td>involved in</td>\n",
              "      <td>detail</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>Noam</td>\n",
              "      <td>proposed</td>\n",
              "      <td>parameter free position representation</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>Noam</td>\n",
              "      <td>proposed</td>\n",
              "      <td>dot product attention</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>Noam</td>\n",
              "      <td>became</td>\n",
              "      <td>other person involved in detail</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>Noam</td>\n",
              "      <td>proposed</td>\n",
              "      <td>parameter position representation</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>other person</td>\n",
              "      <td>involved in</td>\n",
              "      <td>nearly detail</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>Noam</td>\n",
              "      <td>became</td>\n",
              "      <td>other person</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>evaluated countless model variants</td>\n",
              "      <td>is in</td>\n",
              "      <td>our original codebase</td>\n",
              "      <td>Niki designed , implemented , tuned and evaluated countless model variants in our original codebase and tensor2tensor .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>Llion</td>\n",
              "      <td>experimented with</td>\n",
              "      <td>novel model variants</td>\n",
              "      <td>Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>Llion</td>\n",
              "      <td>experimented with</td>\n",
              "      <td>model variants</td>\n",
              "      <td>Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>Llion</td>\n",
              "      <td>also experimented with</td>\n",
              "      <td>novel model variants</td>\n",
              "      <td>Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>Llion</td>\n",
              "      <td>also experimented with</td>\n",
              "      <td>model variants</td>\n",
              "      <td>Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>Lukasz</td>\n",
              "      <td>spent at_time</td>\n",
              "      <td>days</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>replacing</td>\n",
              "      <td>our earlier codebase</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>replacing</td>\n",
              "      <td>our codebase</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>spent at_time</td>\n",
              "      <td>days</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>results</td>\n",
              "      <td>accelerating</td>\n",
              "      <td>our research</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>designing</td>\n",
              "      <td>various parts of tensor2tensor</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>Lukasz</td>\n",
              "      <td>spent at_time</td>\n",
              "      <td>countless long days</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>results</td>\n",
              "      <td>massively accelerating</td>\n",
              "      <td>our research</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>Lukasz</td>\n",
              "      <td>spent at_time</td>\n",
              "      <td>long days</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>designing</td>\n",
              "      <td>various parts</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>designing</td>\n",
              "      <td>parts of tensor2tensor</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>spent at_time</td>\n",
              "      <td>countless days</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>Lukasz</td>\n",
              "      <td>spent at_time</td>\n",
              "      <td>countless days</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>spent at_time</td>\n",
              "      <td>long days</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>spent at_time</td>\n",
              "      <td>countless long days</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>designing</td>\n",
              "      <td>parts</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>Work</td>\n",
              "      <td>performed at</td>\n",
              "      <td>Google Research</td>\n",
              "      <td>Work performed while at Google Research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>31st Conference</td>\n",
              "      <td>NIPS</td>\n",
              "      <td>Long Beach</td>\n",
              "      <td>31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>Conference</td>\n",
              "      <td>NIPS</td>\n",
              "      <td>CA</td>\n",
              "      <td>31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>Conference</td>\n",
              "      <td>NIPS</td>\n",
              "      <td>2017</td>\n",
              "      <td>31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>Conference</td>\n",
              "      <td>NIPS</td>\n",
              "      <td>Long Beach</td>\n",
              "      <td>31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>31st Conference</td>\n",
              "      <td>NIPS</td>\n",
              "      <td>2017</td>\n",
              "      <td>31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>31st Conference</td>\n",
              "      <td>NIPS</td>\n",
              "      <td>CA</td>\n",
              "      <td>31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>models</td>\n",
              "      <td>factor</td>\n",
              "      <td>computation along symbol positions of input sequences</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>Recurrent models</td>\n",
              "      <td>factor</td>\n",
              "      <td>computation along symbol positions of input sequences</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>Recurrent models</td>\n",
              "      <td>typically factor</td>\n",
              "      <td>computation along symbol positions of input sequences</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>models</td>\n",
              "      <td>factor</td>\n",
              "      <td>computation along symbol positions</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>models</td>\n",
              "      <td>typically factor</td>\n",
              "      <td>computation</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>models</td>\n",
              "      <td>factor</td>\n",
              "      <td>computation</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>Recurrent models</td>\n",
              "      <td>factor</td>\n",
              "      <td>computation</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>models</td>\n",
              "      <td>typically factor</td>\n",
              "      <td>computation along symbol positions of input sequences</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>Recurrent models</td>\n",
              "      <td>typically factor</td>\n",
              "      <td>computation along symbol positions</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>models</td>\n",
              "      <td>typically factor</td>\n",
              "      <td>computation along symbol positions</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>Recurrent models</td>\n",
              "      <td>typically factor</td>\n",
              "      <td>computation</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>Recurrent models</td>\n",
              "      <td>factor</td>\n",
              "      <td>computation along symbol positions</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>they</td>\n",
              "      <td>Aligning positions to</td>\n",
              "      <td>steps</td>\n",
              "      <td>Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>they</td>\n",
              "      <td>Aligning positions to</td>\n",
              "      <td>steps in computation time</td>\n",
              "      <td>Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>they</td>\n",
              "      <td>Aligning</td>\n",
              "      <td>positions</td>\n",
              "      <td>Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>sequence</td>\n",
              "      <td>ht as</td>\n",
              "      <td>function of previous state ht 1</td>\n",
              "      <td>Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>sequence</td>\n",
              "      <td>ht as</td>\n",
              "      <td>function</td>\n",
              "      <td>Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>steps</td>\n",
              "      <td>is in</td>\n",
              "      <td>computation time</td>\n",
              "      <td>Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>sequence</td>\n",
              "      <td>ht as</td>\n",
              "      <td>function of state ht 1</td>\n",
              "      <td>Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>sequence</td>\n",
              "      <td>ht as</td>\n",
              "      <td>function of previous hidden state ht 1</td>\n",
              "      <td>Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>sequence</td>\n",
              "      <td>ht as</td>\n",
              "      <td>function of hidden state ht 1</td>\n",
              "      <td>Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>memory constraints</td>\n",
              "      <td>limit</td>\n",
              "      <td>batching</td>\n",
              "      <td>This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>sequential nature</td>\n",
              "      <td>precludes</td>\n",
              "      <td>parallelization within training examples</td>\n",
              "      <td>This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>nature</td>\n",
              "      <td>precludes</td>\n",
              "      <td>parallelization</td>\n",
              "      <td>This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>sequential nature</td>\n",
              "      <td>precludes parallelization</td>\n",
              "      <td>memory constraints limit</td>\n",
              "      <td>This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>memory constraints</td>\n",
              "      <td>batching across</td>\n",
              "      <td>examples</td>\n",
              "      <td>This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>nature</td>\n",
              "      <td>precludes</td>\n",
              "      <td>parallelization within training examples</td>\n",
              "      <td>This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>nature</td>\n",
              "      <td>precludes parallelization</td>\n",
              "      <td>memory constraints limit</td>\n",
              "      <td>This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>sequential nature</td>\n",
              "      <td>precludes</td>\n",
              "      <td>parallelization</td>\n",
              "      <td>This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>memory constraints</td>\n",
              "      <td>limit</td>\n",
              "      <td>batching across examples</td>\n",
              "      <td>This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>work</td>\n",
              "      <td>also improving model performance in case of</td>\n",
              "      <td>latter</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>improving model performance in case of</td>\n",
              "      <td>latter</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>improvements in efficiency</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>work</td>\n",
              "      <td>improving model performance in case of</td>\n",
              "      <td>latter</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>also improving model performance in case of</td>\n",
              "      <td>latter</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>significant improvements in computational efficiency</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>significant improvements in computational efficiency</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>work</td>\n",
              "      <td>has achieved improvements through</td>\n",
              "      <td>factorization tricks 18</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>work</td>\n",
              "      <td>also improving</td>\n",
              "      <td>model performance</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>significant improvements</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>work</td>\n",
              "      <td>improving</td>\n",
              "      <td>model performance</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>significant improvements in efficiency</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>improvements in computational efficiency</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>has achieved improvements through</td>\n",
              "      <td>factorization tricks 18</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>significant improvements in efficiency</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>significant improvements</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>improving</td>\n",
              "      <td>model performance</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>also improving</td>\n",
              "      <td>model performance</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>improvements in computational efficiency</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>significant improvements</td>\n",
              "      <td>is in</td>\n",
              "      <td>computational efficiency</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>improvements</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>improvements</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>improvements in efficiency</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>allowing without regard</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>allowing</td>\n",
              "      <td>modeling</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>part of</td>\n",
              "      <td>sequence modeling in various tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>integral part of</td>\n",
              "      <td>compelling sequence modeling in tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>compelling sequence modeling</td>\n",
              "      <td>is in</td>\n",
              "      <td>various tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>part of sequence modeling in tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>allowing modeling without</td>\n",
              "      <td>regard</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>allowing without</td>\n",
              "      <td>regard to their distance</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>integral part of compelling sequence modeling in various tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>part of</td>\n",
              "      <td>sequence modeling</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>part of</td>\n",
              "      <td>compelling sequence modeling in tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>part of compelling sequence modeling in various tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>integral part</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>allowing modeling without</td>\n",
              "      <td>regard to their distance in input sequences</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>allowing without</td>\n",
              "      <td>regard to their distance in input sequences</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>allowing without regard to their distance</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>allowing</td>\n",
              "      <td>modeling of dependencies</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>integral part of compelling sequence modeling</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>part</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>integral part of sequence modeling in tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>allowing modeling without</td>\n",
              "      <td>regard to their distance</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>integral part of</td>\n",
              "      <td>compelling sequence modeling</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>their distance</td>\n",
              "      <td>is in</td>\n",
              "      <td>input sequences</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>part of</td>\n",
              "      <td>sequence modeling in tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>integral part of sequence modeling</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>part of compelling sequence modeling</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>part of</td>\n",
              "      <td>compelling sequence modeling</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>allowing</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>part of sequence modeling in various tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>integral part of compelling sequence modeling in tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>integral part of</td>\n",
              "      <td>sequence modeling</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>integral part of sequence modeling in various tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>integral part of</td>\n",
              "      <td>sequence modeling in various tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>integral part of</td>\n",
              "      <td>compelling sequence modeling in various tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>part of</td>\n",
              "      <td>compelling sequence modeling in various tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>allowing without</td>\n",
              "      <td>regard</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>allowing without regard to their distance in input sequences</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>part of sequence modeling</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>part of compelling sequence modeling in tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>integral part of</td>\n",
              "      <td>sequence modeling in tasks</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>attention mechanisms</td>\n",
              "      <td>are</td>\n",
              "      <td>however used</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>such attention mechanisms</td>\n",
              "      <td>are used in</td>\n",
              "      <td>conjunction with recurrent network</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>such attention mechanisms</td>\n",
              "      <td>are used in</td>\n",
              "      <td>conjunction</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>attention mechanisms</td>\n",
              "      <td>are used in</td>\n",
              "      <td>conjunction with network</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>attention mechanisms</td>\n",
              "      <td>however are used in</td>\n",
              "      <td>conjunction with network</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>attention mechanisms</td>\n",
              "      <td>however are used in</td>\n",
              "      <td>conjunction with recurrent network</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>such attention mechanisms</td>\n",
              "      <td>however are used in</td>\n",
              "      <td>conjunction</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>attention mechanisms</td>\n",
              "      <td>however are used in</td>\n",
              "      <td>conjunction</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>attention mechanisms</td>\n",
              "      <td>are used in</td>\n",
              "      <td>conjunction</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>conjunction</td>\n",
              "      <td>is with</td>\n",
              "      <td>recurrent network</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>such attention mechanisms</td>\n",
              "      <td>are</td>\n",
              "      <td>used</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>such attention mechanisms</td>\n",
              "      <td>however are used in</td>\n",
              "      <td>conjunction with recurrent network</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>such attention mechanisms</td>\n",
              "      <td>are</td>\n",
              "      <td>however used</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>such attention mechanisms</td>\n",
              "      <td>however are used in</td>\n",
              "      <td>conjunction with network</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>attention mechanisms</td>\n",
              "      <td>are used in</td>\n",
              "      <td>conjunction with recurrent network</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>such attention mechanisms</td>\n",
              "      <td>are used in</td>\n",
              "      <td>conjunction with network</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>attention mechanisms</td>\n",
              "      <td>are</td>\n",
              "      <td>used</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>we</td>\n",
              "      <td>propose model architecture In</td>\n",
              "      <td>work</td>\n",
              "      <td>In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>we</td>\n",
              "      <td>propose</td>\n",
              "      <td>relying</td>\n",
              "      <td>In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>we</td>\n",
              "      <td>propose</td>\n",
              "      <td>Transformer</td>\n",
              "      <td>In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>we</td>\n",
              "      <td>propose</td>\n",
              "      <td>instead relying</td>\n",
              "      <td>In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>we</td>\n",
              "      <td>propose</td>\n",
              "      <td>relying entirely</td>\n",
              "      <td>In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>we</td>\n",
              "      <td>propose</td>\n",
              "      <td>model architecture</td>\n",
              "      <td>In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>we</td>\n",
              "      <td>propose</td>\n",
              "      <td>instead relying entirely</td>\n",
              "      <td>In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>we</td>\n",
              "      <td>propose Transformer In</td>\n",
              "      <td>work</td>\n",
              "      <td>In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>reach state in</td>\n",
              "      <td>translation quality</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>reach state</td>\n",
              "      <td>trained</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>reach state</td>\n",
              "      <td>trained for as little as twelve hours</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>reach</td>\n",
              "      <td>new state of art</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>allows for</td>\n",
              "      <td>parallelization</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>reach</td>\n",
              "      <td>state</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>reach state</td>\n",
              "      <td>trained for as little as twelve hours on eight P100 GPUs</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>reach</td>\n",
              "      <td>state of art</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>allows for</td>\n",
              "      <td>more parallelization</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>reach</td>\n",
              "      <td>new state</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>allows for</td>\n",
              "      <td>significantly more parallelization</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>all</td>\n",
              "      <td>use</td>\n",
              "      <td>networks</td>\n",
              "      <td>2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>all</td>\n",
              "      <td>use networks as</td>\n",
              "      <td>basic building block</td>\n",
              "      <td>2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>all</td>\n",
              "      <td>use</td>\n",
              "      <td>convolutional networks</td>\n",
              "      <td>2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>all</td>\n",
              "      <td>use</td>\n",
              "      <td>convolutional neural networks</td>\n",
              "      <td>2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>all</td>\n",
              "      <td>use networks as</td>\n",
              "      <td>building block</td>\n",
              "      <td>2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>all</td>\n",
              "      <td>use</td>\n",
              "      <td>neural networks</td>\n",
              "      <td>2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>number</td>\n",
              "      <td>grows In</td>\n",
              "      <td>models</td>\n",
              "      <td>In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>number</td>\n",
              "      <td>grows for</td>\n",
              "      <td>linearly ConvS2S</td>\n",
              "      <td>In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>number</td>\n",
              "      <td>grows in</td>\n",
              "      <td>distance</td>\n",
              "      <td>In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>number</td>\n",
              "      <td>grows in</td>\n",
              "      <td>distance between positions</td>\n",
              "      <td>In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>number</td>\n",
              "      <td>grows for</td>\n",
              "      <td>ConvS2S</td>\n",
              "      <td>In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>it</td>\n",
              "      <td>learn</td>\n",
              "      <td>dependencies between distant positions 11</td>\n",
              "      <td>This makes it more difficult to learn dependencies between distant positions 11 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>it</td>\n",
              "      <td>dependencies between</td>\n",
              "      <td>positions 11</td>\n",
              "      <td>This makes it more difficult to learn dependencies between distant positions 11 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>it</td>\n",
              "      <td>learn</td>\n",
              "      <td>dependencies</td>\n",
              "      <td>This makes it more difficult to learn dependencies between distant positions 11 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>317</th>\n",
              "      <td>it</td>\n",
              "      <td>learn</td>\n",
              "      <td>dependencies between positions 11</td>\n",
              "      <td>This makes it more difficult to learn dependencies between distant positions 11 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>318</th>\n",
              "      <td>it</td>\n",
              "      <td>dependencies between</td>\n",
              "      <td>distant positions 11</td>\n",
              "      <td>This makes it more difficult to learn dependencies between distant positions 11 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>relating</td>\n",
              "      <td>different positions</td>\n",
              "      <td>Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>relating</td>\n",
              "      <td>positions of sequence</td>\n",
              "      <td>Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>Self attention</td>\n",
              "      <td>is</td>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>relating</td>\n",
              "      <td>positions of single sequence</td>\n",
              "      <td>Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>relating</td>\n",
              "      <td>different positions of single sequence</td>\n",
              "      <td>Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>324</th>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>relating</td>\n",
              "      <td>different positions of sequence</td>\n",
              "      <td>Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325</th>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>relating</td>\n",
              "      <td>positions</td>\n",
              "      <td>Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326</th>\n",
              "      <td>Self attention</td>\n",
              "      <td>has</td>\n",
              "      <td>has used successfully</td>\n",
              "      <td>Self attention has been used successfully in a variety of tasks including reading comprehension , abstractive summarization , textual entailment and learning task independent sentence representations 4 , 22 , 23 , 19 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>327</th>\n",
              "      <td>Self attention</td>\n",
              "      <td>has</td>\n",
              "      <td>has used</td>\n",
              "      <td>Self attention has been used successfully in a variety of tasks including reading comprehension , abstractive summarization , textual entailment and learning task independent sentence representations 4 , 22 , 23 , 19 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>328</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>is</td>\n",
              "      <td>first transduction model relying entirely on self attention</td>\n",
              "      <td>To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>is</td>\n",
              "      <td>transduction model relying</td>\n",
              "      <td>To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>is</td>\n",
              "      <td>first transduction model</td>\n",
              "      <td>To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>is</td>\n",
              "      <td>transduction model relying on self attention</td>\n",
              "      <td>To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>is</td>\n",
              "      <td>transduction model relying entirely on self attention</td>\n",
              "      <td>To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>is</td>\n",
              "      <td>first transduction model relying</td>\n",
              "      <td>To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>334</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>is</td>\n",
              "      <td>first transduction model relying on self attention</td>\n",
              "      <td>To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>is</td>\n",
              "      <td>first transduction model relying entirely</td>\n",
              "      <td>To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>is</td>\n",
              "      <td>transduction model</td>\n",
              "      <td>To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>337</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>is</td>\n",
              "      <td>transduction model relying entirely</td>\n",
              "      <td>To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>we</td>\n",
              "      <td>discuss</td>\n",
              "      <td>its advantages</td>\n",
              "      <td>In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>we</td>\n",
              "      <td>will describe</td>\n",
              "      <td>Transformer</td>\n",
              "      <td>In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>340</th>\n",
              "      <td>we</td>\n",
              "      <td>will describe Transformer In</td>\n",
              "      <td>following sections</td>\n",
              "      <td>In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>341</th>\n",
              "      <td>we</td>\n",
              "      <td>will describe Transformer In</td>\n",
              "      <td>sections</td>\n",
              "      <td>In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>342</th>\n",
              "      <td>we</td>\n",
              "      <td>motivate</td>\n",
              "      <td>self attention</td>\n",
              "      <td>In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>encoder</td>\n",
              "      <td>Here maps</td>\n",
              "      <td>input sequence of symbol representations x1</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>344</th>\n",
              "      <td>encoder</td>\n",
              "      <td>Here maps</td>\n",
              "      <td>xn to sequence of continuous representations</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>345</th>\n",
              "      <td>encoder</td>\n",
              "      <td>maps</td>\n",
              "      <td>xn to sequence of representations</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>encoder</td>\n",
              "      <td>maps</td>\n",
              "      <td>xn to sequence</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>encoder</td>\n",
              "      <td>Here maps</td>\n",
              "      <td>input sequence</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>encoder</td>\n",
              "      <td>maps</td>\n",
              "      <td>input sequence of symbol representations x1</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>encoder</td>\n",
              "      <td>Here maps</td>\n",
              "      <td>xn to sequence of representations</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>encoder</td>\n",
              "      <td>Here maps</td>\n",
              "      <td>xn</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>351</th>\n",
              "      <td>encoder</td>\n",
              "      <td>maps</td>\n",
              "      <td>xn</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>352</th>\n",
              "      <td>encoder</td>\n",
              "      <td>maps</td>\n",
              "      <td>input sequence</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>353</th>\n",
              "      <td>encoder</td>\n",
              "      <td>Here maps</td>\n",
              "      <td>xn to sequence</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>encoder</td>\n",
              "      <td>maps</td>\n",
              "      <td>xn to sequence of continuous representations</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>decoder</td>\n",
              "      <td>generates</td>\n",
              "      <td>ym of symbols one element</td>\n",
              "      <td>Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>decoder</td>\n",
              "      <td>generates</td>\n",
              "      <td>ym of symbols one element at time</td>\n",
              "      <td>Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>decoder</td>\n",
              "      <td>generates output sequence y1 Given</td>\n",
              "      <td>z</td>\n",
              "      <td>Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>output sequence y1</td>\n",
              "      <td>ym of</td>\n",
              "      <td>symbols one element</td>\n",
              "      <td>Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>decoder</td>\n",
              "      <td>generates</td>\n",
              "      <td>output sequence y1</td>\n",
              "      <td>Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>360</th>\n",
              "      <td>output sequence y1</td>\n",
              "      <td>ym at</td>\n",
              "      <td>time</td>\n",
              "      <td>Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>361</th>\n",
              "      <td>decoder</td>\n",
              "      <td>generates</td>\n",
              "      <td>ym</td>\n",
              "      <td>Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>decoder</td>\n",
              "      <td>generates</td>\n",
              "      <td>ym at time</td>\n",
              "      <td>Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>363</th>\n",
              "      <td>model</td>\n",
              "      <td>consuming symbols as</td>\n",
              "      <td>additional input</td>\n",
              "      <td>At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>364</th>\n",
              "      <td>model</td>\n",
              "      <td>consuming</td>\n",
              "      <td>symbols</td>\n",
              "      <td>At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>365</th>\n",
              "      <td>model</td>\n",
              "      <td>consuming</td>\n",
              "      <td>generated symbols</td>\n",
              "      <td>At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>366</th>\n",
              "      <td>model</td>\n",
              "      <td>consuming</td>\n",
              "      <td>previously generated symbols</td>\n",
              "      <td>At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>367</th>\n",
              "      <td>model</td>\n",
              "      <td>consuming symbols as</td>\n",
              "      <td>input</td>\n",
              "      <td>At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>368</th>\n",
              "      <td>overall architecture</td>\n",
              "      <td>shown in</td>\n",
              "      <td>left halves Figure 1</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>overall architecture</td>\n",
              "      <td>shown in</td>\n",
              "      <td>halves</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>370</th>\n",
              "      <td>overall architecture</td>\n",
              "      <td>shown in</td>\n",
              "      <td>halves Figure 1</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>371</th>\n",
              "      <td>architecture</td>\n",
              "      <td>using</td>\n",
              "      <td>wise</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>372</th>\n",
              "      <td>overall architecture</td>\n",
              "      <td>using</td>\n",
              "      <td>self attention</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>architecture</td>\n",
              "      <td>using</td>\n",
              "      <td>self attention</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>374</th>\n",
              "      <td>overall architecture</td>\n",
              "      <td>using</td>\n",
              "      <td>wise</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>follows respectively</td>\n",
              "      <td>overall architecture</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>376</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>follows respectively</td>\n",
              "      <td>architecture</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>377</th>\n",
              "      <td>overall architecture</td>\n",
              "      <td>using</td>\n",
              "      <td>stacked self attention</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>378</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>follows</td>\n",
              "      <td>architecture</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>379</th>\n",
              "      <td>architecture</td>\n",
              "      <td>shown in</td>\n",
              "      <td>left halves</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380</th>\n",
              "      <td>architecture</td>\n",
              "      <td>using</td>\n",
              "      <td>point wise</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>381</th>\n",
              "      <td>architecture</td>\n",
              "      <td>shown in</td>\n",
              "      <td>halves</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>382</th>\n",
              "      <td>architecture</td>\n",
              "      <td>shown in</td>\n",
              "      <td>left halves Figure 1</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>architecture</td>\n",
              "      <td>using</td>\n",
              "      <td>stacked self attention</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>point</td>\n",
              "      <td>wise</td>\n",
              "      <td>fully connected layers for encoder</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>overall architecture</td>\n",
              "      <td>using</td>\n",
              "      <td>point wise</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>386</th>\n",
              "      <td>overall architecture</td>\n",
              "      <td>shown in</td>\n",
              "      <td>left halves</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>387</th>\n",
              "      <td>architecture</td>\n",
              "      <td>shown in</td>\n",
              "      <td>halves Figure 1</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>388</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>follows</td>\n",
              "      <td>overall architecture</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>layer</td>\n",
              "      <td>has</td>\n",
              "      <td>two sub layers</td>\n",
              "      <td>Each layer has two sub layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390</th>\n",
              "      <td>second</td>\n",
              "      <td>is</td>\n",
              "      <td>simple</td>\n",
              "      <td>The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>391</th>\n",
              "      <td>second</td>\n",
              "      <td>is</td>\n",
              "      <td>simple Figure 1</td>\n",
              "      <td>The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>392</th>\n",
              "      <td>first</td>\n",
              "      <td>is</td>\n",
              "      <td>head self attention mechanism</td>\n",
              "      <td>The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>second</td>\n",
              "      <td>is</td>\n",
              "      <td>position2</td>\n",
              "      <td>The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>394</th>\n",
              "      <td>first</td>\n",
              "      <td>is</td>\n",
              "      <td>multi</td>\n",
              "      <td>The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>second</td>\n",
              "      <td>is</td>\n",
              "      <td>position2 Figure 1</td>\n",
              "      <td>The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>first</td>\n",
              "      <td>is</td>\n",
              "      <td>multi head self attention mechanism</td>\n",
              "      <td>The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>second</td>\n",
              "      <td>is</td>\n",
              "      <td>Figure 1</td>\n",
              "      <td>The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>second</td>\n",
              "      <td>is</td>\n",
              "      <td>simple position2 Figure 1</td>\n",
              "      <td>The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>We</td>\n",
              "      <td>employ</td>\n",
              "      <td>residual connection 10</td>\n",
              "      <td>We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>400</th>\n",
              "      <td>We</td>\n",
              "      <td>followed by</td>\n",
              "      <td>layer normalization 1</td>\n",
              "      <td>We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>401</th>\n",
              "      <td>We</td>\n",
              "      <td>employ</td>\n",
              "      <td>residual connection 10 around each two sub layers</td>\n",
              "      <td>We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>402</th>\n",
              "      <td>We</td>\n",
              "      <td>employ</td>\n",
              "      <td>connection 10 around each of two sub layers</td>\n",
              "      <td>We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>403</th>\n",
              "      <td>We</td>\n",
              "      <td>employ</td>\n",
              "      <td>residual connection 10 around each of two sub layers</td>\n",
              "      <td>We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>404</th>\n",
              "      <td>We</td>\n",
              "      <td>employ</td>\n",
              "      <td>connection 10</td>\n",
              "      <td>We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>405</th>\n",
              "      <td>We</td>\n",
              "      <td>employ</td>\n",
              "      <td>connection 10 around each two sub layers</td>\n",
              "      <td>We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>406</th>\n",
              "      <td>Sublayer x</td>\n",
              "      <td>is</td>\n",
              "      <td>where function</td>\n",
              "      <td>That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>407</th>\n",
              "      <td>Sublayer x</td>\n",
              "      <td>implemented by</td>\n",
              "      <td>sub layer</td>\n",
              "      <td>That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>408</th>\n",
              "      <td>Sublayer x</td>\n",
              "      <td>is</td>\n",
              "      <td>where function implemented by sub layer itself</td>\n",
              "      <td>That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>409</th>\n",
              "      <td>Sublayer x</td>\n",
              "      <td>is</td>\n",
              "      <td>where function implemented</td>\n",
              "      <td>That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>410</th>\n",
              "      <td>Sublayer x</td>\n",
              "      <td>is</td>\n",
              "      <td>function implemented by sub layer itself</td>\n",
              "      <td>That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>Sublayer x</td>\n",
              "      <td>is</td>\n",
              "      <td>function implemented by sub layer</td>\n",
              "      <td>That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>412</th>\n",
              "      <td>Sublayer x</td>\n",
              "      <td>implemented by</td>\n",
              "      <td>sub layer itself</td>\n",
              "      <td>That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>413</th>\n",
              "      <td>Sublayer x</td>\n",
              "      <td>is</td>\n",
              "      <td>function implemented</td>\n",
              "      <td>That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414</th>\n",
              "      <td>Sublayer x</td>\n",
              "      <td>is</td>\n",
              "      <td>function</td>\n",
              "      <td>That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>415</th>\n",
              "      <td>Sublayer x</td>\n",
              "      <td>is</td>\n",
              "      <td>where function implemented by sub layer</td>\n",
              "      <td>That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416</th>\n",
              "      <td>decoder</td>\n",
              "      <td>is composed of</td>\n",
              "      <td>stack of N 6 layers</td>\n",
              "      <td>Decoder : The decoder is also composed of a stack of N 6 identical layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417</th>\n",
              "      <td>decoder</td>\n",
              "      <td>is also composed of</td>\n",
              "      <td>stack of N 6 identical layers</td>\n",
              "      <td>Decoder : The decoder is also composed of a stack of N 6 identical layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418</th>\n",
              "      <td>decoder</td>\n",
              "      <td>is</td>\n",
              "      <td>also composed</td>\n",
              "      <td>Decoder : The decoder is also composed of a stack of N 6 identical layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419</th>\n",
              "      <td>decoder</td>\n",
              "      <td>is composed of</td>\n",
              "      <td>stack</td>\n",
              "      <td>Decoder : The decoder is also composed of a stack of N 6 identical layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420</th>\n",
              "      <td>Decoder</td>\n",
              "      <td>composed of</td>\n",
              "      <td>stack</td>\n",
              "      <td>Decoder : The decoder is also composed of a stack of N 6 identical layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>421</th>\n",
              "      <td>decoder</td>\n",
              "      <td>is also composed of</td>\n",
              "      <td>stack</td>\n",
              "      <td>Decoder : The decoder is also composed of a stack of N 6 identical layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>422</th>\n",
              "      <td>decoder</td>\n",
              "      <td>is composed of</td>\n",
              "      <td>stack of N 6 identical layers</td>\n",
              "      <td>Decoder : The decoder is also composed of a stack of N 6 identical layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>423</th>\n",
              "      <td>decoder</td>\n",
              "      <td>is</td>\n",
              "      <td>composed</td>\n",
              "      <td>Decoder : The decoder is also composed of a stack of N 6 identical layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>decoder</td>\n",
              "      <td>is also composed of</td>\n",
              "      <td>stack of N 6 layers</td>\n",
              "      <td>Decoder : The decoder is also composed of a stack of N 6 identical layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>two sub layers</td>\n",
              "      <td>is in</td>\n",
              "      <td>encoder layer</td>\n",
              "      <td>In addition to the two sub layers in each encoder layer , the decoder inserts a third sub layer , which performs multi head attention over the output of the encoder stack .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>we</td>\n",
              "      <td>Similar employ</td>\n",
              "      <td>residual connections around each of sub layers</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>427</th>\n",
              "      <td>we</td>\n",
              "      <td>employ</td>\n",
              "      <td>connections</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>428</th>\n",
              "      <td>we</td>\n",
              "      <td>Similar employ</td>\n",
              "      <td>connections around each of sub layers</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>429</th>\n",
              "      <td>we</td>\n",
              "      <td>employ</td>\n",
              "      <td>residual connections around each of sub layers</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>430</th>\n",
              "      <td>we</td>\n",
              "      <td>employ</td>\n",
              "      <td>residual connections around each sub layers</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>431</th>\n",
              "      <td>we</td>\n",
              "      <td>employ</td>\n",
              "      <td>connections around each sub layers</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>432</th>\n",
              "      <td>we</td>\n",
              "      <td>followed by</td>\n",
              "      <td>layer normalization</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>we</td>\n",
              "      <td>Similar employ</td>\n",
              "      <td>connections around each sub layers</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>we</td>\n",
              "      <td>Similar employ</td>\n",
              "      <td>residual connections around each sub layers</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>we</td>\n",
              "      <td>Similar employ</td>\n",
              "      <td>residual connections</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>we</td>\n",
              "      <td>Similar employ</td>\n",
              "      <td>connections</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>we</td>\n",
              "      <td>employ</td>\n",
              "      <td>residual connections</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>438</th>\n",
              "      <td>we</td>\n",
              "      <td>employ</td>\n",
              "      <td>connections around each of sub layers</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>self attention sub layer</td>\n",
              "      <td>prevent positions</td>\n",
              "      <td>attending to positions</td>\n",
              "      <td>We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>440</th>\n",
              "      <td>self attention sub layer</td>\n",
              "      <td>prevent positions</td>\n",
              "      <td>attending</td>\n",
              "      <td>We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>441</th>\n",
              "      <td>self attention sub layer</td>\n",
              "      <td>prevent</td>\n",
              "      <td>positions</td>\n",
              "      <td>We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>442</th>\n",
              "      <td>self attention sub layer</td>\n",
              "      <td>is in</td>\n",
              "      <td>decoder</td>\n",
              "      <td>We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>443</th>\n",
              "      <td>self attention sub layer</td>\n",
              "      <td>prevent positions</td>\n",
              "      <td>attending to subsequent positions</td>\n",
              "      <td>We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>444</th>\n",
              "      <td>predictions</td>\n",
              "      <td>depend on</td>\n",
              "      <td>outputs at positions less than i. 3.2 Attention</td>\n",
              "      <td>This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>predictions</td>\n",
              "      <td>depend on</td>\n",
              "      <td>outputs</td>\n",
              "      <td>This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>446</th>\n",
              "      <td>output embeddings</td>\n",
              "      <td>are offset by</td>\n",
              "      <td>position</td>\n",
              "      <td>This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447</th>\n",
              "      <td>predictions</td>\n",
              "      <td>depend on</td>\n",
              "      <td>outputs at positions</td>\n",
              "      <td>This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>output embeddings</td>\n",
              "      <td>are offset by</td>\n",
              "      <td>one position</td>\n",
              "      <td>This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>masking</td>\n",
              "      <td>combined with</td>\n",
              "      <td>fact</td>\n",
              "      <td>This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>predictions</td>\n",
              "      <td>depend on</td>\n",
              "      <td>outputs at positions less</td>\n",
              "      <td>This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>output</td>\n",
              "      <td>is computed as</td>\n",
              "      <td>weighted sum of values</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>weight</td>\n",
              "      <td>is</td>\n",
              "      <td>where computed by compatibility function</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>output</td>\n",
              "      <td>is computed as</td>\n",
              "      <td>sum of values</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454</th>\n",
              "      <td>weight</td>\n",
              "      <td>is</td>\n",
              "      <td>where computed by compatibility function of query</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>weight</td>\n",
              "      <td>is computed by</td>\n",
              "      <td>compatibility function</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456</th>\n",
              "      <td>output</td>\n",
              "      <td>is</td>\n",
              "      <td>computed</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>457</th>\n",
              "      <td>weight</td>\n",
              "      <td>is computed by</td>\n",
              "      <td>compatibility function of query with corresponding key</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>458</th>\n",
              "      <td>weight</td>\n",
              "      <td>assigned to</td>\n",
              "      <td>value</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>459</th>\n",
              "      <td>weight</td>\n",
              "      <td>is</td>\n",
              "      <td>where computed by compatibility function of query with key</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460</th>\n",
              "      <td>weight</td>\n",
              "      <td>is computed by</td>\n",
              "      <td>compatibility function of query with key</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>461</th>\n",
              "      <td>weight</td>\n",
              "      <td>is</td>\n",
              "      <td>where computed</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>462</th>\n",
              "      <td>weight</td>\n",
              "      <td>is</td>\n",
              "      <td>where computed by compatibility function with corresponding key</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>463</th>\n",
              "      <td>weight</td>\n",
              "      <td>is</td>\n",
              "      <td>where computed by compatibility function of query with corresponding key</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>output</td>\n",
              "      <td>is computed as</td>\n",
              "      <td>weighted sum</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>465</th>\n",
              "      <td>weight</td>\n",
              "      <td>is computed by</td>\n",
              "      <td>compatibility function with corresponding key</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>466</th>\n",
              "      <td>weight</td>\n",
              "      <td>is computed by</td>\n",
              "      <td>compatibility function of query</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>467</th>\n",
              "      <td>compatibility function</td>\n",
              "      <td>is with</td>\n",
              "      <td>corresponding key</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>468</th>\n",
              "      <td>weight</td>\n",
              "      <td>is computed by</td>\n",
              "      <td>compatibility function with key</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>469</th>\n",
              "      <td>output</td>\n",
              "      <td>is computed as</td>\n",
              "      <td>sum</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>470</th>\n",
              "      <td>weight</td>\n",
              "      <td>is</td>\n",
              "      <td>computed</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>471</th>\n",
              "      <td>weight</td>\n",
              "      <td>is</td>\n",
              "      <td>where computed by compatibility function with key</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>472</th>\n",
              "      <td>our particular attention</td>\n",
              "      <td>Scaled</td>\n",
              "      <td>Dot Product Attention Figure 2</td>\n",
              "      <td>3.2.1 Scaled Dot Product Attention We call our particular attention Scaled Dot Product Attention Figure 2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>473</th>\n",
              "      <td>our attention</td>\n",
              "      <td>Scaled</td>\n",
              "      <td>Dot Product Attention Figure 2</td>\n",
              "      <td>3.2.1 Scaled Dot Product Attention We call our particular attention Scaled Dot Product Attention Figure 2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>474</th>\n",
              "      <td>input</td>\n",
              "      <td>consists of</td>\n",
              "      <td>queries of dimension dk</td>\n",
              "      <td>The input consists of queries and keys of dimension dk , and values of dimension dv .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>input</td>\n",
              "      <td>values of</td>\n",
              "      <td>dimension dv</td>\n",
              "      <td>The input consists of queries and keys of dimension dk , and values of dimension dv .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>input</td>\n",
              "      <td>consists of</td>\n",
              "      <td>queries</td>\n",
              "      <td>The input consists of queries and keys of dimension dk , and values of dimension dv .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>Multi Head Attention</td>\n",
              "      <td>consists of</td>\n",
              "      <td>several attention layers</td>\n",
              "      <td>right Multi Head Attention consists of several attention layers running in parallel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>478</th>\n",
              "      <td>Multi Head Attention</td>\n",
              "      <td>consists of</td>\n",
              "      <td>several attention layers running</td>\n",
              "      <td>right Multi Head Attention consists of several attention layers running in parallel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>479</th>\n",
              "      <td>Multi Head Attention</td>\n",
              "      <td>consists of</td>\n",
              "      <td>attention layers running in parallel</td>\n",
              "      <td>right Multi Head Attention consists of several attention layers running in parallel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>480</th>\n",
              "      <td>Multi Head Attention</td>\n",
              "      <td>consists of</td>\n",
              "      <td>attention layers running</td>\n",
              "      <td>right Multi Head Attention consists of several attention layers running in parallel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>481</th>\n",
              "      <td>Multi Head Attention</td>\n",
              "      <td>consists of</td>\n",
              "      <td>several attention layers running in parallel</td>\n",
              "      <td>right Multi Head Attention consists of several attention layers running in parallel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>482</th>\n",
              "      <td>Multi Head Attention</td>\n",
              "      <td>consists of</td>\n",
              "      <td>attention layers</td>\n",
              "      <td>right Multi Head Attention consists of several attention layers running in parallel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>483</th>\n",
              "      <td>query</td>\n",
              "      <td>is with</td>\n",
              "      <td>keys</td>\n",
              "      <td>query with all keys , divide each by dk , and apply a softmax function to obtain the weights on the values .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>484</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set packed together into matrix Q</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>485</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set packed</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>486</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set of queries packed into matrix Q</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>487</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set of queries simultaneously packed together</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>488</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set of queries packed together into matrix Q</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>489</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set simultaneously packed together into matrix Q</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>490</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>491</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set simultaneously packed into matrix Q</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>492</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set of queries simultaneously packed</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>493</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set packed together</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>494</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set of queries simultaneously</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set of queries packed together</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set of queries simultaneously packed into matrix Q</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set of queries packed</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set simultaneously packed</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function In</td>\n",
              "      <td>practice</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>500</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set simultaneously packed together</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>we</td>\n",
              "      <td>compute</td>\n",
              "      <td>attention function</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set packed into matrix Q</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set of queries simultaneously packed together into matrix Q</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set simultaneously</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function on</td>\n",
              "      <td>set of queries</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>506</th>\n",
              "      <td>keys</td>\n",
              "      <td>are</td>\n",
              "      <td>also packed together</td>\n",
              "      <td>The keys and values are also packed together into matrices K and V .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>507</th>\n",
              "      <td>keys</td>\n",
              "      <td>are</td>\n",
              "      <td>packed</td>\n",
              "      <td>The keys and values are also packed together into matrices K and V .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>508</th>\n",
              "      <td>keys</td>\n",
              "      <td>are</td>\n",
              "      <td>packed together into matrices K</td>\n",
              "      <td>The keys and values are also packed together into matrices K and V .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>509</th>\n",
              "      <td>keys</td>\n",
              "      <td>are</td>\n",
              "      <td>also packed into matrices K</td>\n",
              "      <td>The keys and values are also packed together into matrices K and V .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>510</th>\n",
              "      <td>keys</td>\n",
              "      <td>are</td>\n",
              "      <td>packed together</td>\n",
              "      <td>The keys and values are also packed together into matrices K and V .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>511</th>\n",
              "      <td>keys</td>\n",
              "      <td>are</td>\n",
              "      <td>packed into matrices K</td>\n",
              "      <td>The keys and values are also packed together into matrices K and V .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>512</th>\n",
              "      <td>keys</td>\n",
              "      <td>are</td>\n",
              "      <td>also packed</td>\n",
              "      <td>The keys and values are also packed together into matrices K and V .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>513</th>\n",
              "      <td>keys</td>\n",
              "      <td>are</td>\n",
              "      <td>also packed together into matrices K</td>\n",
              "      <td>The keys and values are also packed together into matrices K and V .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>514</th>\n",
              "      <td>We</td>\n",
              "      <td>Attention</td>\n",
              "      <td>V softmax QKT dk V 1</td>\n",
              "      <td>We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>515</th>\n",
              "      <td>We</td>\n",
              "      <td>Attention</td>\n",
              "      <td>K</td>\n",
              "      <td>We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>We</td>\n",
              "      <td>Attention</td>\n",
              "      <td>dot product attention</td>\n",
              "      <td>We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517</th>\n",
              "      <td>We</td>\n",
              "      <td>Attention</td>\n",
              "      <td>dot product multiplicative attention</td>\n",
              "      <td>We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>518</th>\n",
              "      <td>We</td>\n",
              "      <td>compute</td>\n",
              "      <td>matrix</td>\n",
              "      <td>We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>519</th>\n",
              "      <td>We</td>\n",
              "      <td>Attention</td>\n",
              "      <td>Q</td>\n",
              "      <td>We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>520</th>\n",
              "      <td>We</td>\n",
              "      <td>compute</td>\n",
              "      <td>matrix of outputs</td>\n",
              "      <td>We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>521</th>\n",
              "      <td>compatibility function</td>\n",
              "      <td>using</td>\n",
              "      <td>feed</td>\n",
              "      <td>Additive attention computes the compatibility function using a feed forward network with a single hidden layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>522</th>\n",
              "      <td>Additive attention</td>\n",
              "      <td>computes</td>\n",
              "      <td>compatibility function</td>\n",
              "      <td>Additive attention computes the compatibility function using a feed forward network with a single hidden layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>523</th>\n",
              "      <td>attention</td>\n",
              "      <td>computes</td>\n",
              "      <td>compatibility function</td>\n",
              "      <td>Additive attention computes the compatibility function using a feed forward network with a single hidden layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>524</th>\n",
              "      <td>forward network</td>\n",
              "      <td>is with</td>\n",
              "      <td>single hidden layer</td>\n",
              "      <td>Additive attention computes the compatibility function using a feed forward network with a single hidden layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>it</td>\n",
              "      <td>can</td>\n",
              "      <td>can implemented</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>dot product attention</td>\n",
              "      <td>is much faster</td>\n",
              "      <td>similar</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>527</th>\n",
              "      <td>it</td>\n",
              "      <td>using</td>\n",
              "      <td>highly optimized matrix multiplication code</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>528</th>\n",
              "      <td>two</td>\n",
              "      <td>are similar in</td>\n",
              "      <td>theoretical complexity</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>529</th>\n",
              "      <td>dot product attention</td>\n",
              "      <td>is faster</td>\n",
              "      <td>similar</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>it</td>\n",
              "      <td>using</td>\n",
              "      <td>optimized matrix multiplication code</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>531</th>\n",
              "      <td>dot product attention</td>\n",
              "      <td>is faster</td>\n",
              "      <td>similar in theoretical complexity</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>532</th>\n",
              "      <td>it</td>\n",
              "      <td>using</td>\n",
              "      <td>matrix multiplication code</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>533</th>\n",
              "      <td>dot product attention</td>\n",
              "      <td>is</td>\n",
              "      <td>much faster</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>534</th>\n",
              "      <td>dot product attention</td>\n",
              "      <td>is</td>\n",
              "      <td>faster</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>535</th>\n",
              "      <td>two</td>\n",
              "      <td>are</td>\n",
              "      <td>similar</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>536</th>\n",
              "      <td>dot product attention</td>\n",
              "      <td>is faster</td>\n",
              "      <td>can implemented</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>537</th>\n",
              "      <td>dot product attention</td>\n",
              "      <td>is much faster</td>\n",
              "      <td>similar in theoretical complexity</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>538</th>\n",
              "      <td>dot product attention</td>\n",
              "      <td>is much faster</td>\n",
              "      <td>can implemented</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>two mechanisms</td>\n",
              "      <td>perform for</td>\n",
              "      <td>small values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>540</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>541</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>542</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>543</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>scaling for values of dk 3</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>544</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>545</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values of dk two mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values of dk two mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>547</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>548</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>scaling</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>549</th>\n",
              "      <td>mechanisms</td>\n",
              "      <td>perform similarly for</td>\n",
              "      <td>values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>scaling for larger values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551</th>\n",
              "      <td>two mechanisms</td>\n",
              "      <td>perform similarly for</td>\n",
              "      <td>small values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values two mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>553</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values two mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>554</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values of dk mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>556</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values of dk two mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>557</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values of dk two mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>two mechanisms</td>\n",
              "      <td>perform similarly for</td>\n",
              "      <td>small values of dk</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>559</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>scaling for</td>\n",
              "      <td>larger values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>560</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values two mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>561</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms</td>\n",
              "      <td>dot product attention</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>562</th>\n",
              "      <td>attention</td>\n",
              "      <td>scaling for</td>\n",
              "      <td>values of dk 3</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>563</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms</td>\n",
              "      <td>dot product attention</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values two mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>two mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values of dk mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>569</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values of dk mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>570</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values two mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>scaling for</td>\n",
              "      <td>values of dk 3</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>572</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values of dk mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>574</th>\n",
              "      <td>mechanisms</td>\n",
              "      <td>perform for</td>\n",
              "      <td>small values of dk</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>575</th>\n",
              "      <td>two mechanisms</td>\n",
              "      <td>perform for</td>\n",
              "      <td>values of dk</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>576</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values of dk two mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>577</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>578</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>two mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>579</th>\n",
              "      <td>attention</td>\n",
              "      <td>scaling for</td>\n",
              "      <td>larger values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>580</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>scaling for values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581</th>\n",
              "      <td>two mechanisms</td>\n",
              "      <td>perform for</td>\n",
              "      <td>values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>582</th>\n",
              "      <td>mechanisms</td>\n",
              "      <td>perform similarly for</td>\n",
              "      <td>small values of dk</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>583</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>scaling for larger values of dk 3</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>584</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>scaling for values of dk 3</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>585</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>two mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>586</th>\n",
              "      <td>mechanisms</td>\n",
              "      <td>perform for</td>\n",
              "      <td>values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>587</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>scaling for larger values of dk 3</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>588</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values two mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>589</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>scaling for values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>590</th>\n",
              "      <td>mechanisms</td>\n",
              "      <td>perform similarly for</td>\n",
              "      <td>values of dk</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values of dk mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>592</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values of dk two mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>593</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>scaling for larger values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>594</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>scaling for</td>\n",
              "      <td>larger values of dk 3</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>595</th>\n",
              "      <td>two mechanisms</td>\n",
              "      <td>perform similarly for</td>\n",
              "      <td>values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>596</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>597</th>\n",
              "      <td>mechanisms</td>\n",
              "      <td>perform similarly for</td>\n",
              "      <td>small values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>598</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>scaling</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599</th>\n",
              "      <td>two mechanisms</td>\n",
              "      <td>perform similarly for</td>\n",
              "      <td>values of dk</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>600</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values of dk two mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>601</th>\n",
              "      <td>two mechanisms</td>\n",
              "      <td>perform for</td>\n",
              "      <td>small values of dk</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>602</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>two mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>603</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values of dk mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>604</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values two mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>scaling for</td>\n",
              "      <td>values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>606</th>\n",
              "      <td>attention</td>\n",
              "      <td>scaling for</td>\n",
              "      <td>larger values of dk 3</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>607</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values of dk mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>608</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values of dk two mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>609</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values two mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>610</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>mechanisms perform</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>611</th>\n",
              "      <td>mechanisms</td>\n",
              "      <td>perform for</td>\n",
              "      <td>values of dk</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>612</th>\n",
              "      <td>attention</td>\n",
              "      <td>scaling for</td>\n",
              "      <td>values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>613</th>\n",
              "      <td>mechanisms</td>\n",
              "      <td>perform for</td>\n",
              "      <td>small values</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>614</th>\n",
              "      <td>attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for values of dk mechanisms perform similarly</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>615</th>\n",
              "      <td>dot products</td>\n",
              "      <td>grow for</td>\n",
              "      <td>values of dk</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>616</th>\n",
              "      <td>dot products</td>\n",
              "      <td>grow in</td>\n",
              "      <td>magnitude</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>617</th>\n",
              "      <td>dot products</td>\n",
              "      <td>pushing softmax function into</td>\n",
              "      <td>regions</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>618</th>\n",
              "      <td>dot products</td>\n",
              "      <td>grow for</td>\n",
              "      <td>large values</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>619</th>\n",
              "      <td>it</td>\n",
              "      <td>has</td>\n",
              "      <td>gradients 4</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>620</th>\n",
              "      <td>dot products</td>\n",
              "      <td>grow for</td>\n",
              "      <td>values</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>621</th>\n",
              "      <td>dot products</td>\n",
              "      <td>pushing</td>\n",
              "      <td>softmax function</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>622</th>\n",
              "      <td>it</td>\n",
              "      <td>has</td>\n",
              "      <td>extremely small gradients 4</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>623</th>\n",
              "      <td>it</td>\n",
              "      <td>has</td>\n",
              "      <td>small gradients 4</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>624</th>\n",
              "      <td>dot products</td>\n",
              "      <td>grow</td>\n",
              "      <td>large</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>dot products</td>\n",
              "      <td>grow for</td>\n",
              "      <td>large values of dk</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>626</th>\n",
              "      <td>we</td>\n",
              "      <td>scale dot products by</td>\n",
              "      <td>1 dk</td>\n",
              "      <td>To counteract this effect , we scale the dot products by 1 dk .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>627</th>\n",
              "      <td>we</td>\n",
              "      <td>scale</td>\n",
              "      <td>dot products</td>\n",
              "      <td>To counteract this effect , we scale the dot products by 1 dk .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>628</th>\n",
              "      <td>we</td>\n",
              "      <td>counteract</td>\n",
              "      <td>effect</td>\n",
              "      <td>To counteract this effect , we scale the dot products by 1 dk .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>629</th>\n",
              "      <td>queries</td>\n",
              "      <td>learned respectively</td>\n",
              "      <td>linear projections</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630</th>\n",
              "      <td>queries</td>\n",
              "      <td>learned</td>\n",
              "      <td>projections</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>631</th>\n",
              "      <td>it</td>\n",
              "      <td>linearly project</td>\n",
              "      <td>queries</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>632</th>\n",
              "      <td>it</td>\n",
              "      <td>project values h times with</td>\n",
              "      <td>different</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>633</th>\n",
              "      <td>it</td>\n",
              "      <td>linearly project</td>\n",
              "      <td>values h times</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>634</th>\n",
              "      <td>it</td>\n",
              "      <td>project</td>\n",
              "      <td>queries</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>635</th>\n",
              "      <td>it</td>\n",
              "      <td>linearly project values h times with</td>\n",
              "      <td>different</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>636</th>\n",
              "      <td>it</td>\n",
              "      <td>learned respectively</td>\n",
              "      <td>projections</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>637</th>\n",
              "      <td>it</td>\n",
              "      <td>learned respectively</td>\n",
              "      <td>linear projections</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>638</th>\n",
              "      <td>it</td>\n",
              "      <td>learned projections respectively to</td>\n",
              "      <td>dk</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>639</th>\n",
              "      <td>it</td>\n",
              "      <td>project queries with</td>\n",
              "      <td>different</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>640</th>\n",
              "      <td>it</td>\n",
              "      <td>learned</td>\n",
              "      <td>projections</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>641</th>\n",
              "      <td>it</td>\n",
              "      <td>project keys with</td>\n",
              "      <td>different</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>642</th>\n",
              "      <td>it</td>\n",
              "      <td>learned projections to</td>\n",
              "      <td>dk</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>643</th>\n",
              "      <td>it</td>\n",
              "      <td>project</td>\n",
              "      <td>keys</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>644</th>\n",
              "      <td>it</td>\n",
              "      <td>learned</td>\n",
              "      <td>linear projections</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>645</th>\n",
              "      <td>it</td>\n",
              "      <td>project</td>\n",
              "      <td>values h times</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>646</th>\n",
              "      <td>it</td>\n",
              "      <td>linearly project</td>\n",
              "      <td>keys</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>647</th>\n",
              "      <td>queries</td>\n",
              "      <td>learned</td>\n",
              "      <td>linear projections</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>648</th>\n",
              "      <td>queries</td>\n",
              "      <td>learned projections to</td>\n",
              "      <td>dk</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>649</th>\n",
              "      <td>it</td>\n",
              "      <td>linearly project keys with</td>\n",
              "      <td>different</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>650</th>\n",
              "      <td>it</td>\n",
              "      <td>linearly project queries with</td>\n",
              "      <td>different</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>651</th>\n",
              "      <td>queries</td>\n",
              "      <td>learned projections respectively to</td>\n",
              "      <td>dk</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>652</th>\n",
              "      <td>queries</td>\n",
              "      <td>learned respectively</td>\n",
              "      <td>projections</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>653</th>\n",
              "      <td>we</td>\n",
              "      <td>yielding</td>\n",
              "      <td>dv dimensional output values</td>\n",
              "      <td>On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>654</th>\n",
              "      <td>we</td>\n",
              "      <td>yielding</td>\n",
              "      <td>dv output values</td>\n",
              "      <td>On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>we</td>\n",
              "      <td>perform</td>\n",
              "      <td>attention function</td>\n",
              "      <td>On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>we</td>\n",
              "      <td>perform attention function in</td>\n",
              "      <td>parallel</td>\n",
              "      <td>On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>657</th>\n",
              "      <td>These</td>\n",
              "      <td>depicted in</td>\n",
              "      <td>Figure 2</td>\n",
              "      <td>These are concatenated and once again projected , resulting in the final values , as depicted in Figure 2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>model</td>\n",
              "      <td>jointly attend to</td>\n",
              "      <td>information at different positions</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>model</td>\n",
              "      <td>jointly attend to</td>\n",
              "      <td>information from different representation subspaces at positions</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>660</th>\n",
              "      <td>Multi head attention</td>\n",
              "      <td>allows</td>\n",
              "      <td>model</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>661</th>\n",
              "      <td>model</td>\n",
              "      <td>attend to</td>\n",
              "      <td>information from representation subspaces at positions</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>662</th>\n",
              "      <td>model</td>\n",
              "      <td>attend to</td>\n",
              "      <td>information at positions</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>663</th>\n",
              "      <td>head attention</td>\n",
              "      <td>allows</td>\n",
              "      <td>model</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>model</td>\n",
              "      <td>jointly attend to</td>\n",
              "      <td>information from representation subspaces at positions</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>665</th>\n",
              "      <td>model</td>\n",
              "      <td>jointly attend to</td>\n",
              "      <td>information from representation subspaces</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>666</th>\n",
              "      <td>model</td>\n",
              "      <td>jointly attend to</td>\n",
              "      <td>information from different representation subspaces</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>667</th>\n",
              "      <td>model</td>\n",
              "      <td>jointly attend to</td>\n",
              "      <td>information from different representation subspaces at different positions</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>668</th>\n",
              "      <td>model</td>\n",
              "      <td>attend to</td>\n",
              "      <td>information from different representation subspaces at positions</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>669</th>\n",
              "      <td>model</td>\n",
              "      <td>attend to</td>\n",
              "      <td>information from different representation subspaces at different positions</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>670</th>\n",
              "      <td>model</td>\n",
              "      <td>attend to</td>\n",
              "      <td>information from representation subspaces</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>671</th>\n",
              "      <td>model</td>\n",
              "      <td>attend to</td>\n",
              "      <td>information from representation subspaces at different positions</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>model</td>\n",
              "      <td>attend to</td>\n",
              "      <td>information</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>673</th>\n",
              "      <td>model</td>\n",
              "      <td>jointly attend to</td>\n",
              "      <td>information from representation subspaces at different positions</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>674</th>\n",
              "      <td>model</td>\n",
              "      <td>attend to</td>\n",
              "      <td>information at different positions</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>675</th>\n",
              "      <td>model</td>\n",
              "      <td>jointly attend to</td>\n",
              "      <td>information at positions</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>676</th>\n",
              "      <td>model</td>\n",
              "      <td>jointly attend to</td>\n",
              "      <td>information</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>677</th>\n",
              "      <td>model</td>\n",
              "      <td>attend to</td>\n",
              "      <td>information from different representation subspaces</td>\n",
              "      <td>Multi head attention allows the model to jointly attend to information from different representation subspaces at different positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>678</th>\n",
              "      <td>averaging</td>\n",
              "      <td>inhibits With</td>\n",
              "      <td>single attention head</td>\n",
              "      <td>With a single attention head , averaging inhibits this .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>679</th>\n",
              "      <td>averaging</td>\n",
              "      <td>inhibits With</td>\n",
              "      <td>attention head</td>\n",
              "      <td>With a single attention head , averaging inhibits this .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>680</th>\n",
              "      <td>q</td>\n",
              "      <td>components of are</td>\n",
              "      <td>independent random variables</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>681</th>\n",
              "      <td>components</td>\n",
              "      <td>are</td>\n",
              "      <td>variables</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>682</th>\n",
              "      <td>dot products</td>\n",
              "      <td>get</td>\n",
              "      <td>large</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>683</th>\n",
              "      <td>components</td>\n",
              "      <td>are</td>\n",
              "      <td>independent random variables</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>684</th>\n",
              "      <td>q</td>\n",
              "      <td>components of are</td>\n",
              "      <td>independent variables</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>685</th>\n",
              "      <td>independent random variables</td>\n",
              "      <td>is with</td>\n",
              "      <td>mean 0</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>686</th>\n",
              "      <td>4To</td>\n",
              "      <td>illustrate</td>\n",
              "      <td>assume</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>687</th>\n",
              "      <td>q</td>\n",
              "      <td>components of are</td>\n",
              "      <td>variables</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>688</th>\n",
              "      <td>q</td>\n",
              "      <td>components of are</td>\n",
              "      <td>random variables</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>689</th>\n",
              "      <td>components</td>\n",
              "      <td>are</td>\n",
              "      <td>independent variables</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>690</th>\n",
              "      <td>components</td>\n",
              "      <td>are</td>\n",
              "      <td>random variables</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>691</th>\n",
              "      <td>their</td>\n",
              "      <td>product</td>\n",
              "      <td>q k dk i 1 qiki</td>\n",
              "      <td>Then their dot product , q k dk i 1 qiki , has mean 0 and variance dk .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>692</th>\n",
              "      <td>we</td>\n",
              "      <td>employ heads In</td>\n",
              "      <td>work</td>\n",
              "      <td>In this work we employ h 8 parallel attention layers , or heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>693</th>\n",
              "      <td>we</td>\n",
              "      <td>employ</td>\n",
              "      <td>h 8 parallel attention layers</td>\n",
              "      <td>In this work we employ h 8 parallel attention layers , or heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>694</th>\n",
              "      <td>we</td>\n",
              "      <td>employ</td>\n",
              "      <td>h 8 attention layers</td>\n",
              "      <td>In this work we employ h 8 parallel attention layers , or heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>we</td>\n",
              "      <td>employ</td>\n",
              "      <td>heads</td>\n",
              "      <td>In this work we employ h 8 parallel attention layers , or heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>696</th>\n",
              "      <td>we</td>\n",
              "      <td>use</td>\n",
              "      <td>dk dv dmodel h 64</td>\n",
              "      <td>For each of these we use dk dv dmodel h 64 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>697</th>\n",
              "      <td>cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>698</th>\n",
              "      <td>total cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>699</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>700</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>701</th>\n",
              "      <td>cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>702</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>703</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>704</th>\n",
              "      <td>cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>705</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>706</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>707</th>\n",
              "      <td>total cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>708</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>709</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>710</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>711</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712</th>\n",
              "      <td>cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>713</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>714</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>715</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>716</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>717</th>\n",
              "      <td>total cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>718</th>\n",
              "      <td>cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>719</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>720</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>721</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>722</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>723</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>724</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>725</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>726</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>727</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>728</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>729</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>730</th>\n",
              "      <td>cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>731</th>\n",
              "      <td>cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>732</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>733</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>734</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>735</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>737</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>738</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>739</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>740</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>741</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>742</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>743</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>745</th>\n",
              "      <td>single head attention</td>\n",
              "      <td>is with</td>\n",
              "      <td>full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>748</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>749</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>750</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>751</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>752</th>\n",
              "      <td>total cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>753</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>754</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>755</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>756</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>757</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>758</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>760</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>761</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>762</th>\n",
              "      <td>total cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>763</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>764</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>765</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>766</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>767</th>\n",
              "      <td>cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>768</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>769</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>770</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>771</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>772</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>773</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>774</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>775</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>776</th>\n",
              "      <td>cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>777</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>778</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>779</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>780</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>783</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>784</th>\n",
              "      <td>total cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>786</th>\n",
              "      <td>cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>787</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>788</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>789</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>790</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>791</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>792</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>793</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>794</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>795</th>\n",
              "      <td>cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>796</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>797</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>798</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>799</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>800</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>801</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>802</th>\n",
              "      <td>total cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>803</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>804</th>\n",
              "      <td>cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>805</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>806</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>807</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>808</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>809</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>810</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>811</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>812</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>813</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>814</th>\n",
              "      <td>cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of single head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>815</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>816</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>817</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of head attention with full dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>818</th>\n",
              "      <td>cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>819</th>\n",
              "      <td>computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>820</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>821</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>822</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to dimension of head similar to that of head attention</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>823</th>\n",
              "      <td>cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due similar</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>824</th>\n",
              "      <td>total cost</td>\n",
              "      <td>is similar to</td>\n",
              "      <td>that of single head attention with dimensionality</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>825</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>Due is similar to</td>\n",
              "      <td>that</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>826</th>\n",
              "      <td>encoder</td>\n",
              "      <td>contains</td>\n",
              "      <td>self attention layers</td>\n",
              "      <td>The encoder contains self attention layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>827</th>\n",
              "      <td>output</td>\n",
              "      <td>is in</td>\n",
              "      <td>encoder</td>\n",
              "      <td>In a self attention layer all of the keys , values and queries come from the same place , in this case , the output of the previous layer in the encoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>828</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>Similarly allow</td>\n",
              "      <td>position in decoder</td>\n",
              "      <td>Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>829</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>is in</td>\n",
              "      <td>decoder</td>\n",
              "      <td>Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>830</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>allow</td>\n",
              "      <td>position in decoder</td>\n",
              "      <td>Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>831</th>\n",
              "      <td>We</td>\n",
              "      <td>prevent</td>\n",
              "      <td>leftward information flow</td>\n",
              "      <td>We need to prevent leftward information flow in the decoder to preserve the auto regressive property .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>832</th>\n",
              "      <td>We</td>\n",
              "      <td>prevent</td>\n",
              "      <td>leftward information flow in decoder</td>\n",
              "      <td>We need to prevent leftward information flow in the decoder to preserve the auto regressive property .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>833</th>\n",
              "      <td>leftward information flow</td>\n",
              "      <td>is in</td>\n",
              "      <td>decoder</td>\n",
              "      <td>We need to prevent leftward information flow in the decoder to preserve the auto regressive property .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>834</th>\n",
              "      <td>We</td>\n",
              "      <td>prevent</td>\n",
              "      <td>information flow in decoder</td>\n",
              "      <td>We need to prevent leftward information flow in the decoder to preserve the auto regressive property .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>835</th>\n",
              "      <td>We</td>\n",
              "      <td>prevent</td>\n",
              "      <td>information flow</td>\n",
              "      <td>We need to prevent leftward information flow in the decoder to preserve the auto regressive property .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>836</th>\n",
              "      <td>We</td>\n",
              "      <td>implement</td>\n",
              "      <td>inside of dot product attention</td>\n",
              "      <td>We implement this inside of scaled dot product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>837</th>\n",
              "      <td>We</td>\n",
              "      <td>implement</td>\n",
              "      <td>inside of scaled dot product attention</td>\n",
              "      <td>We implement this inside of scaled dot product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>838</th>\n",
              "      <td>We</td>\n",
              "      <td>implement</td>\n",
              "      <td>inside</td>\n",
              "      <td>We implement this inside of scaled dot product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>839</th>\n",
              "      <td>two linear transformations</td>\n",
              "      <td>is with</td>\n",
              "      <td>ReLU activation in</td>\n",
              "      <td>This consists of two linear transformations with a ReLU activation in between .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>840</th>\n",
              "      <td>linear transformations</td>\n",
              "      <td>are same across</td>\n",
              "      <td>different positions</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>841</th>\n",
              "      <td>transformations</td>\n",
              "      <td>are same across</td>\n",
              "      <td>positions</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>842</th>\n",
              "      <td>they</td>\n",
              "      <td>FFN use</td>\n",
              "      <td>different parameters</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>843</th>\n",
              "      <td>they</td>\n",
              "      <td>use</td>\n",
              "      <td>different parameters</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>844</th>\n",
              "      <td>linear transformations</td>\n",
              "      <td>are same across</td>\n",
              "      <td>positions</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>845</th>\n",
              "      <td>they</td>\n",
              "      <td>FFN use parameters from</td>\n",
              "      <td>layer</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>846</th>\n",
              "      <td>they</td>\n",
              "      <td>use</td>\n",
              "      <td>parameters</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>847</th>\n",
              "      <td>linear transformations</td>\n",
              "      <td>are</td>\n",
              "      <td>same</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>848</th>\n",
              "      <td>they</td>\n",
              "      <td>FFN use parameters from</td>\n",
              "      <td>layer to layer</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>849</th>\n",
              "      <td>they</td>\n",
              "      <td>use parameters from</td>\n",
              "      <td>layer</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>850</th>\n",
              "      <td>transformations</td>\n",
              "      <td>are</td>\n",
              "      <td>same</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>851</th>\n",
              "      <td>they</td>\n",
              "      <td>use parameters from</td>\n",
              "      <td>layer to layer</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>852</th>\n",
              "      <td>transformations</td>\n",
              "      <td>are same across</td>\n",
              "      <td>different positions</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>853</th>\n",
              "      <td>they</td>\n",
              "      <td>FFN use</td>\n",
              "      <td>parameters</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>854</th>\n",
              "      <td>two convolutions</td>\n",
              "      <td>is with</td>\n",
              "      <td>kernel size 1</td>\n",
              "      <td>Another way of describing this is as two convolutions with kernel size 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>855</th>\n",
              "      <td>layer</td>\n",
              "      <td>has</td>\n",
              "      <td>dimensionality dff 2048</td>\n",
              "      <td>The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>856</th>\n",
              "      <td>inner layer</td>\n",
              "      <td>has</td>\n",
              "      <td>dimensionality dff 2048</td>\n",
              "      <td>The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>857</th>\n",
              "      <td>dimensionality</td>\n",
              "      <td>is</td>\n",
              "      <td>dmodel 512</td>\n",
              "      <td>The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>858</th>\n",
              "      <td>we</td>\n",
              "      <td>convert input tokens to</td>\n",
              "      <td>vectors of dimension dmodel</td>\n",
              "      <td>3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>859</th>\n",
              "      <td>we</td>\n",
              "      <td>convert output tokens to</td>\n",
              "      <td>vectors</td>\n",
              "      <td>3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>860</th>\n",
              "      <td>we</td>\n",
              "      <td>convert output tokens to</td>\n",
              "      <td>vectors of dimension dmodel</td>\n",
              "      <td>3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>861</th>\n",
              "      <td>we</td>\n",
              "      <td>convert</td>\n",
              "      <td>output tokens</td>\n",
              "      <td>3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>862</th>\n",
              "      <td>we</td>\n",
              "      <td>use</td>\n",
              "      <td>learned embeddings</td>\n",
              "      <td>3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>863</th>\n",
              "      <td>we</td>\n",
              "      <td>convert</td>\n",
              "      <td>input tokens</td>\n",
              "      <td>3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>864</th>\n",
              "      <td>we</td>\n",
              "      <td>use</td>\n",
              "      <td>embeddings</td>\n",
              "      <td>3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>865</th>\n",
              "      <td>we</td>\n",
              "      <td>convert input tokens to</td>\n",
              "      <td>vectors</td>\n",
              "      <td>3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>866</th>\n",
              "      <td>We</td>\n",
              "      <td>use</td>\n",
              "      <td>the usual</td>\n",
              "      <td>We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>867</th>\n",
              "      <td>We</td>\n",
              "      <td>also use</td>\n",
              "      <td>the usual</td>\n",
              "      <td>We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>868</th>\n",
              "      <td>We</td>\n",
              "      <td>use</td>\n",
              "      <td>the</td>\n",
              "      <td>We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>869</th>\n",
              "      <td>We</td>\n",
              "      <td>also use</td>\n",
              "      <td>the</td>\n",
              "      <td>We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>870</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>same weight matrix between two embedding layers similar</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>871</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>weight matrix between two embedding layers similar</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>872</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>weight matrix similar</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>873</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>same weight matrix between two embedding layers similar to 24</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>874</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>weight matrix similar to 24</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>875</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>same weight matrix similar</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>876</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>same weight matrix</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>877</th>\n",
              "      <td>we</td>\n",
              "      <td>share weight matrix In</td>\n",
              "      <td>our model</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>878</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>same weight matrix between two embedding layers</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>879</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>same weight matrix similar to 24</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>880</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>weight matrix</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>881</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>weight matrix between two embedding layers</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>882</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>weight matrix between two embedding layers similar to 24</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>883</th>\n",
              "      <td>we</td>\n",
              "      <td>multiply weights In</td>\n",
              "      <td>embedding layers</td>\n",
              "      <td>In the embedding layers , we multiply those weights by dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>884</th>\n",
              "      <td>we</td>\n",
              "      <td>multiply</td>\n",
              "      <td>weights</td>\n",
              "      <td>In the embedding layers , we multiply those weights by dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>885</th>\n",
              "      <td>we</td>\n",
              "      <td>multiply weights by</td>\n",
              "      <td>dmodel</td>\n",
              "      <td>In the embedding layers , we multiply those weights by dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>886</th>\n",
              "      <td>Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about position in sequence</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>887</th>\n",
              "      <td>3.5 Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about relative position in sequence</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>model</td>\n",
              "      <td>make</td>\n",
              "      <td>use</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>3.5 Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about relative position of tokens</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>3.5 Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about position in sequence</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>891</th>\n",
              "      <td>Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about relative position in sequence</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about relative position of tokens</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893</th>\n",
              "      <td>3.5 Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about relative position of tokens in sequence</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>894</th>\n",
              "      <td>model</td>\n",
              "      <td>use of</td>\n",
              "      <td>order</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>model</td>\n",
              "      <td>use of</td>\n",
              "      <td>order of sequence</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about relative position</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>3.5 Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>3.5 Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about position of tokens in sequence</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>900</th>\n",
              "      <td>Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about position of tokens in sequence</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>901</th>\n",
              "      <td>3.5 Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about relative position</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>902</th>\n",
              "      <td>model</td>\n",
              "      <td>make</td>\n",
              "      <td>use of order</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>903</th>\n",
              "      <td>model</td>\n",
              "      <td>make</td>\n",
              "      <td>use of order of sequence</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>904</th>\n",
              "      <td>relative position</td>\n",
              "      <td>is in</td>\n",
              "      <td>sequence</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>905</th>\n",
              "      <td>Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about position of tokens</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>906</th>\n",
              "      <td>Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about position</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>907</th>\n",
              "      <td>3.5 Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about position of tokens</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>908</th>\n",
              "      <td>Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about relative position of tokens in sequence</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>909</th>\n",
              "      <td>3.5 Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about position</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>910</th>\n",
              "      <td>we</td>\n",
              "      <td>add encodings to</td>\n",
              "      <td>input embeddings</td>\n",
              "      <td>To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>911</th>\n",
              "      <td>we</td>\n",
              "      <td>add encodings To</td>\n",
              "      <td>end</td>\n",
              "      <td>To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>912</th>\n",
              "      <td>we</td>\n",
              "      <td>add</td>\n",
              "      <td>encodings</td>\n",
              "      <td>To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>913</th>\n",
              "      <td>we</td>\n",
              "      <td>add encodings at</td>\n",
              "      <td>5 Table 1</td>\n",
              "      <td>To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>914</th>\n",
              "      <td>we</td>\n",
              "      <td>add</td>\n",
              "      <td>positional encodings</td>\n",
              "      <td>To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>915</th>\n",
              "      <td>k</td>\n",
              "      <td>size in</td>\n",
              "      <td>restricted self attention</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>916</th>\n",
              "      <td>k</td>\n",
              "      <td>size in</td>\n",
              "      <td>self attention</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>917</th>\n",
              "      <td>size</td>\n",
              "      <td>is in</td>\n",
              "      <td>restricted self attention</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>918</th>\n",
              "      <td>k</td>\n",
              "      <td>is</td>\n",
              "      <td>kernel size of convolutions</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>919</th>\n",
              "      <td>k</td>\n",
              "      <td>size of</td>\n",
              "      <td>neighborhood</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>920</th>\n",
              "      <td>k</td>\n",
              "      <td>is</td>\n",
              "      <td>kernel size</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>921</th>\n",
              "      <td>d</td>\n",
              "      <td>is</td>\n",
              "      <td>representation dimension</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>922</th>\n",
              "      <td>n</td>\n",
              "      <td>is</td>\n",
              "      <td>sequence length</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>923</th>\n",
              "      <td>encodings</td>\n",
              "      <td>have</td>\n",
              "      <td>dimension dmodel</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>924</th>\n",
              "      <td>encodings</td>\n",
              "      <td>have</td>\n",
              "      <td>same dimension dmodel as embeddings</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>925</th>\n",
              "      <td>encodings</td>\n",
              "      <td>have</td>\n",
              "      <td>same dimension dmodel</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>926</th>\n",
              "      <td>positional encodings</td>\n",
              "      <td>have</td>\n",
              "      <td>same dimension dmodel</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>927</th>\n",
              "      <td>two</td>\n",
              "      <td>can</td>\n",
              "      <td>can summed</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>928</th>\n",
              "      <td>encodings</td>\n",
              "      <td>have dimension dmodel</td>\n",
              "      <td>can summed</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>929</th>\n",
              "      <td>positional encodings</td>\n",
              "      <td>have</td>\n",
              "      <td>same dimension dmodel as embeddings</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>930</th>\n",
              "      <td>positional encodings</td>\n",
              "      <td>have</td>\n",
              "      <td>dimension dmodel</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>931</th>\n",
              "      <td>positional encodings</td>\n",
              "      <td>have</td>\n",
              "      <td>dimension dmodel as embeddings</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>932</th>\n",
              "      <td>encodings</td>\n",
              "      <td>have</td>\n",
              "      <td>dimension dmodel as embeddings</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>933</th>\n",
              "      <td>positional encodings</td>\n",
              "      <td>have dimension dmodel</td>\n",
              "      <td>can summed</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>934</th>\n",
              "      <td>i</td>\n",
              "      <td>is</td>\n",
              "      <td>dimension</td>\n",
              "      <td>In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>935</th>\n",
              "      <td>we</td>\n",
              "      <td>use</td>\n",
              "      <td>sine functions of frequencies</td>\n",
              "      <td>In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>936</th>\n",
              "      <td>we</td>\n",
              "      <td>use</td>\n",
              "      <td>sine functions of different frequencies</td>\n",
              "      <td>In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>937</th>\n",
              "      <td>pos</td>\n",
              "      <td>is</td>\n",
              "      <td>position</td>\n",
              "      <td>In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>938</th>\n",
              "      <td>pos</td>\n",
              "      <td>is</td>\n",
              "      <td>where position</td>\n",
              "      <td>In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>939</th>\n",
              "      <td>we</td>\n",
              "      <td>use sine functions In</td>\n",
              "      <td>work</td>\n",
              "      <td>In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>940</th>\n",
              "      <td>we</td>\n",
              "      <td>use</td>\n",
              "      <td>sine functions</td>\n",
              "      <td>In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>941</th>\n",
              "      <td>wavelengths</td>\n",
              "      <td>form</td>\n",
              "      <td>progression</td>\n",
              "      <td>The wavelengths form a geometric progression from 2 to 10000 2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>942</th>\n",
              "      <td>wavelengths</td>\n",
              "      <td>form</td>\n",
              "      <td>geometric progression</td>\n",
              "      <td>The wavelengths form a geometric progression from 2 to 10000 2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>943</th>\n",
              "      <td>PEpos k</td>\n",
              "      <td>can</td>\n",
              "      <td>since for fixed offset k can represented as function of PEpos</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>944</th>\n",
              "      <td>it</td>\n",
              "      <td>allow</td>\n",
              "      <td>learn</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>945</th>\n",
              "      <td>model</td>\n",
              "      <td>easily learn</td>\n",
              "      <td>attend</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>946</th>\n",
              "      <td>We</td>\n",
              "      <td>chose</td>\n",
              "      <td>function</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>947</th>\n",
              "      <td>it</td>\n",
              "      <td>allow</td>\n",
              "      <td>easily learn</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>948</th>\n",
              "      <td>PEpos k</td>\n",
              "      <td>can</td>\n",
              "      <td>since for fixed offset k can represented as function</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>949</th>\n",
              "      <td>model</td>\n",
              "      <td>learn</td>\n",
              "      <td>attend</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>950</th>\n",
              "      <td>it</td>\n",
              "      <td>allow</td>\n",
              "      <td>model</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>951</th>\n",
              "      <td>PEpos k</td>\n",
              "      <td>can</td>\n",
              "      <td>since for fixed offset k can represented as linear function of PEpos</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>952</th>\n",
              "      <td>PEpos k</td>\n",
              "      <td>can</td>\n",
              "      <td>since for fixed offset k can represented as linear function</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>953</th>\n",
              "      <td>model</td>\n",
              "      <td>learn</td>\n",
              "      <td>attend by relative positions</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>954</th>\n",
              "      <td>model</td>\n",
              "      <td>learn</td>\n",
              "      <td>attend by positions</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>955</th>\n",
              "      <td>model</td>\n",
              "      <td>easily learn</td>\n",
              "      <td>attend by positions</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>956</th>\n",
              "      <td>PEpos k</td>\n",
              "      <td>can</td>\n",
              "      <td>since for fixed offset k can represented</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>957</th>\n",
              "      <td>We</td>\n",
              "      <td>chose function</td>\n",
              "      <td>we hypothesized</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>958</th>\n",
              "      <td>model</td>\n",
              "      <td>easily learn</td>\n",
              "      <td>attend by relative positions</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>959</th>\n",
              "      <td>results</td>\n",
              "      <td>see</td>\n",
              "      <td>Table 3 row E</td>\n",
              "      <td>We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>960</th>\n",
              "      <td>We</td>\n",
              "      <td>also experimented</td>\n",
              "      <td>using</td>\n",
              "      <td>We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>961</th>\n",
              "      <td>We</td>\n",
              "      <td>experimented</td>\n",
              "      <td>using</td>\n",
              "      <td>We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>962</th>\n",
              "      <td>identical results</td>\n",
              "      <td>see</td>\n",
              "      <td>Table 3 row E</td>\n",
              "      <td>We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>963</th>\n",
              "      <td>model</td>\n",
              "      <td>extrapolate to</td>\n",
              "      <td>sequence lengths longer than ones</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>964</th>\n",
              "      <td>We</td>\n",
              "      <td>chose</td>\n",
              "      <td>sinusoidal version</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>965</th>\n",
              "      <td>model</td>\n",
              "      <td>extrapolate to</td>\n",
              "      <td>sequence lengths longer than ones encountered</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>966</th>\n",
              "      <td>it</td>\n",
              "      <td>may allow</td>\n",
              "      <td>extrapolate to sequence lengths longer</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>967</th>\n",
              "      <td>it</td>\n",
              "      <td>may allow</td>\n",
              "      <td>extrapolate to sequence lengths</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>968</th>\n",
              "      <td>it</td>\n",
              "      <td>may allow</td>\n",
              "      <td>extrapolate</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>969</th>\n",
              "      <td>it</td>\n",
              "      <td>may allow</td>\n",
              "      <td>model</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>970</th>\n",
              "      <td>it</td>\n",
              "      <td>may allow</td>\n",
              "      <td>extrapolate to sequence lengths longer than ones</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>971</th>\n",
              "      <td>We</td>\n",
              "      <td>chose</td>\n",
              "      <td>version</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>972</th>\n",
              "      <td>model</td>\n",
              "      <td>extrapolate to</td>\n",
              "      <td>sequence lengths longer</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>We</td>\n",
              "      <td>chose version</td>\n",
              "      <td>it may allow</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>974</th>\n",
              "      <td>it</td>\n",
              "      <td>may allow</td>\n",
              "      <td>extrapolate to sequence lengths longer than ones encountered during training</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>975</th>\n",
              "      <td>model</td>\n",
              "      <td>extrapolate to</td>\n",
              "      <td>sequence lengths</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>976</th>\n",
              "      <td>model</td>\n",
              "      <td>extrapolate to</td>\n",
              "      <td>sequence lengths longer than ones encountered during training</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>977</th>\n",
              "      <td>it</td>\n",
              "      <td>may allow</td>\n",
              "      <td>extrapolate to sequence lengths longer than ones encountered</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>978</th>\n",
              "      <td>we</td>\n",
              "      <td>compare</td>\n",
              "      <td>aspects of self attention layers</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>979</th>\n",
              "      <td>we</td>\n",
              "      <td>compare aspects to</td>\n",
              "      <td>recurrent layers used</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>980</th>\n",
              "      <td>4 Self Attention</td>\n",
              "      <td>zn with</td>\n",
              "      <td>xi</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>981</th>\n",
              "      <td>we</td>\n",
              "      <td>compare aspects to</td>\n",
              "      <td>layers</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982</th>\n",
              "      <td>we</td>\n",
              "      <td>compare aspects to</td>\n",
              "      <td>layers used</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983</th>\n",
              "      <td>we</td>\n",
              "      <td>compare</td>\n",
              "      <td>various aspects of self attention layers</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984</th>\n",
              "      <td>section</td>\n",
              "      <td>In Attention is</td>\n",
              "      <td>zn with xi such hidden layer</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>985</th>\n",
              "      <td>we</td>\n",
              "      <td>compare aspects to</td>\n",
              "      <td>recurrent layers commonly used</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>986</th>\n",
              "      <td>hidden layer</td>\n",
              "      <td>is in</td>\n",
              "      <td>typical sequence transduction encoder</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987</th>\n",
              "      <td>section</td>\n",
              "      <td>In Attention is</td>\n",
              "      <td>xn to sequence of equal length z1</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>we</td>\n",
              "      <td>compare</td>\n",
              "      <td>aspects</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>4 Self Attention</td>\n",
              "      <td>zn such as</td>\n",
              "      <td>hidden layer in typical sequence transduction encoder</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>we</td>\n",
              "      <td>compare</td>\n",
              "      <td>various aspects</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991</th>\n",
              "      <td>hidden layer</td>\n",
              "      <td>such as zn is</td>\n",
              "      <td>zi Rd</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992</th>\n",
              "      <td>4 Self Attention</td>\n",
              "      <td>is In</td>\n",
              "      <td>section</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>we</td>\n",
              "      <td>various aspects of</td>\n",
              "      <td>self attention layers</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>zn</td>\n",
              "      <td>is with</td>\n",
              "      <td>xi</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>we</td>\n",
              "      <td>compare aspects to</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>4 Self Attention</td>\n",
              "      <td>xn to</td>\n",
              "      <td>sequence of equal length z1</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>we</td>\n",
              "      <td>aspects of</td>\n",
              "      <td>self attention layers</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>we</td>\n",
              "      <td>compare aspects to</td>\n",
              "      <td>layers commonly used</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>we</td>\n",
              "      <td>Motivating</td>\n",
              "      <td>our use of self attention</td>\n",
              "      <td>Motivating our use of self attention we consider three desiderata .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>we</td>\n",
              "      <td>Motivating</td>\n",
              "      <td>our use</td>\n",
              "      <td>Motivating our use of self attention we consider three desiderata .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1001</th>\n",
              "      <td>we</td>\n",
              "      <td>consider</td>\n",
              "      <td>three desiderata</td>\n",
              "      <td>Motivating our use of self attention we consider three desiderata .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1002</th>\n",
              "      <td>One</td>\n",
              "      <td>is</td>\n",
              "      <td>computational complexity</td>\n",
              "      <td>One is the total computational complexity per layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1003</th>\n",
              "      <td>One</td>\n",
              "      <td>is total computational complexity per</td>\n",
              "      <td>layer</td>\n",
              "      <td>One is the total computational complexity per layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1004</th>\n",
              "      <td>One</td>\n",
              "      <td>is total complexity per</td>\n",
              "      <td>layer</td>\n",
              "      <td>One is the total computational complexity per layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1005</th>\n",
              "      <td>One</td>\n",
              "      <td>is complexity per</td>\n",
              "      <td>layer</td>\n",
              "      <td>One is the total computational complexity per layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1006</th>\n",
              "      <td>One</td>\n",
              "      <td>is</td>\n",
              "      <td>total complexity</td>\n",
              "      <td>One is the total computational complexity per layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1007</th>\n",
              "      <td>One</td>\n",
              "      <td>is</td>\n",
              "      <td>total computational complexity</td>\n",
              "      <td>One is the total computational complexity per layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1008</th>\n",
              "      <td>One</td>\n",
              "      <td>is computational complexity per</td>\n",
              "      <td>layer</td>\n",
              "      <td>One is the total computational complexity per layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1009</th>\n",
              "      <td>One</td>\n",
              "      <td>is</td>\n",
              "      <td>complexity</td>\n",
              "      <td>One is the total computational complexity per layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1010</th>\n",
              "      <td>Another</td>\n",
              "      <td>is</td>\n",
              "      <td>amount</td>\n",
              "      <td>Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1011</th>\n",
              "      <td>Another</td>\n",
              "      <td>is amount of</td>\n",
              "      <td>computation</td>\n",
              "      <td>Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1012</th>\n",
              "      <td>Another</td>\n",
              "      <td>is amount</td>\n",
              "      <td>can parallelized</td>\n",
              "      <td>Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1013</th>\n",
              "      <td>third</td>\n",
              "      <td>is</td>\n",
              "      <td>path length</td>\n",
              "      <td>The third is the path length between long range dependencies in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1014</th>\n",
              "      <td>long range dependencies</td>\n",
              "      <td>is in</td>\n",
              "      <td>network</td>\n",
              "      <td>The third is the path length between long range dependencies in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1015</th>\n",
              "      <td>third</td>\n",
              "      <td>is</td>\n",
              "      <td>path length between range dependencies in network</td>\n",
              "      <td>The third is the path length between long range dependencies in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1016</th>\n",
              "      <td>third</td>\n",
              "      <td>is</td>\n",
              "      <td>path length between long range dependencies</td>\n",
              "      <td>The third is the path length between long range dependencies in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1017</th>\n",
              "      <td>third</td>\n",
              "      <td>is</td>\n",
              "      <td>path length between range dependencies</td>\n",
              "      <td>The third is the path length between long range dependencies in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1018</th>\n",
              "      <td>third</td>\n",
              "      <td>is</td>\n",
              "      <td>path length between long range dependencies in network</td>\n",
              "      <td>The third is the path length between long range dependencies in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1019</th>\n",
              "      <td>dependencies</td>\n",
              "      <td>is key challenge in</td>\n",
              "      <td>sequence transduction tasks</td>\n",
              "      <td>Learning long range dependencies is a key challenge in many sequence transduction tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1020</th>\n",
              "      <td>key challenge</td>\n",
              "      <td>is in</td>\n",
              "      <td>many sequence transduction tasks</td>\n",
              "      <td>Learning long range dependencies is a key challenge in many sequence transduction tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1021</th>\n",
              "      <td>dependencies</td>\n",
              "      <td>is key challenge in</td>\n",
              "      <td>many sequence transduction tasks</td>\n",
              "      <td>Learning long range dependencies is a key challenge in many sequence transduction tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1022</th>\n",
              "      <td>dependencies</td>\n",
              "      <td>is challenge in</td>\n",
              "      <td>many sequence transduction tasks</td>\n",
              "      <td>Learning long range dependencies is a key challenge in many sequence transduction tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1023</th>\n",
              "      <td>dependencies</td>\n",
              "      <td>is</td>\n",
              "      <td>key</td>\n",
              "      <td>Learning long range dependencies is a key challenge in many sequence transduction tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1024</th>\n",
              "      <td>dependencies</td>\n",
              "      <td>is challenge in</td>\n",
              "      <td>sequence transduction tasks</td>\n",
              "      <td>Learning long range dependencies is a key challenge in many sequence transduction tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1025</th>\n",
              "      <td>signals</td>\n",
              "      <td>traverse in</td>\n",
              "      <td>network</td>\n",
              "      <td>One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1026</th>\n",
              "      <td>signals</td>\n",
              "      <td>have</td>\n",
              "      <td>traverse</td>\n",
              "      <td>One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1027</th>\n",
              "      <td>forward signals</td>\n",
              "      <td>traverse in</td>\n",
              "      <td>network</td>\n",
              "      <td>One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td>forward signals</td>\n",
              "      <td>have</td>\n",
              "      <td>traverse in network</td>\n",
              "      <td>One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>signals</td>\n",
              "      <td>have</td>\n",
              "      <td>traverse in network</td>\n",
              "      <td>One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1030</th>\n",
              "      <td>forward signals</td>\n",
              "      <td>have</td>\n",
              "      <td>traverse</td>\n",
              "      <td>One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1031</th>\n",
              "      <td>it</td>\n",
              "      <td>learn</td>\n",
              "      <td>long range dependencies 11</td>\n",
              "      <td>The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long range dependencies 11 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1032</th>\n",
              "      <td>it</td>\n",
              "      <td>learn</td>\n",
              "      <td>range dependencies 11</td>\n",
              "      <td>The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long range dependencies 11 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1033</th>\n",
              "      <td>self attention layer</td>\n",
              "      <td>connects positions with</td>\n",
              "      <td>constant number of sequentially executed operations</td>\n",
              "      <td>As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1034</th>\n",
              "      <td>self attention layer</td>\n",
              "      <td>connects positions with</td>\n",
              "      <td>number of executed operations</td>\n",
              "      <td>As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1035</th>\n",
              "      <td>self attention layer</td>\n",
              "      <td>connects positions with</td>\n",
              "      <td>constant number</td>\n",
              "      <td>As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1036</th>\n",
              "      <td>self attention layer</td>\n",
              "      <td>connects positions with</td>\n",
              "      <td>number of operations</td>\n",
              "      <td>As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1037</th>\n",
              "      <td>self attention layer</td>\n",
              "      <td>connects positions with</td>\n",
              "      <td>number</td>\n",
              "      <td>As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1038</th>\n",
              "      <td>self attention layer</td>\n",
              "      <td>connects</td>\n",
              "      <td>positions</td>\n",
              "      <td>As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1039</th>\n",
              "      <td>self attention layer</td>\n",
              "      <td>connects positions with</td>\n",
              "      <td>constant number of operations</td>\n",
              "      <td>As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1040</th>\n",
              "      <td>self attention layer</td>\n",
              "      <td>connects positions with</td>\n",
              "      <td>constant number of executed operations</td>\n",
              "      <td>As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1041</th>\n",
              "      <td>self attention layer</td>\n",
              "      <td>connects positions with</td>\n",
              "      <td>number of sequentially executed operations</td>\n",
              "      <td>As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1042</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are faster than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1043</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are</td>\n",
              "      <td>In terms faster than layers</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1044</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are</td>\n",
              "      <td>In terms of computational complexity faster than layers</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1045</th>\n",
              "      <td>case</td>\n",
              "      <td>is with</td>\n",
              "      <td>sentence representations used by state of art models in machine translations such word piece 31</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1046</th>\n",
              "      <td>sequence length n</td>\n",
              "      <td>is</td>\n",
              "      <td>when smaller</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1047</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are faster In</td>\n",
              "      <td>terms of complexity</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1048</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are</td>\n",
              "      <td>In terms faster than recurrent layers</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1049</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are</td>\n",
              "      <td>In terms of computational complexity faster than recurrent layers</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1050</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are</td>\n",
              "      <td>faster</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1051</th>\n",
              "      <td>sequence length n</td>\n",
              "      <td>is smaller than</td>\n",
              "      <td>representation dimensionality d</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1052</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are faster In</td>\n",
              "      <td>terms</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1053</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are faster In</td>\n",
              "      <td>terms of computational complexity</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1054</th>\n",
              "      <td>sequence length n</td>\n",
              "      <td>is</td>\n",
              "      <td>smaller</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1055</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are</td>\n",
              "      <td>In terms of complexity faster than layers</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1056</th>\n",
              "      <td>sequence length n</td>\n",
              "      <td>is</td>\n",
              "      <td>when smaller than representation dimensionality d</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1057</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are faster than</td>\n",
              "      <td>layers</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1058</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are</td>\n",
              "      <td>In terms of complexity faster than recurrent layers</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1059</th>\n",
              "      <td>self attention</td>\n",
              "      <td>improve</td>\n",
              "      <td>performance for tasks</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1060</th>\n",
              "      <td>self attention</td>\n",
              "      <td>considering</td>\n",
              "      <td>neighborhood</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1061</th>\n",
              "      <td>self attention</td>\n",
              "      <td>considering</td>\n",
              "      <td>neighborhood of size r</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1062</th>\n",
              "      <td>self attention</td>\n",
              "      <td>considering neighborhood in</td>\n",
              "      <td>6 input sequence centered around respective output position</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1063</th>\n",
              "      <td>self attention</td>\n",
              "      <td>considering</td>\n",
              "      <td>only neighborhood</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1064</th>\n",
              "      <td>self attention</td>\n",
              "      <td>considering neighborhood in</td>\n",
              "      <td>6 input sequence</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1065</th>\n",
              "      <td>self attention</td>\n",
              "      <td>could</td>\n",
              "      <td>could restricted</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1066</th>\n",
              "      <td>self attention</td>\n",
              "      <td>improve</td>\n",
              "      <td>computational performance for tasks</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1067</th>\n",
              "      <td>self attention</td>\n",
              "      <td>improve</td>\n",
              "      <td>performance</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1068</th>\n",
              "      <td>self attention</td>\n",
              "      <td>considering</td>\n",
              "      <td>only neighborhood of size r</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1069</th>\n",
              "      <td>self attention</td>\n",
              "      <td>improve</td>\n",
              "      <td>computational performance</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1070</th>\n",
              "      <td>self attention</td>\n",
              "      <td>considering neighborhood in</td>\n",
              "      <td>6 input sequence centered</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1071</th>\n",
              "      <td>self attention</td>\n",
              "      <td>considering neighborhood in</td>\n",
              "      <td>6 input sequence centered around output position</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1072</th>\n",
              "      <td>We</td>\n",
              "      <td>investigate</td>\n",
              "      <td>approach</td>\n",
              "      <td>We plan to investigate this approach further in future work .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1073</th>\n",
              "      <td>We</td>\n",
              "      <td>investigate approach further in</td>\n",
              "      <td>future work</td>\n",
              "      <td>We plan to investigate this approach further in future work .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1074</th>\n",
              "      <td>We</td>\n",
              "      <td>investigate further</td>\n",
              "      <td>approach</td>\n",
              "      <td>We plan to investigate this approach further in future work .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1075</th>\n",
              "      <td>We</td>\n",
              "      <td>investigate approach in</td>\n",
              "      <td>future work</td>\n",
              "      <td>We plan to investigate this approach further in future work .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1076</th>\n",
              "      <td>single convolutional layer</td>\n",
              "      <td>is with</td>\n",
              "      <td>kernel width k n</td>\n",
              "      <td>A single convolutional layer with kernel width k n does not connect all pairs of input and output positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1077</th>\n",
              "      <td>stack</td>\n",
              "      <td>increasing</td>\n",
              "      <td>length</td>\n",
              "      <td>Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1078</th>\n",
              "      <td>increasing</td>\n",
              "      <td>length of</td>\n",
              "      <td>longest paths between two positions in network</td>\n",
              "      <td>Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1079</th>\n",
              "      <td>stack</td>\n",
              "      <td>increasing</td>\n",
              "      <td>length of longest paths between two positions in network</td>\n",
              "      <td>Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1080</th>\n",
              "      <td>case</td>\n",
              "      <td>is in</td>\n",
              "      <td>case of dilated convolutions 15</td>\n",
              "      <td>Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1081</th>\n",
              "      <td>Doing</td>\n",
              "      <td>requires</td>\n",
              "      <td>stack of O n k convolutional layers</td>\n",
              "      <td>Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1082</th>\n",
              "      <td>increasing</td>\n",
              "      <td>length of</td>\n",
              "      <td>paths between two positions in network</td>\n",
              "      <td>Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1083</th>\n",
              "      <td>Doing</td>\n",
              "      <td>requires</td>\n",
              "      <td>stack of O n k layers</td>\n",
              "      <td>Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1084</th>\n",
              "      <td>stack</td>\n",
              "      <td>increasing</td>\n",
              "      <td>length of paths between two positions in network</td>\n",
              "      <td>Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1085</th>\n",
              "      <td>Doing</td>\n",
              "      <td>requires</td>\n",
              "      <td>stack</td>\n",
              "      <td>Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1086</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than recurrent layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1087</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally expensive however than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1088</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than recurrent layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1089</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally expensive however by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1090</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally more expensive than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1091</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1092</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than recurrent layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1093</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than recurrent layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1094</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally more expensive however by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1095</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1096</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally more expensive however by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1097</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1098</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1099</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are more expensive however by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1100</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally expensive by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1101</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1102</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally expensive however by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1103</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1104</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than recurrent layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1105</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1106</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are expensive however by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1107</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than recurrent layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1108</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally expensive however by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1109</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1110</th>\n",
              "      <td>layers</td>\n",
              "      <td>are expensive than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1111</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than recurrent layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1112</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than recurrent layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1113</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1114</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than recurrent layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1115</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are expensive however than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1116</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally more expensive by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1117</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally expensive however than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1118</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1119</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than recurrent layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1120</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1121</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than recurrent layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1122</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1123</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally expensive than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1124</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than recurrent layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1125</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally expensive than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1126</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1127</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1128</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally more expensive than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1129</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1130</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally expensive than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1131</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are expensive however by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1132</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than recurrent layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1133</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1134</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally expensive however by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1135</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally more expensive by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1136</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than recurrent layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1137</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than recurrent layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1138</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally more expensive than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1139</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1140</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than recurrent layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1141</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are expensive however than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1142</th>\n",
              "      <td>layers</td>\n",
              "      <td>are expensive by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1143</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1144</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally expensive than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1145</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are expensive by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1146</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1147</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than recurrent layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1148</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1149</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1150</th>\n",
              "      <td>layers</td>\n",
              "      <td>are more expensive however by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1151</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1152</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1153</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are more expensive however by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1154</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1155</th>\n",
              "      <td>layers</td>\n",
              "      <td>are more expensive than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1156</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally expensive however than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1157</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally more expensive however by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1158</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than recurrent layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1159</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally more expensive than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1160</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1161</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than recurrent layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1162</th>\n",
              "      <td>layers</td>\n",
              "      <td>are expensive however by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1163</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than recurrent layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1164</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than recurrent layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1165</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1166</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally more expensive however than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1167</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1168</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are expensive by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1169</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1170</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally more expensive however by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1171</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally more expensive however by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1172</th>\n",
              "      <td>layers</td>\n",
              "      <td>are expensive than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1173</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are more expensive however than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1174</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are more expensive by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1175</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1176</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1177</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1178</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1179</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally expensive by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1180</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than recurrent layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1181</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are expensive than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1182</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1183</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are expensive than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1184</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than recurrent layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1185</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are expensive however by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1186</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than recurrent layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1187</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1188</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than recurrent layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1189</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are more expensive than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1190</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1191</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1192</th>\n",
              "      <td>layers</td>\n",
              "      <td>are more expensive than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1193</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally expensive by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1194</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1195</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than recurrent layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1196</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally more expensive by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1197</th>\n",
              "      <td>layers</td>\n",
              "      <td>are expensive by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1198</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than recurrent layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1199</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1200</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally expensive by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1201</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally more expensive however than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1202</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than recurrent layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1203</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally more expensive however by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1204</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1205</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than recurrent layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1206</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally expensive by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1207</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1208</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1209</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than recurrent layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1210</th>\n",
              "      <td>layers</td>\n",
              "      <td>are expensive however by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1211</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1212</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1213</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are more expensive than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1214</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1215</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1216</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally expensive however than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1217</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally expensive however by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1218</th>\n",
              "      <td>layers</td>\n",
              "      <td>are more expensive by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1219</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1220</th>\n",
              "      <td>layers</td>\n",
              "      <td>are expensive however than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1221</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally expensive by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1222</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1223</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally more expensive by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1224</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally more expensive however than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1225</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1226</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally expensive however by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1227</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1228</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than recurrent layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1229</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are more expensive by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1230</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than recurrent layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1231</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally more expensive by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1232</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1233</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than recurrent layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1234</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than recurrent layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1235</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1236</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are more expensive however by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1237</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1238</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1239</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than recurrent layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1240</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1241</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than recurrent layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1242</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than recurrent layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1243</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1244</th>\n",
              "      <td>layers</td>\n",
              "      <td>are expensive however by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1245</th>\n",
              "      <td>layers</td>\n",
              "      <td>are more expensive however than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1246</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than recurrent layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1247</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1248</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1249</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than recurrent layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1250</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are more expensive by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1251</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1252</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than recurrent layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1253</th>\n",
              "      <td>layers</td>\n",
              "      <td>are more expensive by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1254</th>\n",
              "      <td>layers</td>\n",
              "      <td>are more expensive however by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1255</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1256</th>\n",
              "      <td>layers</td>\n",
              "      <td>are expensive by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1257</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than recurrent layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1258</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are more expensive however than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1259</th>\n",
              "      <td>layers</td>\n",
              "      <td>are more expensive by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1260</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than recurrent layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1261</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1262</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are generally more expensive by</td>\n",
              "      <td>factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1263</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are expensive by</td>\n",
              "      <td>factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1264</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than recurrent layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1265</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than recurrent layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1266</th>\n",
              "      <td>layers</td>\n",
              "      <td>are more expensive however by</td>\n",
              "      <td>factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1267</th>\n",
              "      <td>layers</td>\n",
              "      <td>are more expensive however than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1268</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive than recurrent layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1269</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than layers by factor</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1270</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than recurrent layers by factor of k. convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1271</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than recurrent layers by factor of k. Separable convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1272</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally expensive than recurrent layers by factor however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1273</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>generally more expensive than layers by factor of k. convolutions 6</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1274</th>\n",
              "      <td>layers</td>\n",
              "      <td>are expensive however than</td>\n",
              "      <td>recurrent layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1275</th>\n",
              "      <td>layers</td>\n",
              "      <td>are generally more expensive however than</td>\n",
              "      <td>layers</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1276</th>\n",
              "      <td>Convolutional layers</td>\n",
              "      <td>are</td>\n",
              "      <td>more expensive however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1277</th>\n",
              "      <td>layers</td>\n",
              "      <td>are</td>\n",
              "      <td>expensive than layers by factor of k. Separable convolutions 6 however</td>\n",
              "      <td>Convolutional layers are generally more expensive than recurrent layers , by a factor of k. Separable convolutions 6 , however , decrease the complexity considerably , to O k n d n d2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1278</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is equal to</td>\n",
              "      <td>combination of self attention layer</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1279</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is</td>\n",
              "      <td>Even with k n however equal to combination</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1280</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is</td>\n",
              "      <td>equal</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1281</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is</td>\n",
              "      <td>Even with k n equal to combination of self attention layer</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1282</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is</td>\n",
              "      <td>with k n equal to combination</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1283</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is</td>\n",
              "      <td>with k n however equal to combination of self attention layer</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1284</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is</td>\n",
              "      <td>Even with k n however equal to combination of self attention layer</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1285</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is</td>\n",
              "      <td>with k n equal to combination of self attention layer</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1286</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is equal with</td>\n",
              "      <td>k n</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1287</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is equal with</td>\n",
              "      <td>Even k n</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1288</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is</td>\n",
              "      <td>with k n however equal to combination</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1289</th>\n",
              "      <td>complexity</td>\n",
              "      <td>however is equal with</td>\n",
              "      <td>Even k n</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1290</th>\n",
              "      <td>complexity</td>\n",
              "      <td>however is equal to</td>\n",
              "      <td>combination of self attention layer</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1291</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is</td>\n",
              "      <td>however equal</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1292</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is</td>\n",
              "      <td>Even with k n equal to combination</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1293</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is equal to</td>\n",
              "      <td>combination</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1294</th>\n",
              "      <td>complexity</td>\n",
              "      <td>however is equal with</td>\n",
              "      <td>k n</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1295</th>\n",
              "      <td>complexity</td>\n",
              "      <td>however is equal to</td>\n",
              "      <td>combination</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1296</th>\n",
              "      <td>self attention</td>\n",
              "      <td>could yield</td>\n",
              "      <td>more interpretable models</td>\n",
              "      <td>As side benefit , self attention could yield more interpretable models .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1297</th>\n",
              "      <td>self attention</td>\n",
              "      <td>could yield</td>\n",
              "      <td>interpretable models</td>\n",
              "      <td>As side benefit , self attention could yield more interpretable models .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1298</th>\n",
              "      <td>We</td>\n",
              "      <td>inspect</td>\n",
              "      <td>attention distributions</td>\n",
              "      <td>We inspect attention distributions from our models and present and discuss examples in the appendix .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1299</th>\n",
              "      <td>We</td>\n",
              "      <td>inspect attention distributions from</td>\n",
              "      <td>our models</td>\n",
              "      <td>We inspect attention distributions from our models and present and discuss examples in the appendix .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1300</th>\n",
              "      <td>examples</td>\n",
              "      <td>is in</td>\n",
              "      <td>appendix</td>\n",
              "      <td>We inspect attention distributions from our models and present and discuss examples in the appendix .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1301</th>\n",
              "      <td>We</td>\n",
              "      <td>discuss</td>\n",
              "      <td>examples</td>\n",
              "      <td>We inspect attention distributions from our models and present and discuss examples in the appendix .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1302</th>\n",
              "      <td>We</td>\n",
              "      <td>discuss</td>\n",
              "      <td>examples in appendix</td>\n",
              "      <td>We inspect attention distributions from our models and present and discuss examples in the appendix .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1303</th>\n",
              "      <td>many</td>\n",
              "      <td>exhibit</td>\n",
              "      <td>behavior</td>\n",
              "      <td>Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1304</th>\n",
              "      <td>behavior</td>\n",
              "      <td>related to</td>\n",
              "      <td>structure</td>\n",
              "      <td>Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1305</th>\n",
              "      <td>many</td>\n",
              "      <td>exhibit</td>\n",
              "      <td>behavior related to syntactic structure</td>\n",
              "      <td>Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1306</th>\n",
              "      <td>many</td>\n",
              "      <td>exhibit</td>\n",
              "      <td>behavior related to structure</td>\n",
              "      <td>Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1307</th>\n",
              "      <td>many</td>\n",
              "      <td>exhibit</td>\n",
              "      <td>behavior related</td>\n",
              "      <td>Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1308</th>\n",
              "      <td>behavior</td>\n",
              "      <td>related to</td>\n",
              "      <td>syntactic structure of sentences</td>\n",
              "      <td>Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1309</th>\n",
              "      <td>many</td>\n",
              "      <td>exhibit</td>\n",
              "      <td>behavior related to syntactic structure of sentences</td>\n",
              "      <td>Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1310</th>\n",
              "      <td>behavior</td>\n",
              "      <td>related to</td>\n",
              "      <td>structure of sentences</td>\n",
              "      <td>Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1311</th>\n",
              "      <td>behavior</td>\n",
              "      <td>related to</td>\n",
              "      <td>syntactic structure</td>\n",
              "      <td>Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1312</th>\n",
              "      <td>many</td>\n",
              "      <td>exhibit</td>\n",
              "      <td>behavior related to structure of sentences</td>\n",
              "      <td>Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1313</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 English dataset consisting of about 4.5 million sentence pairs</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1314</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>WMT 2014 English German dataset consisting of about 4.5 million sentence pairs</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1315</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 German dataset</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1316</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 English dataset consisting</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1317</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>WMT 2014 English dataset consisting</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1318</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 dataset consisting</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1319</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>WMT 2014 English dataset consisting of about 4.5 million sentence pairs</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1320</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 dataset consisting of about 4.5 million sentence pairs</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1321</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>WMT 2014 English German dataset consisting</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1322</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 German dataset consisting</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1323</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>WMT 2014 English dataset</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1324</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>WMT 2014 dataset consisting</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>WMT 2014 dataset consisting of about 4.5 million sentence pairs</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1326</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>WMT 2014 German dataset</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1327</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>WMT 2014 English German dataset</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1328</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>WMT 2014 dataset</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1329</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>WMT 2014 German dataset consisting of about 4.5 million sentence pairs</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1330</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>WMT 2014 German dataset consisting</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1331</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 English dataset</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1332</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1333</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 German dataset consisting of about 4.5 million sentence pairs</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1334</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 dataset</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1335</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 English German dataset consisting</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1336</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 English German dataset</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1337</th>\n",
              "      <td>Sentences</td>\n",
              "      <td>using</td>\n",
              "      <td>byte pair</td>\n",
              "      <td>Sentences were encoded using byte pair encoding 3 , which has a shared sourcetarget vocabulary of about 37000 tokens .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1338</th>\n",
              "      <td>Sentences</td>\n",
              "      <td>were</td>\n",
              "      <td>encoded</td>\n",
              "      <td>Sentences were encoded using byte pair encoding 3 , which has a shared sourcetarget vocabulary of about 37000 tokens .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1339</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>larger WMT 2014 English dataset consisting</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1340</th>\n",
              "      <td>we</td>\n",
              "      <td>used For</td>\n",
              "      <td>French</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1341</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>significantly larger WMT 2014 English dataset</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1342</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>WMT 2014 English dataset</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1343</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>larger WMT 2014 English French dataset consisting</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1344</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>significantly larger WMT 2014 English dataset consisting of 36M sentences</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1345</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>larger WMT 2014 English dataset consisting of 36M sentences</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1346</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>WMT 2014 English dataset consisting</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1347</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>larger WMT 2014 English dataset</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1348</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>significantly larger WMT 2014 English dataset consisting</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1349</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>larger WMT 2014 English French dataset</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1350</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>WMT 2014 English French dataset</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1351</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>significantly larger WMT 2014 English French dataset consisting</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1352</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>WMT 2014 English French dataset consisting</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1353</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>WMT 2014 English French dataset consisting of 36M sentences</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1354</th>\n",
              "      <td>we</td>\n",
              "      <td>used For</td>\n",
              "      <td>English French</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1355</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>larger WMT 2014 English French dataset consisting of 36M sentences</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1356</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>significantly larger WMT 2014 English French dataset consisting of 36M sentences</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1357</th>\n",
              "      <td>we</td>\n",
              "      <td>used into</td>\n",
              "      <td>32000 word piece vocabulary 31</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1358</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>WMT 2014 English dataset consisting of 36M sentences</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1359</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>significantly larger WMT 2014 English French dataset</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1360</th>\n",
              "      <td>Sentence pairs</td>\n",
              "      <td>were batched together by</td>\n",
              "      <td>approximate sequence length</td>\n",
              "      <td>Sentence pairs were batched together by approximate sequence length .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1361</th>\n",
              "      <td>Sentence pairs</td>\n",
              "      <td>were batched by</td>\n",
              "      <td>approximate sequence length</td>\n",
              "      <td>Sentence pairs were batched together by approximate sequence length .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1362</th>\n",
              "      <td>Sentence pairs</td>\n",
              "      <td>were batched by</td>\n",
              "      <td>sequence length</td>\n",
              "      <td>Sentence pairs were batched together by approximate sequence length .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1363</th>\n",
              "      <td>Sentence pairs</td>\n",
              "      <td>were</td>\n",
              "      <td>batched together</td>\n",
              "      <td>Sentence pairs were batched together by approximate sequence length .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1364</th>\n",
              "      <td>Sentence pairs</td>\n",
              "      <td>were batched together by</td>\n",
              "      <td>sequence length</td>\n",
              "      <td>Sentence pairs were batched together by approximate sequence length .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1365</th>\n",
              "      <td>Sentence pairs</td>\n",
              "      <td>were</td>\n",
              "      <td>batched</td>\n",
              "      <td>Sentence pairs were batched together by approximate sequence length .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1366</th>\n",
              "      <td>training batch</td>\n",
              "      <td>contained</td>\n",
              "      <td>set</td>\n",
              "      <td>Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1367</th>\n",
              "      <td>training batch</td>\n",
              "      <td>contained</td>\n",
              "      <td>set of sentence pairs</td>\n",
              "      <td>Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1368</th>\n",
              "      <td>We</td>\n",
              "      <td>trained</td>\n",
              "      <td>our models</td>\n",
              "      <td>5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1369</th>\n",
              "      <td>training step</td>\n",
              "      <td>took</td>\n",
              "      <td>about 0.4 seconds</td>\n",
              "      <td>For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1370</th>\n",
              "      <td>our base models</td>\n",
              "      <td>described throughout</td>\n",
              "      <td>paper</td>\n",
              "      <td>For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1371</th>\n",
              "      <td>our base models</td>\n",
              "      <td>using</td>\n",
              "      <td>hyperparameters</td>\n",
              "      <td>For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1372</th>\n",
              "      <td>We</td>\n",
              "      <td>trained</td>\n",
              "      <td>base models</td>\n",
              "      <td>We trained the base models for a total of 100,000 steps or 12 hours .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1373</th>\n",
              "      <td>step time</td>\n",
              "      <td>described on</td>\n",
              "      <td>line</td>\n",
              "      <td>For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1374</th>\n",
              "      <td>step time</td>\n",
              "      <td>was</td>\n",
              "      <td>1.0 seconds</td>\n",
              "      <td>For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1375</th>\n",
              "      <td>step time</td>\n",
              "      <td>described on</td>\n",
              "      <td>bottom line</td>\n",
              "      <td>For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1376</th>\n",
              "      <td>step time</td>\n",
              "      <td>described on</td>\n",
              "      <td>bottom line of table 3</td>\n",
              "      <td>For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1377</th>\n",
              "      <td>step time</td>\n",
              "      <td>was</td>\n",
              "      <td>For our models 1.0 seconds</td>\n",
              "      <td>For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1378</th>\n",
              "      <td>step time</td>\n",
              "      <td>described on</td>\n",
              "      <td>line of table 3</td>\n",
              "      <td>For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1379</th>\n",
              "      <td>step time</td>\n",
              "      <td>was</td>\n",
              "      <td>For our big models 1.0 seconds</td>\n",
              "      <td>For our big models , described on the bottom line of table 3 , step time was 1.0 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1380</th>\n",
              "      <td>big models</td>\n",
              "      <td>were</td>\n",
              "      <td>trained</td>\n",
              "      <td>The big models were trained for 300,000 steps 3.5 days .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1381</th>\n",
              "      <td>big models</td>\n",
              "      <td>were trained for</td>\n",
              "      <td>300,000 steps</td>\n",
              "      <td>The big models were trained for 300,000 steps 3.5 days .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1382</th>\n",
              "      <td>models</td>\n",
              "      <td>were trained for</td>\n",
              "      <td>300,000 steps 3.5 days</td>\n",
              "      <td>The big models were trained for 300,000 steps 3.5 days .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1383</th>\n",
              "      <td>models</td>\n",
              "      <td>were trained for</td>\n",
              "      <td>300,000 steps</td>\n",
              "      <td>The big models were trained for 300,000 steps 3.5 days .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1384</th>\n",
              "      <td>big models</td>\n",
              "      <td>were trained for</td>\n",
              "      <td>300,000 steps 3.5 days</td>\n",
              "      <td>The big models were trained for 300,000 steps 3.5 days .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1385</th>\n",
              "      <td>models</td>\n",
              "      <td>were</td>\n",
              "      <td>trained</td>\n",
              "      <td>The big models were trained for 300,000 steps 3.5 days .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1386</th>\n",
              "      <td>We</td>\n",
              "      <td>used</td>\n",
              "      <td>Adam optimizer 17</td>\n",
              "      <td>5.3 Optimizer We used the Adam optimizer 17 with 1 0.9 , 2 0.98 and 10 9 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1387</th>\n",
              "      <td>first warmup steps</td>\n",
              "      <td>training</td>\n",
              "      <td>steps</td>\n",
              "      <td>We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1388</th>\n",
              "      <td>warmup steps</td>\n",
              "      <td>training</td>\n",
              "      <td>steps</td>\n",
              "      <td>We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1389</th>\n",
              "      <td>We</td>\n",
              "      <td>varied</td>\n",
              "      <td>learning rate</td>\n",
              "      <td>We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1390</th>\n",
              "      <td>learning rate</td>\n",
              "      <td>decreasing thereafter proportionally</td>\n",
              "      <td>it</td>\n",
              "      <td>We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1391</th>\n",
              "      <td>learning rate</td>\n",
              "      <td>decreasing proportionally</td>\n",
              "      <td>it</td>\n",
              "      <td>We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1392</th>\n",
              "      <td>learning rate</td>\n",
              "      <td>decreasing</td>\n",
              "      <td>it</td>\n",
              "      <td>We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1393</th>\n",
              "      <td>We</td>\n",
              "      <td>used</td>\n",
              "      <td>warmup steps 4000</td>\n",
              "      <td>We used warmup steps 4000 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1394</th>\n",
              "      <td>We</td>\n",
              "      <td>employ</td>\n",
              "      <td>three types of regularization</td>\n",
              "      <td>5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1395</th>\n",
              "      <td>We</td>\n",
              "      <td>employ</td>\n",
              "      <td>three types</td>\n",
              "      <td>5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1396</th>\n",
              "      <td>We</td>\n",
              "      <td>apply</td>\n",
              "      <td>dropout 27</td>\n",
              "      <td>5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1397</th>\n",
              "      <td>sums</td>\n",
              "      <td>is in</td>\n",
              "      <td>encoder</td>\n",
              "      <td>In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1398</th>\n",
              "      <td>we</td>\n",
              "      <td>apply dropout In</td>\n",
              "      <td>addition</td>\n",
              "      <td>In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1399</th>\n",
              "      <td>we</td>\n",
              "      <td>apply</td>\n",
              "      <td>dropout</td>\n",
              "      <td>In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1400</th>\n",
              "      <td>we</td>\n",
              "      <td>apply dropout to</td>\n",
              "      <td>sums</td>\n",
              "      <td>In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1401</th>\n",
              "      <td>we</td>\n",
              "      <td>apply dropout to</td>\n",
              "      <td>sums of embeddings</td>\n",
              "      <td>In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1402</th>\n",
              "      <td>we</td>\n",
              "      <td>apply dropout to</td>\n",
              "      <td>sums of embeddings in encoder</td>\n",
              "      <td>In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1403</th>\n",
              "      <td>we</td>\n",
              "      <td>apply dropout to</td>\n",
              "      <td>sums in encoder</td>\n",
              "      <td>In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1404</th>\n",
              "      <td>we</td>\n",
              "      <td>use</td>\n",
              "      <td>rate of Pdrop 0.1</td>\n",
              "      <td>For the base model , we use a rate of Pdrop 0.1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1405</th>\n",
              "      <td>we</td>\n",
              "      <td>use</td>\n",
              "      <td>rate</td>\n",
              "      <td>For the base model , we use a rate of Pdrop 0.1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1406</th>\n",
              "      <td>we</td>\n",
              "      <td>use rate For</td>\n",
              "      <td>base model</td>\n",
              "      <td>For the base model , we use a rate of Pdrop 0.1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1407</th>\n",
              "      <td>we</td>\n",
              "      <td>employed</td>\n",
              "      <td>label smoothing of value ls 0.1 30</td>\n",
              "      <td>Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1408</th>\n",
              "      <td>Model BLEU</td>\n",
              "      <td>Training</td>\n",
              "      <td>Cost FLOPs EN</td>\n",
              "      <td>Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1409</th>\n",
              "      <td>Model BLEU</td>\n",
              "      <td>Training</td>\n",
              "      <td>Cost FLOPs</td>\n",
              "      <td>Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1410</th>\n",
              "      <td>we</td>\n",
              "      <td>employed</td>\n",
              "      <td>label smoothing</td>\n",
              "      <td>Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1411</th>\n",
              "      <td>model</td>\n",
              "      <td>learns</td>\n",
              "      <td>unsure</td>\n",
              "      <td>This hurts perplexity , as the model learns to be more unsure , but improves accuracy and BLEU score .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1412</th>\n",
              "      <td>model</td>\n",
              "      <td>learns</td>\n",
              "      <td>more unsure</td>\n",
              "      <td>This hurts perplexity , as the model learns to be more unsure , but improves accuracy and BLEU score .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1413</th>\n",
              "      <td>German translation task</td>\n",
              "      <td>to Translation is</td>\n",
              "      <td>big transformer model Transformer big in Table 2</td>\n",
              "      <td>6 Results 6.1 Machine Translation On the WMT 2014 English to German translation task , the big transformer model Transformer big in Table 2 outperforms the best previously reported models including ensembles by more than 2.0 BLEU , establishing a new state of the art BLEU score of 28.4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1414</th>\n",
              "      <td>WMT 2014 English</td>\n",
              "      <td>On Translation is</td>\n",
              "      <td>big transformer model Transformer big in Table 2</td>\n",
              "      <td>6 Results 6.1 Machine Translation On the WMT 2014 English to German translation task , the big transformer model Transformer big in Table 2 outperforms the best previously reported models including ensembles by more than 2.0 BLEU , establishing a new state of the art BLEU score of 28.4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1415</th>\n",
              "      <td>configuration</td>\n",
              "      <td>is</td>\n",
              "      <td>listed</td>\n",
              "      <td>The configuration of this model is listed in the bottom line of Table 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1416</th>\n",
              "      <td>configuration</td>\n",
              "      <td>is listed in</td>\n",
              "      <td>line of Table 3</td>\n",
              "      <td>The configuration of this model is listed in the bottom line of Table 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1417</th>\n",
              "      <td>configuration</td>\n",
              "      <td>is listed in</td>\n",
              "      <td>bottom line of Table 3</td>\n",
              "      <td>The configuration of this model is listed in the bottom line of Table 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1418</th>\n",
              "      <td>configuration</td>\n",
              "      <td>is listed in</td>\n",
              "      <td>bottom line</td>\n",
              "      <td>The configuration of this model is listed in the bottom line of Table 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1419</th>\n",
              "      <td>configuration</td>\n",
              "      <td>is listed in</td>\n",
              "      <td>line</td>\n",
              "      <td>The configuration of this model is listed in the bottom line of Table 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1420</th>\n",
              "      <td>Training</td>\n",
              "      <td>took on</td>\n",
              "      <td>8 P100 GPUs</td>\n",
              "      <td>Training took 3.5 days on 8 P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1421</th>\n",
              "      <td>Training</td>\n",
              "      <td>took at_time</td>\n",
              "      <td>3.5 days</td>\n",
              "      <td>Training took 3.5 days on 8 P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1422</th>\n",
              "      <td>our big model</td>\n",
              "      <td>achieves BLEU score On</td>\n",
              "      <td>WMT 2014 English</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1423</th>\n",
              "      <td>our big model</td>\n",
              "      <td>achieves BLEU score On</td>\n",
              "      <td>WMT 2014 English to translation task</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1424</th>\n",
              "      <td>our big model</td>\n",
              "      <td>achieves</td>\n",
              "      <td>BLEU score</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1425</th>\n",
              "      <td>our model</td>\n",
              "      <td>achieves BLEU score On</td>\n",
              "      <td>WMT 2014 English to French translation task</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1426</th>\n",
              "      <td>our model</td>\n",
              "      <td>achieves</td>\n",
              "      <td>BLEU score</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1427</th>\n",
              "      <td>our big model</td>\n",
              "      <td>achieves</td>\n",
              "      <td>BLEU score of 41.0</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1428</th>\n",
              "      <td>our model</td>\n",
              "      <td>achieves BLEU score On</td>\n",
              "      <td>WMT 2014 English</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1429</th>\n",
              "      <td>our model</td>\n",
              "      <td>achieves</td>\n",
              "      <td>BLEU score of 41.0</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1430</th>\n",
              "      <td>our big model</td>\n",
              "      <td>achieves BLEU score On</td>\n",
              "      <td>WMT 2014 English to French translation task</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1431</th>\n",
              "      <td>our model</td>\n",
              "      <td>achieves BLEU score On</td>\n",
              "      <td>WMT 2014 English to translation task</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1432</th>\n",
              "      <td>we</td>\n",
              "      <td>used model For</td>\n",
              "      <td>base models</td>\n",
              "      <td>For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1433</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>single model</td>\n",
              "      <td>For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1434</th>\n",
              "      <td>we</td>\n",
              "      <td>used For</td>\n",
              "      <td>base models</td>\n",
              "      <td>For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1435</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>model obtained</td>\n",
              "      <td>For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1436</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>model</td>\n",
              "      <td>For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1437</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>single model obtained</td>\n",
              "      <td>For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1438</th>\n",
              "      <td>we</td>\n",
              "      <td>averaged</td>\n",
              "      <td>last 20 checkpoints</td>\n",
              "      <td>For the big models , we averaged the last 20 checkpoints .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1439</th>\n",
              "      <td>we</td>\n",
              "      <td>averaged</td>\n",
              "      <td>20 checkpoints</td>\n",
              "      <td>For the big models , we averaged the last 20 checkpoints .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1440</th>\n",
              "      <td>We</td>\n",
              "      <td>used with</td>\n",
              "      <td>beam size</td>\n",
              "      <td>We used beam search with a beam size of 4 and length penalty 0.6 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1441</th>\n",
              "      <td>We</td>\n",
              "      <td>used</td>\n",
              "      <td>beam search</td>\n",
              "      <td>We used beam search with a beam size of 4 and length penalty 0.6 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1442</th>\n",
              "      <td>We</td>\n",
              "      <td>used beam search with</td>\n",
              "      <td>beam size</td>\n",
              "      <td>We used beam search with a beam size of 4 and length penalty 0.6 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1443</th>\n",
              "      <td>hyperparameters</td>\n",
              "      <td>were</td>\n",
              "      <td>chosen</td>\n",
              "      <td>These hyperparameters were chosen after experimentation on the development set .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1444</th>\n",
              "      <td>hyperparameters</td>\n",
              "      <td>were chosen after</td>\n",
              "      <td>experimentation on development set</td>\n",
              "      <td>These hyperparameters were chosen after experimentation on the development set .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1445</th>\n",
              "      <td>hyperparameters</td>\n",
              "      <td>were chosen after</td>\n",
              "      <td>experimentation</td>\n",
              "      <td>These hyperparameters were chosen after experimentation on the development set .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1446</th>\n",
              "      <td>Table 2</td>\n",
              "      <td>summarizes</td>\n",
              "      <td>our results</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447</th>\n",
              "      <td>Table 2</td>\n",
              "      <td>compares</td>\n",
              "      <td>our translation quality</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1448</th>\n",
              "      <td>Table 2</td>\n",
              "      <td>compares training costs to</td>\n",
              "      <td>model architectures</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1449</th>\n",
              "      <td>Table</td>\n",
              "      <td>compares</td>\n",
              "      <td>our translation quality</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1450</th>\n",
              "      <td>Table</td>\n",
              "      <td>summarizes</td>\n",
              "      <td>our results</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1451</th>\n",
              "      <td>Table</td>\n",
              "      <td>compares training costs to</td>\n",
              "      <td>other model architectures</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1452</th>\n",
              "      <td>Table</td>\n",
              "      <td>compares</td>\n",
              "      <td>training costs</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1453</th>\n",
              "      <td>Table</td>\n",
              "      <td>compares training costs to</td>\n",
              "      <td>model architectures</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1454</th>\n",
              "      <td>Table 2</td>\n",
              "      <td>compares</td>\n",
              "      <td>training costs</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1455</th>\n",
              "      <td>Table 2</td>\n",
              "      <td>compares training costs to</td>\n",
              "      <td>other model architectures from literature</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1456</th>\n",
              "      <td>Table</td>\n",
              "      <td>compares training costs to</td>\n",
              "      <td>other model architectures from literature</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457</th>\n",
              "      <td>Table 2</td>\n",
              "      <td>compares training costs to</td>\n",
              "      <td>other model architectures</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1458</th>\n",
              "      <td>Table</td>\n",
              "      <td>compares training costs to</td>\n",
              "      <td>model architectures from literature</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1459</th>\n",
              "      <td>Table 2</td>\n",
              "      <td>compares training costs to</td>\n",
              "      <td>model architectures from literature</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1460</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>number</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1461</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>number</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1462</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>estimate of precision</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1463</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>estimate of sustained single precision</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1464</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>number used</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1465</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>estimate of single precision</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1466</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>number of point operations used</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1467</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>number of point operations</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1468</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>estimate</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1469</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>estimate of sustained precision</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1470</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>number of floating point operations</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1471</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>number of floating point operations used</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1472</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change in performance to translation</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1473</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change on English to translation on development set</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1474</th>\n",
              "      <td>measuring</td>\n",
              "      <td>change on</td>\n",
              "      <td>English</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1475</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change in performance to German translation</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1476</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1477</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change in performance to translation on development set</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1478</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change on English to German translation on development set</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1479</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change in performance on English to translation on development set</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1480</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change to translation on development set</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1481</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change in performance on English</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1482</th>\n",
              "      <td>measuring</td>\n",
              "      <td>change to</td>\n",
              "      <td>German translation on development set</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1483</th>\n",
              "      <td>change</td>\n",
              "      <td>is in</td>\n",
              "      <td>performance</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1484</th>\n",
              "      <td>we</td>\n",
              "      <td>varied in</td>\n",
              "      <td>different ways</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1485</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change to German translation</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1486</th>\n",
              "      <td>measuring</td>\n",
              "      <td>change in</td>\n",
              "      <td>performance</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1487</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change to translation</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1488</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change to German translation on development set</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1489</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change in performance</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1490</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change in performance to German translation on development set</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1491</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change in performance on English to German translation on development set</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1492</th>\n",
              "      <td>we</td>\n",
              "      <td>varied</td>\n",
              "      <td>our base model</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1493</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change in performance on English to translation</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1494</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change on English to translation</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>we</td>\n",
              "      <td>varied in</td>\n",
              "      <td>ways</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>measuring</td>\n",
              "      <td>change to</td>\n",
              "      <td>translation on development set</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change on English to German translation</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change in performance on English to German translation</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>measuring</td>\n",
              "      <td>change to</td>\n",
              "      <td>translation</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1500</th>\n",
              "      <td>measuring</td>\n",
              "      <td>change to</td>\n",
              "      <td>German translation</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1501</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change on English</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1502</th>\n",
              "      <td>We</td>\n",
              "      <td>described in</td>\n",
              "      <td>section</td>\n",
              "      <td>We used beam search as described in the previous section , but no checkpoint averaging .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1503</th>\n",
              "      <td>We</td>\n",
              "      <td>described in</td>\n",
              "      <td>previous section</td>\n",
              "      <td>We used beam search as described in the previous section , but no checkpoint averaging .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1504</th>\n",
              "      <td>We</td>\n",
              "      <td>present</td>\n",
              "      <td>results in Table 3</td>\n",
              "      <td>We present these results in Table 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1505</th>\n",
              "      <td>results</td>\n",
              "      <td>is in</td>\n",
              "      <td>Table 3</td>\n",
              "      <td>We present these results in Table 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1506</th>\n",
              "      <td>We</td>\n",
              "      <td>present</td>\n",
              "      <td>results</td>\n",
              "      <td>We present these results in Table 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1507</th>\n",
              "      <td>we</td>\n",
              "      <td>vary In</td>\n",
              "      <td>Table</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1508</th>\n",
              "      <td>we</td>\n",
              "      <td>keeping amount</td>\n",
              "      <td>described</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1509</th>\n",
              "      <td>we</td>\n",
              "      <td>keeping amount</td>\n",
              "      <td>described in Section 3.2.2</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1510</th>\n",
              "      <td>we</td>\n",
              "      <td>vary number In</td>\n",
              "      <td>Table</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1511</th>\n",
              "      <td>we</td>\n",
              "      <td>vary</td>\n",
              "      <td>attention key</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1512</th>\n",
              "      <td>we</td>\n",
              "      <td>vary</td>\n",
              "      <td>number</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1513</th>\n",
              "      <td>we</td>\n",
              "      <td>vary</td>\n",
              "      <td>number of attention heads</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1514</th>\n",
              "      <td>we</td>\n",
              "      <td>vary attention key In</td>\n",
              "      <td>Table</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1515</th>\n",
              "      <td>we</td>\n",
              "      <td>keeping</td>\n",
              "      <td>amount of computation constant</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1516</th>\n",
              "      <td>we</td>\n",
              "      <td>keeping</td>\n",
              "      <td>amount</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1517</th>\n",
              "      <td>single head attention</td>\n",
              "      <td>is worse than</td>\n",
              "      <td>setting</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1518</th>\n",
              "      <td>single head attention</td>\n",
              "      <td>is</td>\n",
              "      <td>0.9 BLEU worse than best setting</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1519</th>\n",
              "      <td>single head attention</td>\n",
              "      <td>is worse than</td>\n",
              "      <td>best setting</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1520</th>\n",
              "      <td>head attention</td>\n",
              "      <td>is</td>\n",
              "      <td>0.9 BLEU worse than setting</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521</th>\n",
              "      <td>head attention</td>\n",
              "      <td>is</td>\n",
              "      <td>worse</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1522</th>\n",
              "      <td>quality</td>\n",
              "      <td>also drops off with</td>\n",
              "      <td>heads</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1523</th>\n",
              "      <td>single head attention</td>\n",
              "      <td>is</td>\n",
              "      <td>worse</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1524</th>\n",
              "      <td>quality</td>\n",
              "      <td>drops off with</td>\n",
              "      <td>many heads</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1525</th>\n",
              "      <td>single head attention</td>\n",
              "      <td>is worse</td>\n",
              "      <td>0.9 BLEU</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1526</th>\n",
              "      <td>quality</td>\n",
              "      <td>also drops off with</td>\n",
              "      <td>many heads</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1527</th>\n",
              "      <td>head attention</td>\n",
              "      <td>is worse than</td>\n",
              "      <td>best setting</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1528</th>\n",
              "      <td>single head attention</td>\n",
              "      <td>is</td>\n",
              "      <td>0.9 BLEU worse than setting</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1529</th>\n",
              "      <td>quality</td>\n",
              "      <td>drops off with</td>\n",
              "      <td>too many heads</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1530</th>\n",
              "      <td>quality</td>\n",
              "      <td>also drops off with</td>\n",
              "      <td>too many heads</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1531</th>\n",
              "      <td>head attention</td>\n",
              "      <td>is worse than</td>\n",
              "      <td>setting</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1532</th>\n",
              "      <td>head attention</td>\n",
              "      <td>is</td>\n",
              "      <td>0.9 BLEU worse than best setting</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1533</th>\n",
              "      <td>quality</td>\n",
              "      <td>drops off with</td>\n",
              "      <td>heads</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1534</th>\n",
              "      <td>head attention</td>\n",
              "      <td>is worse</td>\n",
              "      <td>0.9 BLEU</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1535</th>\n",
              "      <td>5We</td>\n",
              "      <td>used for</td>\n",
              "      <td>K80</td>\n",
              "      <td>5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1536</th>\n",
              "      <td>5We</td>\n",
              "      <td>used respectively for</td>\n",
              "      <td>K80</td>\n",
              "      <td>5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1537</th>\n",
              "      <td>5We</td>\n",
              "      <td>used</td>\n",
              "      <td>values</td>\n",
              "      <td>5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1538</th>\n",
              "      <td>5We</td>\n",
              "      <td>used values for</td>\n",
              "      <td>K80</td>\n",
              "      <td>5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1539</th>\n",
              "      <td>5We</td>\n",
              "      <td>used values respectively for</td>\n",
              "      <td>K80</td>\n",
              "      <td>5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1540</th>\n",
              "      <td>5We</td>\n",
              "      <td>used respectively</td>\n",
              "      <td>values</td>\n",
              "      <td>5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1541</th>\n",
              "      <td>values</td>\n",
              "      <td>are identical to</td>\n",
              "      <td>those of base model</td>\n",
              "      <td>Unlisted values are identical to those of the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1542</th>\n",
              "      <td>values</td>\n",
              "      <td>are</td>\n",
              "      <td>identical</td>\n",
              "      <td>Unlisted values are identical to those of the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1543</th>\n",
              "      <td>Unlisted values</td>\n",
              "      <td>are identical to</td>\n",
              "      <td>those</td>\n",
              "      <td>Unlisted values are identical to those of the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1544</th>\n",
              "      <td>Unlisted values</td>\n",
              "      <td>are</td>\n",
              "      <td>identical</td>\n",
              "      <td>Unlisted values are identical to those of the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1545</th>\n",
              "      <td>Unlisted values</td>\n",
              "      <td>are identical to</td>\n",
              "      <td>those of base model</td>\n",
              "      <td>Unlisted values are identical to those of the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1546</th>\n",
              "      <td>values</td>\n",
              "      <td>are identical to</td>\n",
              "      <td>those</td>\n",
              "      <td>Unlisted values are identical to those of the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1547</th>\n",
              "      <td>metrics</td>\n",
              "      <td>are on</td>\n",
              "      <td>English</td>\n",
              "      <td>All metrics are on the English to German translation development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1548</th>\n",
              "      <td>metrics</td>\n",
              "      <td>are on</td>\n",
              "      <td>English to translation development set</td>\n",
              "      <td>All metrics are on the English to German translation development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1549</th>\n",
              "      <td>metrics</td>\n",
              "      <td>are on</td>\n",
              "      <td>English to German translation development set</td>\n",
              "      <td>All metrics are on the English to German translation development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1550</th>\n",
              "      <td>Listed perplexities</td>\n",
              "      <td>are per</td>\n",
              "      <td>wordpiece</td>\n",
              "      <td>Listed perplexities are per wordpiece , according to our byte pair encoding , and should not be compared to per word perplexities .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1551</th>\n",
              "      <td>perplexities</td>\n",
              "      <td>are per</td>\n",
              "      <td>wordpiece</td>\n",
              "      <td>Listed perplexities are per wordpiece , according to our byte pair encoding , and should not be compared to per word perplexities .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1552</th>\n",
              "      <td>model quality</td>\n",
              "      <td>reducing</td>\n",
              "      <td>attention size dk</td>\n",
              "      <td>N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1553</th>\n",
              "      <td>quality</td>\n",
              "      <td>reducing</td>\n",
              "      <td>attention key size dk</td>\n",
              "      <td>N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1554</th>\n",
              "      <td>steps</td>\n",
              "      <td>dev</td>\n",
              "      <td>dev 106 base</td>\n",
              "      <td>N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1555</th>\n",
              "      <td>quality</td>\n",
              "      <td>reducing</td>\n",
              "      <td>attention size dk</td>\n",
              "      <td>N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1556</th>\n",
              "      <td>model quality</td>\n",
              "      <td>reducing</td>\n",
              "      <td>attention key size dk</td>\n",
              "      <td>N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1557</th>\n",
              "      <td>We</td>\n",
              "      <td>further observe in</td>\n",
              "      <td>rows C</td>\n",
              "      <td>We further observe in rows C and D that , as expected , bigger models are better , and dropout is very helpful in avoiding over fitting .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1558</th>\n",
              "      <td>We</td>\n",
              "      <td>observe in</td>\n",
              "      <td>rows C</td>\n",
              "      <td>We further observe in rows C and D that , as expected , bigger models are better , and dropout is very helpful in avoiding over fitting .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1559</th>\n",
              "      <td>we</td>\n",
              "      <td>observe results to</td>\n",
              "      <td>base model</td>\n",
              "      <td>In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1560</th>\n",
              "      <td>we</td>\n",
              "      <td>observe</td>\n",
              "      <td>results</td>\n",
              "      <td>In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1561</th>\n",
              "      <td>we</td>\n",
              "      <td>replace</td>\n",
              "      <td>our sinusoidal encoding</td>\n",
              "      <td>In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1562</th>\n",
              "      <td>we</td>\n",
              "      <td>observe</td>\n",
              "      <td>identical results</td>\n",
              "      <td>In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1563</th>\n",
              "      <td>we</td>\n",
              "      <td>observe</td>\n",
              "      <td>nearly identical results</td>\n",
              "      <td>In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1564</th>\n",
              "      <td>we</td>\n",
              "      <td>replace</td>\n",
              "      <td>our positional encoding</td>\n",
              "      <td>In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1565</th>\n",
              "      <td>we</td>\n",
              "      <td>replace</td>\n",
              "      <td>our encoding</td>\n",
              "      <td>In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1566</th>\n",
              "      <td>we</td>\n",
              "      <td>replace</td>\n",
              "      <td>our sinusoidal positional encoding</td>\n",
              "      <td>In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1567</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>replacing</td>\n",
              "      <td>recurrent layers commonly used in encoder decoder architectures with multi</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1568</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>replacing</td>\n",
              "      <td>layers commonly used in encoder decoder architectures with multi</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1569</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>replacing</td>\n",
              "      <td>layers most commonly used in encoder decoder architectures with multi</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1570</th>\n",
              "      <td>sequence transduction model</td>\n",
              "      <td>based entirely on</td>\n",
              "      <td>attention</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1571</th>\n",
              "      <td>7 Conclusion</td>\n",
              "      <td>is In</td>\n",
              "      <td>work</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1572</th>\n",
              "      <td>first sequence transduction model</td>\n",
              "      <td>based entirely on</td>\n",
              "      <td>attention</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1573</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>replacing</td>\n",
              "      <td>recurrent layers most commonly used in encoder decoder architectures with multi</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1574</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>headed</td>\n",
              "      <td>self attention</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1575</th>\n",
              "      <td>sequence transduction model</td>\n",
              "      <td>based on</td>\n",
              "      <td>attention</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1576</th>\n",
              "      <td>first sequence transduction model</td>\n",
              "      <td>based on</td>\n",
              "      <td>attention</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1577</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>can</td>\n",
              "      <td>For translation tasks can trained faster</td>\n",
              "      <td>For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1578</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>can</td>\n",
              "      <td>can trained</td>\n",
              "      <td>For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1579</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>can</td>\n",
              "      <td>can trained faster than architectures</td>\n",
              "      <td>For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1580</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>can</td>\n",
              "      <td>can trained faster</td>\n",
              "      <td>For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1581</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>can</td>\n",
              "      <td>For translation tasks can trained</td>\n",
              "      <td>For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1582</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>can</td>\n",
              "      <td>For translation tasks can trained significantly faster than architectures</td>\n",
              "      <td>For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1583</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>can</td>\n",
              "      <td>can trained significantly faster</td>\n",
              "      <td>For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1584</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>can</td>\n",
              "      <td>For translation tasks can trained significantly faster</td>\n",
              "      <td>For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1585</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>can</td>\n",
              "      <td>can trained significantly faster than architectures</td>\n",
              "      <td>For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1586</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>can</td>\n",
              "      <td>For translation tasks can trained faster than architectures</td>\n",
              "      <td>For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1587</th>\n",
              "      <td>our model</td>\n",
              "      <td>outperforms</td>\n",
              "      <td>even previously reported ensembles</td>\n",
              "      <td>In the former task our best model outperforms even all previously reported ensembles .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1588</th>\n",
              "      <td>our model</td>\n",
              "      <td>outperforms</td>\n",
              "      <td>previously reported ensembles</td>\n",
              "      <td>In the former task our best model outperforms even all previously reported ensembles .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1589</th>\n",
              "      <td>our best model</td>\n",
              "      <td>outperforms</td>\n",
              "      <td>even previously reported ensembles</td>\n",
              "      <td>In the former task our best model outperforms even all previously reported ensembles .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1590</th>\n",
              "      <td>our best model</td>\n",
              "      <td>outperforms</td>\n",
              "      <td>previously reported ensembles</td>\n",
              "      <td>In the former task our best model outperforms even all previously reported ensembles .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1591</th>\n",
              "      <td>We</td>\n",
              "      <td>are excited about</td>\n",
              "      <td>future of attention</td>\n",
              "      <td>We are excited about the future of attention based models and plan to apply them to other tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1592</th>\n",
              "      <td>We</td>\n",
              "      <td>apply</td>\n",
              "      <td>them</td>\n",
              "      <td>We are excited about the future of attention based models and plan to apply them to other tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1593</th>\n",
              "      <td>We</td>\n",
              "      <td>are excited about</td>\n",
              "      <td>future</td>\n",
              "      <td>We are excited about the future of attention based models and plan to apply them to other tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>We</td>\n",
              "      <td>are</td>\n",
              "      <td>excited</td>\n",
              "      <td>We are excited about the future of attention based models and plan to apply them to other tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>problems</td>\n",
              "      <td>involving</td>\n",
              "      <td>input modalities other than text</td>\n",
              "      <td>We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>We</td>\n",
              "      <td>extend Transformer to</td>\n",
              "      <td>problems</td>\n",
              "      <td>We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>problems</td>\n",
              "      <td>involving</td>\n",
              "      <td>input modalities</td>\n",
              "      <td>We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>problems</td>\n",
              "      <td>involving</td>\n",
              "      <td>input modalities other</td>\n",
              "      <td>We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599</th>\n",
              "      <td>We</td>\n",
              "      <td>extend</td>\n",
              "      <td>Transformer</td>\n",
              "      <td>We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1600</th>\n",
              "      <td>outputs</td>\n",
              "      <td>large inputs such as</td>\n",
              "      <td>images</td>\n",
              "      <td>We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1601</th>\n",
              "      <td>outputs</td>\n",
              "      <td>inputs such as</td>\n",
              "      <td>images</td>\n",
              "      <td>We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1602</th>\n",
              "      <td>code</td>\n",
              "      <td>is available at</td>\n",
              "      <td>https</td>\n",
              "      <td>The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1603</th>\n",
              "      <td>we</td>\n",
              "      <td>train</td>\n",
              "      <td>our models</td>\n",
              "      <td>The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1604</th>\n",
              "      <td>code</td>\n",
              "      <td>is</td>\n",
              "      <td>available</td>\n",
              "      <td>The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1605</th>\n",
              "      <td>We</td>\n",
              "      <td>are</td>\n",
              "      <td>grateful to Nal Kalchbrenner for their fruitful comments</td>\n",
              "      <td>Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1606</th>\n",
              "      <td>We</td>\n",
              "      <td>are grateful for</td>\n",
              "      <td>their comments</td>\n",
              "      <td>Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1607</th>\n",
              "      <td>We</td>\n",
              "      <td>are grateful for</td>\n",
              "      <td>their fruitful comments</td>\n",
              "      <td>Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1608</th>\n",
              "      <td>We</td>\n",
              "      <td>are</td>\n",
              "      <td>grateful</td>\n",
              "      <td>Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1609</th>\n",
              "      <td>We</td>\n",
              "      <td>are grateful to</td>\n",
              "      <td>Nal Kalchbrenner</td>\n",
              "      <td>Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1610</th>\n",
              "      <td>We</td>\n",
              "      <td>are</td>\n",
              "      <td>grateful to Nal Kalchbrenner for their comments</td>\n",
              "      <td>Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-86d94000-0247-48ab-80af-af85737b31ed')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-86d94000-0247-48ab-80af-af85737b31ed button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-86d94000-0247-48ab-80af-af85737b31ed');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-01a95955-9528-4b85-9325-df2f12a3b636\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-01a95955-9528-4b85-9325-df2f12a3b636')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-01a95955-9528-4b85-9325-df2f12a3b636 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_94f9440e-2a1d-42ee-9d06-24d43191a6e3\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_94f9440e-2a1d-42ee-9d06-24d43191a6e3 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1611,\n  \"fields\": [\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 184,\n        \"samples\": [\n          \"Llion\",\n          \"we\",\n          \"learning rate\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relation\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 345,\n        \"samples\": [\n          \"extrapolate to\",\n          \"Due is similar to\",\n          \"considering neighborhood in\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"object\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1008,\n        \"samples\": [\n          \"German translation\",\n          \"hidden layer in typical sequence transduction encoder\",\n          \"neighborhood of size r\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 155,\n        \"samples\": [\n          \" To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .\",\n          \" Unlisted values are identical to those of the base model .\",\n          \" Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('The no. of entity pairs extracted without removing duplicates: ', len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HEd52uGQr8U",
        "outputId": "ead6f295-b434-40b6-8f62-b640e6f4ac42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The no. of entity pairs extracted without removing duplicates:  1611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Duplicate Entity pairs"
      ],
      "metadata": {
        "id": "NZVAdfo1Q7Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "25k09Cw7tSWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spacy_work_tokenize(sent):\n",
        "    tokens =[]\n",
        "    spacy_sent = nlp(sent)\n",
        "    for token in spacy_sent:\n",
        "        tokens.append(str(token))\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "7q95viTttCc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grouping entity pairs extracted from the same sentence"
      ],
      "metadata": {
        "id": "qOQ4rRZolNoq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def group_by_sents(original_sentences, df):\n",
        "    from collections import defaultdict\n",
        "\n",
        "    # Initialize a dictionary with the initial words as keys\n",
        "    grouped_sentences = defaultdict(list)\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "      sent = row['subject']+ ' '+ row['relation'] + ' ' + row['object'] + '.'\n",
        "      for org_sent in original_sentences:\n",
        "        if df['sentence'][index] == org_sent:\n",
        "          grouped_sentences[org_sent].append(sent)\n",
        "          break\n",
        "\n",
        "    # Combine the grouped sentences into a single string for each key\n",
        "    result = [' '.join(group) for group in grouped_sentences.values()]\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "hpr-oUznP4d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_sentences = []\n",
        "for index, row in df.iterrows():\n",
        "  original_sentences.append(row['sentence'])"
      ],
      "metadata": {
        "id": "LnifJCmBwCuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = group_by_sents(original_sentences, df)"
      ],
      "metadata": {
        "id": "oCV99lvKoeMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collecting all the unique sentences using Set Difference"
      ],
      "metadata": {
        "id": "rB0HEh40gcDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "without_duplicates = []\n",
        "for x in result:\n",
        "  text = nlp(x)\n",
        "  sents = [sent.text.strip() for sent in text.sents]\n",
        "  tokenized_sets = [set(spacy_work_tokenize(x)) for x in sents]\n",
        "  for k in range(len(tokenized_sets)):\n",
        "        keep_set = True\n",
        "        for l in range(len(tokenized_sets)):\n",
        "            if k != l:\n",
        "                  if not tokenized_sets[k].difference(tokenized_sets[l]):\n",
        "                            keep_set = False\n",
        "                            break\n",
        "        if keep_set:\n",
        "                without_duplicates.append(sents[k])"
      ],
      "metadata": {
        "id": "GGQgCwFfxJKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for getting entity type using ```flair``` and ```Spacy```"
      ],
      "metadata": {
        "id": "GppTnCg4nWa8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flair.nn import Classifier\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "# load the model\n",
        "tagger = Classifier.load('pos')\n",
        "tagger_ner = SequenceTagger.load(\"flair/ner-english-ontonotes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "39c04e78333c4c8d93b3a887da8fdc37",
            "6c444c8266cc4618a8272869a6b36e49",
            "a71d068301624624a76c43a22732398e",
            "12358736d0a64586a11d3defa51f382c",
            "0cd50460d4704707bd6803498669f068",
            "c1c54aa7441845fc9f664f6f7e21d235",
            "2af4362e43c84e628b4d5536255a4a88",
            "422630d1097b401bbf0c69103c9d615e",
            "46ffae3a64614aa3ad3d9ca6fd6acacf",
            "a9256f9c190c4f68a8344c212713444a",
            "fabfb57dd03341309e23b7f20002e091",
            "706cc90d418443249c366ded112814d4",
            "f42b26650c304c99a392c02ef6a529bb",
            "38e8fb44567a4f489bf88cbb12347017",
            "5697eb4982ec4f94ad794704d8381472",
            "e5cd183c06114ceab84b484729c37757",
            "bfa13087d5974480986e70ce3bedc9f1",
            "d132889b97aa4ea49c9d4f56a22ef9b8",
            "6b43cb53cf59448f88623718c40093a0",
            "5b52bd488cf54c55be00c5cf05355d82",
            "7e0d1bb4b89c46ae9bc1a988a851d82e",
            "6f4ad2561d7644beba4d2a04d842bf22"
          ]
        },
        "id": "0QTW7QJM3Yk6",
        "outputId": "14c0bb63-c9f2-4752-e7a0-476008a8bbd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/249M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "39c04e78333c4c8d93b3a887da8fdc37"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-17 17:48:23,953 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.47G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "706cc90d418443249c366ded112814d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-06-17 17:48:42,672 SequenceTagger predicts: Dictionary with 75 tags: O, S-PERSON, B-PERSON, E-PERSON, I-PERSON, S-GPE, B-GPE, E-GPE, I-GPE, S-ORG, B-ORG, E-ORG, I-ORG, S-DATE, B-DATE, E-DATE, I-DATE, S-CARDINAL, B-CARDINAL, E-CARDINAL, I-CARDINAL, S-NORP, B-NORP, E-NORP, I-NORP, S-MONEY, B-MONEY, E-MONEY, I-MONEY, S-PERCENT, B-PERCENT, E-PERCENT, I-PERCENT, S-ORDINAL, B-ORDINAL, E-ORDINAL, I-ORDINAL, S-LOC, B-LOC, E-LOC, I-LOC, S-TIME, B-TIME, E-TIME, I-TIME, S-WORK_OF_ART, B-WORK_OF_ART, E-WORK_OF_ART, I-WORK_OF_ART, S-FAC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def spans_filter(sent1):\n",
        "    spans = list(sent1.ents) + list(sent1.noun_chunks) # collecting nodes\n",
        "    spans = spacy.util.filter_spans(spans)\n",
        "    with sent1.retokenize() as retokenizer:\n",
        "        [retokenizer.merge(span, attrs={'tag': span.root.tag, 'dep': span.root.dep}) for span in spans]\n",
        "    deps = [token.dep_ for token in sent1]\n",
        "\n",
        "    return deps\n"
      ],
      "metadata": {
        "id": "JwRcNIEl5f7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_meaning = {\n",
        "    'CARDINAL': 'Cardinal',\n",
        "    'DATE': 'Date',\n",
        "    'EVENT': 'Event',\n",
        "    'FAC': 'Building',\n",
        "    'GPE': 'Geo-political entity',\n",
        "    'LANGUAGE': 'Language',\n",
        "    'LAW': 'Law',\n",
        "    'LOC': 'Location',\n",
        "    'MONEY': 'Money',\n",
        "    'NORP': 'Affiliation',\n",
        "    'ORDINAL': 'Ordinal',\n",
        "    'ORG': 'Organization',\n",
        "    'PERCENT': 'Percent value',\n",
        "    'PERSON': 'Person',\n",
        "    'PRODUCT': 'Product',\n",
        "    'QUANTITY': 'Quantity',\n",
        "    'TIME': 'Time',\n",
        "    'WORK_OF_ART': 'Work of art'\n",
        "}"
      ],
      "metadata": {
        "id": "xCwHeah485KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tags_dict = {\n",
        "    \"ADD\": \"Email\",\n",
        "    \"AFX\": \"Affix\",\n",
        "    \"CC\": \"Conjunction\",\n",
        "    \"CD\": \"Cardinal\",\n",
        "    \"DT\": \"Determiner\",\n",
        "    \"EX\": \"Existential there\",\n",
        "    \"FW\": \"Foreign word\",\n",
        "    \"HYPH\": \"Hyphen\",\n",
        "    \"IN\": \"Preposition or subordinating conjunction\",\n",
        "    \"JJ\": \"Adjective\",\n",
        "    \"JJR\": \"Adjective\",\n",
        "    \"JJS\": \"Adjective\",\n",
        "    \"LS\": \"List item marker\",\n",
        "    \"MD\": \"Modal\",\n",
        "    \"NFP\": \"Punctuation\",\n",
        "    \"NN\": \"Noun\",\n",
        "    \"NNP\": \"Name\",\n",
        "    \"NNPS\": \"Noun\",\n",
        "    \"NNS\": \"Noun\",\n",
        "    \"PDT\": \"Predeterminer\",\n",
        "    \"POS\": \"Possessive ending\",\n",
        "    \"PRP\": \"pronoun\",\n",
        "    \"PRP$\": \"pronoun\",\n",
        "    \"RB\": \"Adverb\",\n",
        "    \"RBR\": \"Adverb\",\n",
        "    \"RBS\": \"Adverb\",\n",
        "    \"RP\": \"Particle\",\n",
        "    \"SYM\": \"Symbol\",\n",
        "    \"TO\": \"to\",\n",
        "    \"UH\": \"Interjection\",\n",
        "    \"VB\": \"Verb\",\n",
        "    \"VBD\": \"Verb\",\n",
        "    \"VBG\": \"Verb\",\n",
        "    \"VBN\": \"Verb\",\n",
        "    \"VBP\": \"Verb\",\n",
        "    \"VBZ\": \"Verb\",\n",
        "    \"WDT\": \"determiner\",\n",
        "    \"WP\": \"pronoun\",\n",
        "    \"WP$\": \"pronoun\",\n",
        "    \"WRB\": \"adverb\",\n",
        "    \"XX\": \"Unknown\"\n",
        "}"
      ],
      "metadata": {
        "id": "Kv7mo7EI7dCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ent_type_sub_using_flair(ent, sent,use_original_sent,original):\n",
        "        unwanted_tokens = (\n",
        "#             'PRON',  # pronouns\n",
        "            'PART',  # particle\n",
        "#             'DET',  # determiner\n",
        "            'SCONJ',  # subordinating conjunction\n",
        "            'PUNCT',  # punctuation\n",
        "            'SYM',  # symbol\n",
        "            'X',  # other\n",
        "        )\n",
        "        if use_original_sent:\n",
        "          sent = original\n",
        "        else:\n",
        "          sent = sent\n",
        "\n",
        "        # make a sentence\n",
        "        sentence = Sentence(sent)\n",
        "\n",
        "        # predict pos tags\n",
        "        tagger.predict(sentence)\n",
        "        dict_ent_types = {}\n",
        "        for i in range(len(sentence)):\n",
        "            x = sentence[i]\n",
        "            dict_ent_types[x.text] = x.tag\n",
        "\n",
        "        if ent.text not in dict_ent_types:\n",
        "               ent_type = ''\n",
        "        else:\n",
        "            temp_ent_type = dict_ent_types[ent.text]\n",
        "            ent_type = pos_tags_dict.get(temp_ent_type)\n",
        "\n",
        "\n",
        "        if ent_type == '':\n",
        "            ent_type = 'None'\n",
        "            ent = ' '.join(str(t.text) for t in\n",
        "                           nlp(str(ent)) if t.pos_\n",
        "                           not in unwanted_tokens)\n",
        "        elif ent_type in ('NOMINAL', 'Cardinal number', 'ORDINAL') and str(ent).find(' ') == -1:\n",
        "            refined = ''\n",
        "            for i in range(len(sent) - ent.i):\n",
        "                if ent.nbor(i).pos_ not in ('PUNCT'):\n",
        "                    refined += ' ' + str(ent.nbor(i))\n",
        "                else:\n",
        "                    ent = refined.strip()\n",
        "                    break\n",
        "\n",
        "        #predict ner tags\n",
        "        tagger_ner.predict(sentence)\n",
        "        dict_ner_types = {}\n",
        "        for entity in sentence.get_spans('ner'):\n",
        "                dict_ner_types[entity.text] = entity.tag\n",
        "\n",
        "        ner_type = \"Couldn't find\"\n",
        "        for key in dict_ner_types:\n",
        "\n",
        "            if str(ent) in key:\n",
        "                ner_type = dict_ner_types[key]\n",
        "                ner_type = tag_meaning.get(ner_type)\n",
        "\n",
        "\n",
        "        if ner_type != \"Couldn't find\":\n",
        "            ent_type = ner_type\n",
        "        else:\n",
        "            ent_type = ent_type\n",
        "\n",
        "\n",
        "        return ent, ent_type"
      ],
      "metadata": {
        "id": "x9JC5IXV45FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retokenize(sent1):\n",
        "    spans = list(sent1.ents) + list(sent1.noun_chunks) # collecting nodes\n",
        "    spans = spacy.util.filter_spans(spans)\n",
        "    with sent1.retokenize() as retokenizer:\n",
        "        [retokenizer.merge(span, attrs={'tag': span.root.tag, 'dep': span.root.dep}) for span in spans]\n",
        "\n",
        "    return sent1\n"
      ],
      "metadata": {
        "id": "U7hCVeZlV0S-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_tokens(ent,sent,unwanted_tokens):\n",
        "    refined = ''\n",
        "    if hasattr(ent, 'i'):\n",
        "        sent = nlp(sent)\n",
        "        retokenized_sent = retokenize(sent)\n",
        "        for i in range(len(retokenized_sent) - ent.i):\n",
        "            if ent.nbor(i).pos_ not in unwanted_tokens:\n",
        "                refined += ' ' + str(ent.nbor(i))\n",
        "            else:\n",
        "                ent = refined.strip()\n",
        "                break\n",
        "    else:\n",
        "        sent = nlp(sent)\n",
        "        retokenized_sent = retokenize(sent)\n",
        "        for token in retokenized_sent:\n",
        "            if str(token) == ent:\n",
        "                ent = token\n",
        "                for i in range(len(retokenized_sent) - token.i):\n",
        "                    if token.nbor(i).pos_ not in unwanted_tokens:\n",
        "                        refined += ' ' + str(token.nbor(i))\n",
        "                    else:\n",
        "                        ent = refined.strip()\n",
        "                        break\n",
        "    return ent"
      ],
      "metadata": {
        "id": "9xqD2BjhVrnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ent_type_obj_using_flair(ent, sent,use_original_sent,original): # modular approach\n",
        "        unwanted_tokens = (\n",
        "            'PRON',  # pronouns\n",
        "            'PART',  # particle\n",
        "            'PREP',\n",
        "#             'DET',  # determiner\n",
        "            'SCONJ',  # subordinating conjunction\n",
        "            'PUNCT',  # punctuation\n",
        "            'SYM',  # symbol\n",
        "#             'X',  # other\n",
        "        )\n",
        "\n",
        "        if use_original_sent:\n",
        "          sent = original\n",
        "        else:\n",
        "          sent = sent\n",
        "\n",
        "        # make a sentence\n",
        "        sentence = Sentence(sent)\n",
        "\n",
        "        # predict pos tags\n",
        "        tagger.predict(sentence)\n",
        "        dict_ent_types = {}\n",
        "        for i in range(len(sentence)):\n",
        "            x = sentence[i]\n",
        "            dict_ent_types[x.text] = x.tag\n",
        "\n",
        "        if ent.text not in dict_ent_types:\n",
        "               ent_type = ''\n",
        "        else:\n",
        "            temp_ent_type = dict_ent_types[ent.text]\n",
        "            ent_type = pos_tags_dict.get(temp_ent_type)\n",
        "\n",
        "\n",
        "        if ent_type == '':\n",
        "            ent_type = 'None'\n",
        "            ent = ' '.join(str(t.text) for t in\n",
        "                           nlp(str(ent)) if t.pos_\n",
        "                           not in unwanted_tokens)\n",
        "        elif ent_type in ('NOMINAL', 'Cardinal number', 'ORDINAL') and str(ent).find(' ') == -1:\n",
        "            refined = ''\n",
        "            for i in range(len(sent) - ent.i):\n",
        "                if ent.nbor(i).pos_ not in ('PUNCT'):\n",
        "                    refined += ' ' + str(ent.nbor(i))\n",
        "                else:\n",
        "                    ent = refined.strip()\n",
        "                    break\n",
        "\n",
        "        #predict ner tags\n",
        "        tagger_ner.predict(sentence)\n",
        "        dict_ner_types = {}\n",
        "        for entity in sentence.get_spans('ner'):\n",
        "                dict_ner_types[entity.text] = entity.tag\n",
        "\n",
        "        ner_type = \"Couldn't find\"\n",
        "        for key in dict_ner_types:\n",
        "\n",
        "            if str(ent) in key:\n",
        "                ner_type = dict_ner_types[key]\n",
        "                ner_type = tag_meaning.get(ner_type)\n",
        "\n",
        "\n",
        "        if ner_type != \"Couldn't find\":\n",
        "            ent_type = ner_type\n",
        "        else:\n",
        "            ent_type = ent_type\n",
        "\n",
        "        new_ent = add_tokens(ent,sent,unwanted_tokens)\n",
        "\n",
        "\n",
        "\n",
        "        return new_ent, ent_type"
      ],
      "metadata": {
        "id": "k7X7Yk8L45Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ent_type_using_spacy(ent, sent,use_original_sent,original):\n",
        "        unwanted_tokens = (\n",
        "            'PRON',  # pronouns\n",
        "            'PART',  # particle\n",
        "            'DET',  # determiner\n",
        "            'SCONJ',  # subordinating conjunction\n",
        "            'PUNCT',  # punctuation\n",
        "            'SYM',  # symbol\n",
        "            'X',  # other\n",
        "        )\n",
        "        if use_original_sent:\n",
        "          sent = original\n",
        "        else:\n",
        "          sent = sent\n",
        "        ent_type = ent.ent_type_  # get entity type\n",
        "        if ent_type == '':\n",
        "            ent_type = 'NOUN_CHUNK'\n",
        "            ent = ' '.join(str(t.text) for t in\n",
        "                           nlp(str(ent)) if t.pos_\n",
        "                           not in unwanted_tokens and t.is_stop == False)\n",
        "        elif ent_type in ('NOMINAL', 'CARDINAL', 'ORDINAL') and str(ent).find(' ') == -1:\n",
        "            refined = ''\n",
        "            for i in range(len(sent) - ent.i):\n",
        "                if ent.nbor(i).pos_ not in ('VERB', 'PUNCT'):\n",
        "                    refined += ' ' + str(ent.nbor(i))\n",
        "                else:\n",
        "                    ent = refined.strip()\n",
        "                    break\n",
        "\n",
        "        return ent, ent_type"
      ],
      "metadata": {
        "id": "4V97JeZbWGAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplicate_rows(df, without_duplicates, pos_flag='flair', use_original_sent = False):\n",
        "\n",
        "  new_rows = []\n",
        "  for index, row in df.iterrows():\n",
        "        sent = row['subject']+ ' '+ row['relation'] + ' ' + row['object'] + '.'\n",
        "        for x in without_duplicates:\n",
        "              if x == sent:\n",
        "                        original =  df['sentence'][index]\n",
        "                        obj = nlp(row['object'])\n",
        "                        subject = nlp(row['subject'])\n",
        "                        if pos_flag == 'flair':\n",
        "                          obj_ent, obj_ent_type = get_ent_type_obj_using_flair(obj,use_original_sent, sent, original)\n",
        "                          sub_ent, sub_ent_type = get_ent_type_sub_using_flair(subject,use_original_sent,sent, original)\n",
        "                        elif pos_flag == 'spacy':\n",
        "                          obj_ent, obj_ent_type = get_ent_type_using_spacy(obj, use_original_sent, sent, original)\n",
        "                          sub_ent, sub_ent_type = get_ent_type_using_spacy(subject, use_original_sent, sent, original)\n",
        "                        else:\n",
        "                          print('Enter a valid POS tagger')\n",
        "\n",
        "                        new_rows.append({'subject':row['subject'], 'relation': row['relation'],'object':row['object'],'subject_type':sub_ent_type,'object_type': obj_ent_type, 'sentence': df['sentence'][index] })\n",
        "                        without_duplicates.remove(x)\n",
        "                        break\n",
        "  new_df = pd.DataFrame(new_rows)\n",
        "\n",
        "  return new_df\n"
      ],
      "metadata": {
        "id": "dMAVZKlIWUnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df = remove_duplicate_rows(df, without_duplicates, pos_flag='flair', use_original_sent = False)"
      ],
      "metadata": {
        "id": "cb6F93ImaWhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4wwYp4aB2K-q",
        "outputId": "fdff4c12-a56a-481c-c5c9-f263a0f752b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      subject  \\\n",
              "0                               convolutional   \n",
              "1                               convolutional   \n",
              "2                                   Attention   \n",
              "3                           performing models   \n",
              "4                           performing models   \n",
              "5                                          We   \n",
              "6                                          We   \n",
              "7                                   Our model   \n",
              "8                                   Our model   \n",
              "9                                   our model   \n",
              "10                                  our model   \n",
              "11                                  our model   \n",
              "12                                  our model   \n",
              "13                                  our model   \n",
              "14                                  our model   \n",
              "15   1 Introduction Recurrent neural networks   \n",
              "16                           Numerous efforts   \n",
              "17                              Listing order   \n",
              "18                                      Jakob   \n",
              "19                                     Ashish   \n",
              "20                                       Noam   \n",
              "21                                       Noam   \n",
              "22                                       Noam   \n",
              "23                                       Noam   \n",
              "24         evaluated countless model variants   \n",
              "25                                      Llion   \n",
              "26                                      Aidan   \n",
              "27                                      Aidan   \n",
              "28                                     Lukasz   \n",
              "29                                    results   \n",
              "30                                      Aidan   \n",
              "31                                       Work   \n",
              "32                            31st Conference   \n",
              "33                            31st Conference   \n",
              "34                            31st Conference   \n",
              "35                           Recurrent models   \n",
              "36                                       they   \n",
              "37                                      steps   \n",
              "38                          sequential nature   \n",
              "39                          sequential nature   \n",
              "40                         memory constraints   \n",
              "41                                Recent work   \n",
              "42                                Recent work   \n",
              "43                                Recent work   \n",
              "44                   significant improvements   \n",
              "45               compelling sequence modeling   \n",
              "46                       Attention mechanisms   \n",
              "47                       Attention mechanisms   \n",
              "48                       Attention mechanisms   \n",
              "49                             their distance   \n",
              "50                       Attention mechanisms   \n",
              "51                                conjunction   \n",
              "52                  such attention mechanisms   \n",
              "53                                         we   \n",
              "54                                         we   \n",
              "55                                         we   \n",
              "56                                Transformer   \n",
              "57                                Transformer   \n",
              "58                                Transformer   \n",
              "59                                Transformer   \n",
              "60                                        all   \n",
              "61                                        all   \n",
              "62                                     number   \n",
              "63                                     number   \n",
              "64                                         it   \n",
              "65                             Self attention   \n",
              "66                        attention mechanism   \n",
              "67                             Self attention   \n",
              "68                                Transformer   \n",
              "69                                         we   \n",
              "70                                         we   \n",
              "71                                         we   \n",
              "72                                    encoder   \n",
              "73                                    encoder   \n",
              "74                                    decoder   \n",
              "75                                    decoder   \n",
              "76                         output sequence y1   \n",
              "77                                      model   \n",
              "78                                      model   \n",
              "79                       overall architecture   \n",
              "80                                Transformer   \n",
              "81                       overall architecture   \n",
              "82                                      point   \n",
              "83                       overall architecture   \n",
              "84                                      layer   \n",
              "85                                      first   \n",
              "86                                     second   \n",
              "87                                         We   \n",
              "88                                         We   \n",
              "89                                 Sublayer x   \n",
              "90                                    decoder   \n",
              "91                                    Decoder   \n",
              "92                             two sub layers   \n",
              "93                                         we   \n",
              "94                                         we   \n",
              "95                   self attention sub layer   \n",
              "96                   self attention sub layer   \n",
              "97                                predictions   \n",
              "98                          output embeddings   \n",
              "99                                    masking   \n",
              "100                                    output   \n",
              "101                                    weight   \n",
              "102                                    weight   \n",
              "103                  our particular attention   \n",
              "104                                     input   \n",
              "105                                     input   \n",
              "106                      Multi Head Attention   \n",
              "107                                     query   \n",
              "108                                        we   \n",
              "109                                        We   \n",
              "110                                        We   \n",
              "111                    compatibility function   \n",
              "112                        Additive attention   \n",
              "113                           forward network   \n",
              "114                                        it   \n",
              "115                                        it   \n",
              "116                                       two   \n",
              "117                     dot product attention   \n",
              "118                     dot product attention   \n",
              "119                        additive attention   \n",
              "120                        additive attention   \n",
              "121                              dot products   \n",
              "122                              dot products   \n",
              "123                                        it   \n",
              "124                              dot products   \n",
              "125                                        we   \n",
              "126                                        we   \n",
              "127                                   queries   \n",
              "128                                        it   \n",
              "129                                        it   \n",
              "130                                        it   \n",
              "131                                        it   \n",
              "132                                        it   \n",
              "133                                   queries   \n",
              "134                                        we   \n",
              "135                                        we   \n",
              "136                                     These   \n",
              "137                                 averaging   \n",
              "138                                         q   \n",
              "139                              dot products   \n",
              "140              independent random variables   \n",
              "141                                       4To   \n",
              "142                                     their   \n",
              "143                                        we   \n",
              "144                                        we   \n",
              "145                                        we   \n",
              "146                  total computational cost   \n",
              "147                                   encoder   \n",
              "148                                    output   \n",
              "149                     self attention layers   \n",
              "150                     self attention layers   \n",
              "151                                        We   \n",
              "152                 leftward information flow   \n",
              "153                                        We   \n",
              "154                two linear transformations   \n",
              "155                    linear transformations   \n",
              "156                                      they   \n",
              "157                                      they   \n",
              "158                          two convolutions   \n",
              "159                               inner layer   \n",
              "160                            dimensionality   \n",
              "161                                        we   \n",
              "162                                        we   \n",
              "163                                        we   \n",
              "164                                        We   \n",
              "165                                        we   \n",
              "166                                        we   \n",
              "167                                        we   \n",
              "168                                        we   \n",
              "169                   3.5 Positional Encoding   \n",
              "170                                     model   \n",
              "171                         relative position   \n",
              "172                                        we   \n",
              "173                                        we   \n",
              "174                                        we   \n",
              "175                                        we   \n",
              "176                                         k   \n",
              "177                                      size   \n",
              "178                                         k   \n",
              "179                                         k   \n",
              "180                                         d   \n",
              "181                                         n   \n",
              "182                                       two   \n",
              "183                      positional encodings   \n",
              "184                      positional encodings   \n",
              "185                                         i   \n",
              "186                                        we   \n",
              "187                                       pos   \n",
              "188                                        we   \n",
              "189                               wavelengths   \n",
              "190                                        it   \n",
              "191                                        it   \n",
              "192                                   PEpos k   \n",
              "193                                        We   \n",
              "194                                     model   \n",
              "195                                        We   \n",
              "196                         identical results   \n",
              "197                                        We   \n",
              "198                                        it   \n",
              "199                                        We   \n",
              "200                                        it   \n",
              "201                                     model   \n",
              "202                          4 Self Attention   \n",
              "203                                        we   \n",
              "204                                   section   \n",
              "205                                        we   \n",
              "206                              hidden layer   \n",
              "207                                   section   \n",
              "208                          4 Self Attention   \n",
              "209                              hidden layer   \n",
              "210                          4 Self Attention   \n",
              "211                                        we   \n",
              "212                                        we   \n",
              "213                                       One   \n",
              "214                                   Another   \n",
              "215                                   Another   \n",
              "216                                     third   \n",
              "217                              dependencies   \n",
              "218                           forward signals   \n",
              "219                                        it   \n",
              "220                      self attention layer   \n",
              "221                                      case   \n",
              "222                     self attention layers   \n",
              "223                            self attention   \n",
              "224                            self attention   \n",
              "225                                        We   \n",
              "226                single convolutional layer   \n",
              "227                                     stack   \n",
              "228                                      case   \n",
              "229                                     Doing   \n",
              "230                                complexity   \n",
              "231                            self attention   \n",
              "232                                        We   \n",
              "233                                  examples   \n",
              "234                                        We   \n",
              "235                                      many   \n",
              "236                                        We   \n",
              "237                                 Sentences   \n",
              "238                                 Sentences   \n",
              "239                                        we   \n",
              "240                                        we   \n",
              "241                                        we   \n",
              "242                            Sentence pairs   \n",
              "243                            training batch   \n",
              "244                                        We   \n",
              "245                             training step   \n",
              "246                           our base models   \n",
              "247                           our base models   \n",
              "248                                        We   \n",
              "249                                big models   \n",
              "250                                        We   \n",
              "251                        first warmup steps   \n",
              "252                                        We   \n",
              "253                             learning rate   \n",
              "254                                        We   \n",
              "255                                        We   \n",
              "256                                        We   \n",
              "257                                      sums   \n",
              "258                                        we   \n",
              "259                                        we   \n",
              "260                                        we   \n",
              "261                                        we   \n",
              "262                                        we   \n",
              "263                                Model BLEU   \n",
              "264                                     model   \n",
              "265                   German translation task   \n",
              "266                             configuration   \n",
              "267                                  Training   \n",
              "268                                  Training   \n",
              "269                             our big model   \n",
              "270                             our big model   \n",
              "271                                        we   \n",
              "272                                        we   \n",
              "273                                        we   \n",
              "274                                        We   \n",
              "275                           hyperparameters   \n",
              "276                                   Table 2   \n",
              "277                                   Table 2   \n",
              "278                                   Table 2   \n",
              "279                                        We   \n",
              "280                                        We   \n",
              "281                                    change   \n",
              "282                                        we   \n",
              "283                                        we   \n",
              "284                                        we   \n",
              "285                                        We   \n",
              "286                                        We   \n",
              "287                                   results   \n",
              "288                                        we   \n",
              "289                                        we   \n",
              "290                                        we   \n",
              "291                                        we   \n",
              "292                                        we   \n",
              "293                     single head attention   \n",
              "294                                   quality   \n",
              "295                                       5We   \n",
              "296                           Unlisted values   \n",
              "297                                   metrics   \n",
              "298                       Listed perplexities   \n",
              "299                                     steps   \n",
              "300                                        we   \n",
              "301                                        we   \n",
              "302                                        we   \n",
              "303                              7 Conclusion   \n",
              "304         first sequence transduction model   \n",
              "305                               Transformer   \n",
              "306                               Transformer   \n",
              "307                               Transformer   \n",
              "308                            our best model   \n",
              "309                                        We   \n",
              "310                                        We   \n",
              "311                                  problems   \n",
              "312                                        We   \n",
              "313                                   outputs   \n",
              "314                                      code   \n",
              "315                                        we   \n",
              "316                                        We   \n",
              "\n",
              "                                        relation  \\\n",
              "0                                        include   \n",
              "1                                        include   \n",
              "2                                         is All   \n",
              "3                   also connect decoder through   \n",
              "4                   also connect encoder through   \n",
              "5                                        propose   \n",
              "6                                        propose   \n",
              "7                                 improving over   \n",
              "8                                       achieves   \n",
              "9                                    establishes   \n",
              "10                 establishes model state after   \n",
              "11                                   establishes   \n",
              "12                    establishes model state On   \n",
              "13                       establishes fraction On   \n",
              "14                    establishes fraction after   \n",
              "15                                     memory in   \n",
              "16                                          push   \n",
              "17                                            is   \n",
              "18                           replacing RNNs with   \n",
              "19                                       is with   \n",
              "20                                        became   \n",
              "21                                      proposed   \n",
              "22                                      proposed   \n",
              "23                                      proposed   \n",
              "24                                         is in   \n",
              "25                        also experimented with   \n",
              "26                                     replacing   \n",
              "27                                     designing   \n",
              "28                                 spent at_time   \n",
              "29                        massively accelerating   \n",
              "30                                 spent at_time   \n",
              "31                                  performed at   \n",
              "32                                          NIPS   \n",
              "33                                          NIPS   \n",
              "34                                          NIPS   \n",
              "35                              typically factor   \n",
              "36                         Aligning positions to   \n",
              "37                                         is in   \n",
              "38                                     precludes   \n",
              "39                     precludes parallelization   \n",
              "40                                         limit   \n",
              "41   also improving model performance in case of   \n",
              "42                                  has achieved   \n",
              "43             has achieved improvements through   \n",
              "44                                         is in   \n",
              "45                                         is in   \n",
              "46                                   have become   \n",
              "47                     allowing modeling without   \n",
              "48                                      allowing   \n",
              "49                                         is in   \n",
              "50                                   have become   \n",
              "51                                       is with   \n",
              "52                           however are used in   \n",
              "53                 propose model architecture In   \n",
              "54                                       propose   \n",
              "55                        propose Transformer In   \n",
              "56                                reach state in   \n",
              "57                                         reach   \n",
              "58                                   reach state   \n",
              "59                                    allows for   \n",
              "60                               use networks as   \n",
              "61                                           use   \n",
              "62                                      grows In   \n",
              "63                                      grows in   \n",
              "64                                         learn   \n",
              "65                                            is   \n",
              "66                                      relating   \n",
              "67                                           has   \n",
              "68                                            is   \n",
              "69                                       discuss   \n",
              "70                  will describe Transformer In   \n",
              "71                                      motivate   \n",
              "72                                     Here maps   \n",
              "73                                          maps   \n",
              "74                                     generates   \n",
              "75                                     generates   \n",
              "76                                         ym at   \n",
              "77                          consuming symbols as   \n",
              "78                                     consuming   \n",
              "79                                      shown in   \n",
              "80                          follows respectively   \n",
              "81                                         using   \n",
              "82                                          wise   \n",
              "83                                         using   \n",
              "84                                           has   \n",
              "85                                            is   \n",
              "86                                            is   \n",
              "87                                   followed by   \n",
              "88                                        employ   \n",
              "89                                            is   \n",
              "90                           is also composed of   \n",
              "91                                   composed of   \n",
              "92                                         is in   \n",
              "93                                Similar employ   \n",
              "94                                   followed by   \n",
              "95                                         is in   \n",
              "96                             prevent positions   \n",
              "97                                     depend on   \n",
              "98                                 are offset by   \n",
              "99                                 combined with   \n",
              "100                               is computed as   \n",
              "101                                  assigned to   \n",
              "102                                           is   \n",
              "103                                       Scaled   \n",
              "104                                  consists of   \n",
              "105                                    values of   \n",
              "106                                  consists of   \n",
              "107                                      is with   \n",
              "108                compute attention function In   \n",
              "109                                    Attention   \n",
              "110                                    Attention   \n",
              "111                                        using   \n",
              "112                                     computes   \n",
              "113                                      is with   \n",
              "114                                          can   \n",
              "115                                        using   \n",
              "116                               are similar in   \n",
              "117                               is much faster   \n",
              "118                               is much faster   \n",
              "119            outperforms dot product attention   \n",
              "120            outperforms dot product attention   \n",
              "121                                      grow in   \n",
              "122                pushing softmax function into   \n",
              "123                                          has   \n",
              "124                                     grow for   \n",
              "125                        scale dot products by   \n",
              "126                                   counteract   \n",
              "127                         learned respectively   \n",
              "128         linearly project values h times with   \n",
              "129                         learned respectively   \n",
              "130          learned projections respectively to   \n",
              "131                   linearly project keys with   \n",
              "132                linearly project queries with   \n",
              "133          learned projections respectively to   \n",
              "134                                     yielding   \n",
              "135                perform attention function in   \n",
              "136                                  depicted in   \n",
              "137                                inhibits With   \n",
              "138                            components of are   \n",
              "139                                          get   \n",
              "140                                      is with   \n",
              "141                                   illustrate   \n",
              "142                                      product   \n",
              "143                              employ heads In   \n",
              "144                                       employ   \n",
              "145                                          use   \n",
              "146                                           is   \n",
              "147                                     contains   \n",
              "148                                        is in   \n",
              "149                              Similarly allow   \n",
              "150                                        is in   \n",
              "151                                      prevent   \n",
              "152                                        is in   \n",
              "153                                    implement   \n",
              "154                                      is with   \n",
              "155                              are same across   \n",
              "156                                      FFN use   \n",
              "157                      FFN use parameters from   \n",
              "158                                      is with   \n",
              "159                                          has   \n",
              "160                                           is   \n",
              "161                      convert input tokens to   \n",
              "162                     convert output tokens to   \n",
              "163                                          use   \n",
              "164                                     also use   \n",
              "165                                        share   \n",
              "166                       share weight matrix In   \n",
              "167                          multiply weights In   \n",
              "168                          multiply weights by   \n",
              "169                                  must inject   \n",
              "170                                         make   \n",
              "171                                        is in   \n",
              "172                             add encodings to   \n",
              "173                             add encodings To   \n",
              "174                             add encodings at   \n",
              "175                                          add   \n",
              "176                                      size in   \n",
              "177                                        is in   \n",
              "178                                           is   \n",
              "179                                      size of   \n",
              "180                                           is   \n",
              "181                                           is   \n",
              "182                                          can   \n",
              "183                                         have   \n",
              "184                        have dimension dmodel   \n",
              "185                                           is   \n",
              "186                                          use   \n",
              "187                                           is   \n",
              "188                        use sine functions In   \n",
              "189                                         form   \n",
              "190                                        allow   \n",
              "191                                        allow   \n",
              "192                                          can   \n",
              "193                               chose function   \n",
              "194                                 easily learn   \n",
              "195                            also experimented   \n",
              "196                                          see   \n",
              "197                                        chose   \n",
              "198                                    may allow   \n",
              "199                                chose version   \n",
              "200                                    may allow   \n",
              "201                               extrapolate to   \n",
              "202                                      zn with   \n",
              "203                                      compare   \n",
              "204                              In Attention is   \n",
              "205                           compare aspects to   \n",
              "206                                        is in   \n",
              "207                              In Attention is   \n",
              "208                                   zn such as   \n",
              "209                                such as zn is   \n",
              "210                                        is In   \n",
              "211                                   Motivating   \n",
              "212                                     consider   \n",
              "213        is total computational complexity per   \n",
              "214                                 is amount of   \n",
              "215                                    is amount   \n",
              "216                                           is   \n",
              "217                          is key challenge in   \n",
              "218                                         have   \n",
              "219                                        learn   \n",
              "220                      connects positions with   \n",
              "221                                      is with   \n",
              "222                                          are   \n",
              "223                                        could   \n",
              "224                                      improve   \n",
              "225              investigate approach further in   \n",
              "226                                      is with   \n",
              "227                                   increasing   \n",
              "228                                        is in   \n",
              "229                                     requires   \n",
              "230                                           is   \n",
              "231                                  could yield   \n",
              "232         inspect attention distributions from   \n",
              "233                                        is in   \n",
              "234                                      discuss   \n",
              "235                                      exhibit   \n",
              "236                                   trained on   \n",
              "237                                        using   \n",
              "238                                         were   \n",
              "239                                     used For   \n",
              "240                                         used   \n",
              "241                                    used into   \n",
              "242                     were batched together by   \n",
              "243                                    contained   \n",
              "244                                      trained   \n",
              "245                                         took   \n",
              "246                         described throughout   \n",
              "247                                        using   \n",
              "248                                      trained   \n",
              "249                             were trained for   \n",
              "250                                         used   \n",
              "251                                     training   \n",
              "252                                       varied   \n",
              "253         decreasing thereafter proportionally   \n",
              "254                                         used   \n",
              "255                                       employ   \n",
              "256                                        apply   \n",
              "257                                        is in   \n",
              "258                             apply dropout In   \n",
              "259                             apply dropout to   \n",
              "260                                          use   \n",
              "261                                 use rate For   \n",
              "262                                     employed   \n",
              "263                                     Training   \n",
              "264                                       learns   \n",
              "265                            to Translation is   \n",
              "266                                 is listed in   \n",
              "267                                      took on   \n",
              "268                                 took at_time   \n",
              "269                                     achieves   \n",
              "270                       achieves BLEU score On   \n",
              "271                               used model For   \n",
              "272                                         used   \n",
              "273                                     averaged   \n",
              "274                        used beam search with   \n",
              "275                            were chosen after   \n",
              "276                                   summarizes   \n",
              "277                                     compares   \n",
              "278                   compares training costs to   \n",
              "279                                     estimate   \n",
              "280                                     estimate   \n",
              "281                                        is in   \n",
              "282                                    varied in   \n",
              "283                                    measuring   \n",
              "284                                       varied   \n",
              "285                                 described in   \n",
              "286                                      present   \n",
              "287                                        is in   \n",
              "288                               keeping amount   \n",
              "289                               vary number In   \n",
              "290                                         vary   \n",
              "291                        vary attention key In   \n",
              "292                                      keeping   \n",
              "293                                           is   \n",
              "294                          also drops off with   \n",
              "295                 used values respectively for   \n",
              "296                             are identical to   \n",
              "297                                       are on   \n",
              "298                                      are per   \n",
              "299                                          dev   \n",
              "300                           observe results to   \n",
              "301                                      observe   \n",
              "302                                      replace   \n",
              "303                                        is In   \n",
              "304                            based entirely on   \n",
              "305                                    replacing   \n",
              "306                                       headed   \n",
              "307                                          can   \n",
              "308                                  outperforms   \n",
              "309                            are excited about   \n",
              "310                                        apply   \n",
              "311                                    involving   \n",
              "312                        extend Transformer to   \n",
              "313                         large inputs such as   \n",
              "314                              is available at   \n",
              "315                                        train   \n",
              "316                                          are   \n",
              "\n",
              "                                                                                                       object  \\\n",
              "0                                                                                                     encoder   \n",
              "1                                                                                                     decoder   \n",
              "2                                                                                                    you Need   \n",
              "3                                                                                         attention mechanism   \n",
              "4                                                                                         attention mechanism   \n",
              "5                                                                                                 Transformer   \n",
              "6    new simple network architecture based solely on attention mechanisms dispensing with recurrence entirely   \n",
              "7                                                    existing best results including ensembles by over 2 BLEU   \n",
              "8                                                     28.4 BLEU on WMT 2014 Englishto German translation task   \n",
              "9                                                            new single model state of art BLEU score of 41.0   \n",
              "10                                                                                                   training   \n",
              "11                                            small fraction of training costs of best models from literature   \n",
              "12                                                                WMT 2014 English to French translation task   \n",
              "13                                                                WMT 2014 English to French translation task   \n",
              "14                                                                                                   training   \n",
              "15                                                                                                 particular   \n",
              "16                                                                    boundaries of recurrent language models   \n",
              "17                                                                                                     random   \n",
              "18                                                                                             self attention   \n",
              "19                                                                                             Illia designed   \n",
              "20                                                                     other person involved in nearly detail   \n",
              "21                                                                                       multi head attention   \n",
              "22                                                                               scaled dot product attention   \n",
              "23                                                                     parameter free position representation   \n",
              "24                                                                                      our original codebase   \n",
              "25                                                                                       novel model variants   \n",
              "26                                                                                       our earlier codebase   \n",
              "27                                                                             various parts of tensor2tensor   \n",
              "28                                                                                        countless long days   \n",
              "29                                                                                               our research   \n",
              "30                                                                                        countless long days   \n",
              "31                                                                                            Google Research   \n",
              "32                                                                                                 Long Beach   \n",
              "33                                                                                                       2017   \n",
              "34                                                                                                         CA   \n",
              "35                                                      computation along symbol positions of input sequences   \n",
              "36                                                                                  steps in computation time   \n",
              "37                                                                                           computation time   \n",
              "38                                                                   parallelization within training examples   \n",
              "39                                                                                   memory constraints limit   \n",
              "40                                                                                   batching across examples   \n",
              "41                                                                                                     latter   \n",
              "42                                                       significant improvements in computational efficiency   \n",
              "43                                                                                    factorization tricks 18   \n",
              "44                                                                                   computational efficiency   \n",
              "45                                                                                              various tasks   \n",
              "46                                             integral part of compelling sequence modeling in various tasks   \n",
              "47                                                                regard to their distance in input sequences   \n",
              "48                                                                                   modeling of dependencies   \n",
              "49                                                                                            input sequences   \n",
              "50                                               allowing without regard to their distance in input sequences   \n",
              "51                                                                                          recurrent network   \n",
              "52                                                                         conjunction with recurrent network   \n",
              "53                                                                                                       work   \n",
              "54                                                                                   instead relying entirely   \n",
              "55                                                                                                       work   \n",
              "56                                                                                        translation quality   \n",
              "57                                                                                           new state of art   \n",
              "58                                                   trained for as little as twelve hours on eight P100 GPUs   \n",
              "59                                                                         significantly more parallelization   \n",
              "60                                                                                       basic building block   \n",
              "61                                                                              convolutional neural networks   \n",
              "62                                                                                                     models   \n",
              "63                                                                                 distance between positions   \n",
              "64                                                                  dependencies between distant positions 11   \n",
              "65                                                                                        attention mechanism   \n",
              "66                                                                     different positions of single sequence   \n",
              "67                                                                                      has used successfully   \n",
              "68                                                first transduction model relying entirely on self attention   \n",
              "69                                                                                             its advantages   \n",
              "70                                                                                         following sections   \n",
              "71                                                                                             self attention   \n",
              "72                                                                input sequence of symbol representations x1   \n",
              "73                                                               xn to sequence of continuous representations   \n",
              "74                                                                          ym of symbols one element at time   \n",
              "75                                                                                         output sequence y1   \n",
              "76                                                                                                       time   \n",
              "77                                                                                           additional input   \n",
              "78                                                                               previously generated symbols   \n",
              "79                                                                                       left halves Figure 1   \n",
              "80                                                                                       overall architecture   \n",
              "81                                                                                     stacked self attention   \n",
              "82                                                                         fully connected layers for encoder   \n",
              "83                                                                                                 point wise   \n",
              "84                                                                                             two sub layers   \n",
              "85                                                                        multi head self attention mechanism   \n",
              "86                                                                                  simple position2 Figure 1   \n",
              "87                                                                                      layer normalization 1   \n",
              "88                                                       residual connection 10 around each of two sub layers   \n",
              "89                                                             where function implemented by sub layer itself   \n",
              "90                                                                              stack of N 6 identical layers   \n",
              "91                                                                                                      stack   \n",
              "92                                                                                              encoder layer   \n",
              "93                                                             residual connections around each of sub layers   \n",
              "94                                                                                        layer normalization   \n",
              "95                                                                                                    decoder   \n",
              "96                                                                          attending to subsequent positions   \n",
              "97                                                            outputs at positions less than i. 3.2 Attention   \n",
              "98                                                                                               one position   \n",
              "99                                                                                                       fact   \n",
              "100                                                                                    weighted sum of values   \n",
              "101                                                                                                     value   \n",
              "102                                  where computed by compatibility function of query with corresponding key   \n",
              "103                                                                            Dot Product Attention Figure 2   \n",
              "104                                                                                   queries of dimension dk   \n",
              "105                                                                                              dimension dv   \n",
              "106                                                              several attention layers running in parallel   \n",
              "107                                                                                                      keys   \n",
              "108                                                                                                  practice   \n",
              "109                                                                                      V softmax QKT dk V 1   \n",
              "110                                                                      dot product multiplicative attention   \n",
              "111                                                                                                      feed   \n",
              "112                                                                                    compatibility function   \n",
              "113                                                                                       single hidden layer   \n",
              "114                                                                                           can implemented   \n",
              "115                                                               highly optimized matrix multiplication code   \n",
              "116                                                                                    theoretical complexity   \n",
              "117                                                                         similar in theoretical complexity   \n",
              "118                                                                                           can implemented   \n",
              "119                                                   for small values of dk two mechanisms perform similarly   \n",
              "120                                                                         scaling for larger values of dk 3   \n",
              "121                                                                                                 magnitude   \n",
              "122                                                                                                   regions   \n",
              "123                                                                               extremely small gradients 4   \n",
              "124                                                                                        large values of dk   \n",
              "125                                                                                                      1 dk   \n",
              "126                                                                                                    effect   \n",
              "127                                                                                        linear projections   \n",
              "128                                                                                                 different   \n",
              "129                                                                                        linear projections   \n",
              "130                                                                                                        dk   \n",
              "131                                                                                                 different   \n",
              "132                                                                                                 different   \n",
              "133                                                                                                        dk   \n",
              "134                                                                              dv dimensional output values   \n",
              "135                                                                                                  parallel   \n",
              "136                                                                                                  Figure 2   \n",
              "137                                                                                     single attention head   \n",
              "138                                                                              independent random variables   \n",
              "139                                                                                                     large   \n",
              "140                                                                                                    mean 0   \n",
              "141                                                                                                    assume   \n",
              "142                                                                                           q k dk i 1 qiki   \n",
              "143                                                                                                      work   \n",
              "144                                                                             h 8 parallel attention layers   \n",
              "145                                                                                         dk dv dmodel h 64   \n",
              "146        Due to reduced dimension of head similar to that of single head attention with full dimensionality   \n",
              "147                                                                                     self attention layers   \n",
              "148                                                                                                   encoder   \n",
              "149                                                                                       position in decoder   \n",
              "150                                                                                                   decoder   \n",
              "151                                                                      leftward information flow in decoder   \n",
              "152                                                                                                   decoder   \n",
              "153                                                                    inside of scaled dot product attention   \n",
              "154                                                                                        ReLU activation in   \n",
              "155                                                                                       different positions   \n",
              "156                                                                                      different parameters   \n",
              "157                                                                                            layer to layer   \n",
              "158                                                                                             kernel size 1   \n",
              "159                                                                                   dimensionality dff 2048   \n",
              "160                                                                                                dmodel 512   \n",
              "161                                                                               vectors of dimension dmodel   \n",
              "162                                                                               vectors of dimension dmodel   \n",
              "163                                                                                        learned embeddings   \n",
              "164                                                                                                 the usual   \n",
              "165                                             same weight matrix between two embedding layers similar to 24   \n",
              "166                                                                                                 our model   \n",
              "167                                                                                          embedding layers   \n",
              "168                                                                                                    dmodel   \n",
              "169                                                 information about relative position of tokens in sequence   \n",
              "170                                                                                  use of order of sequence   \n",
              "171                                                                                                  sequence   \n",
              "172                                                                                          input embeddings   \n",
              "173                                                                                                       end   \n",
              "174                                                                                                 5 Table 1   \n",
              "175                                                                                      positional encodings   \n",
              "176                                                                                 restricted self attention   \n",
              "177                                                                                 restricted self attention   \n",
              "178                                                                               kernel size of convolutions   \n",
              "179                                                                                              neighborhood   \n",
              "180                                                                                  representation dimension   \n",
              "181                                                                                           sequence length   \n",
              "182                                                                                                can summed   \n",
              "183                                                                       same dimension dmodel as embeddings   \n",
              "184                                                                                                can summed   \n",
              "185                                                                                                 dimension   \n",
              "186                                                                   sine functions of different frequencies   \n",
              "187                                                                                            where position   \n",
              "188                                                                                                      work   \n",
              "189                                                                                     geometric progression   \n",
              "190                                                                                              easily learn   \n",
              "191                                                                                                     model   \n",
              "192                                      since for fixed offset k can represented as linear function of PEpos   \n",
              "193                                                                                           we hypothesized   \n",
              "194                                                                              attend by relative positions   \n",
              "195                                                                                                     using   \n",
              "196                                                                                             Table 3 row E   \n",
              "197                                                                                        sinusoidal version   \n",
              "198                                                                                                     model   \n",
              "199                                                                                              it may allow   \n",
              "200                              extrapolate to sequence lengths longer than ones encountered during training   \n",
              "201                                             sequence lengths longer than ones encountered during training   \n",
              "202                                                                                                        xi   \n",
              "203                                                                  various aspects of self attention layers   \n",
              "204                                                                              zn with xi such hidden layer   \n",
              "205                                                                            recurrent layers commonly used   \n",
              "206                                                                     typical sequence transduction encoder   \n",
              "207                                                                         xn to sequence of equal length z1   \n",
              "208                                                     hidden layer in typical sequence transduction encoder   \n",
              "209                                                                                                     zi Rd   \n",
              "210                                                                                                   section   \n",
              "211                                                                                 our use of self attention   \n",
              "212                                                                                          three desiderata   \n",
              "213                                                                                                     layer   \n",
              "214                                                                                               computation   \n",
              "215                                                                                          can parallelized   \n",
              "216                                                    path length between long range dependencies in network   \n",
              "217                                                                          many sequence transduction tasks   \n",
              "218                                                                                       traverse in network   \n",
              "219                                                                                long range dependencies 11   \n",
              "220                                                       constant number of sequentially executed operations   \n",
              "221           sentence representations used by state of art models in machine translations such word piece 31   \n",
              "222                                         In terms of computational complexity faster than recurrent layers   \n",
              "223                                                                                          could restricted   \n",
              "224                                                                       computational performance for tasks   \n",
              "225                                                                                               future work   \n",
              "226                                                                                          kernel width k n   \n",
              "227                                                  length of longest paths between two positions in network   \n",
              "228                                                                           case of dilated convolutions 15   \n",
              "229                                                                       stack of O n k convolutional layers   \n",
              "230                                        Even with k n however equal to combination of self attention layer   \n",
              "231                                                                                 more interpretable models   \n",
              "232                                                                                                our models   \n",
              "233                                                                                                  appendix   \n",
              "234                                                                                      examples in appendix   \n",
              "235                                                      behavior related to syntactic structure of sentences   \n",
              "236                   standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs   \n",
              "237                                                                                                 byte pair   \n",
              "238                                                                                                   encoded   \n",
              "239                                                                                            English French   \n",
              "240                          significantly larger WMT 2014 English French dataset consisting of 36M sentences   \n",
              "241                                                                            32000 word piece vocabulary 31   \n",
              "242                                                                               approximate sequence length   \n",
              "243                                                                                     set of sentence pairs   \n",
              "244                                                                                                our models   \n",
              "245                                                                                         about 0.4 seconds   \n",
              "246                                                                                                     paper   \n",
              "247                                                                                           hyperparameters   \n",
              "248                                                                                               base models   \n",
              "249                                                                                    300,000 steps 3.5 days   \n",
              "250                                                                                         Adam optimizer 17   \n",
              "251                                                                                                     steps   \n",
              "252                                                                                             learning rate   \n",
              "253                                                                                                        it   \n",
              "254                                                                                         warmup steps 4000   \n",
              "255                                                                             three types of regularization   \n",
              "256                                                                                                dropout 27   \n",
              "257                                                                                                   encoder   \n",
              "258                                                                                                  addition   \n",
              "259                                                                             sums of embeddings in encoder   \n",
              "260                                                                                         rate of Pdrop 0.1   \n",
              "261                                                                                                base model   \n",
              "262                                                                        label smoothing of value ls 0.1 30   \n",
              "263                                                                                             Cost FLOPs EN   \n",
              "264                                                                                               more unsure   \n",
              "265                                                          big transformer model Transformer big in Table 2   \n",
              "266                                                                                    bottom line of Table 3   \n",
              "267                                                                                               8 P100 GPUs   \n",
              "268                                                                                                  3.5 days   \n",
              "269                                                                                        BLEU score of 41.0   \n",
              "270                                                               WMT 2014 English to French translation task   \n",
              "271                                                                                               base models   \n",
              "272                                                                                     single model obtained   \n",
              "273                                                                                       last 20 checkpoints   \n",
              "274                                                                                                 beam size   \n",
              "275                                                                        experimentation on development set   \n",
              "276                                                                                               our results   \n",
              "277                                                                                   our translation quality   \n",
              "278                                                                 other model architectures from literature   \n",
              "279                                                                    estimate of sustained single precision   \n",
              "280                                                                  number of floating point operations used   \n",
              "281                                                                                               performance   \n",
              "282                                                                                            different ways   \n",
              "283                                 change in performance on English to German translation on development set   \n",
              "284                                                                                            our base model   \n",
              "285                                                                                          previous section   \n",
              "286                                                                                        results in Table 3   \n",
              "287                                                                                                   Table 3   \n",
              "288                                                                                described in Section 3.2.2   \n",
              "289                                                                                                     Table   \n",
              "290                                                                                 number of attention heads   \n",
              "291                                                                                                     Table   \n",
              "292                                                                            amount of computation constant   \n",
              "293                                                                          0.9 BLEU worse than best setting   \n",
              "294                                                                                            too many heads   \n",
              "295                                                                                                       K80   \n",
              "296                                                                                       those of base model   \n",
              "297                                                             English to German translation development set   \n",
              "298                                                                                                 wordpiece   \n",
              "299                                                                                              dev 106 base   \n",
              "300                                                                                                base model   \n",
              "301                                                                                  nearly identical results   \n",
              "302                                                                        our sinusoidal positional encoding   \n",
              "303                                                                                                      work   \n",
              "304                                                                                                 attention   \n",
              "305                           recurrent layers most commonly used in encoder decoder architectures with multi   \n",
              "306                                                                                            self attention   \n",
              "307                                 For translation tasks can trained significantly faster than architectures   \n",
              "308                                                                        even previously reported ensembles   \n",
              "309                                                                                       future of attention   \n",
              "310                                                                                                      them   \n",
              "311                                                                          input modalities other than text   \n",
              "312                                                                                                  problems   \n",
              "313                                                                                                    images   \n",
              "314                                                                                                     https   \n",
              "315                                                                                                our models   \n",
              "316                                                  grateful to Nal Kalchbrenner for their fruitful comments   \n",
              "\n",
              "     subject_type           object_type  \\\n",
              "0       Adjective                  Noun   \n",
              "1       Adjective                  Noun   \n",
              "2            Noun                  None   \n",
              "3            None                  None   \n",
              "4            None                  None   \n",
              "5         pronoun               Product   \n",
              "6         pronoun                  None   \n",
              "7            None                  None   \n",
              "8            None                  None   \n",
              "9            None                  None   \n",
              "10           None                  Noun   \n",
              "11           None                  None   \n",
              "12           None                  None   \n",
              "13           None                  None   \n",
              "14           None                  Noun   \n",
              "15           None             Adjective   \n",
              "16           None                  None   \n",
              "17           None             Adjective   \n",
              "18         Person                  None   \n",
              "19         Person                  None   \n",
              "20         Person                  None   \n",
              "21         Person                  None   \n",
              "22         Person                  None   \n",
              "23         Person                  None   \n",
              "24           None                  None   \n",
              "25   Organization                  None   \n",
              "26         Person                  None   \n",
              "27         Person                  None   \n",
              "28         Person                  None   \n",
              "29           Noun                  None   \n",
              "30         Person                  None   \n",
              "31           Noun          Organization   \n",
              "32           None  Geo-political entity   \n",
              "33           None                  Date   \n",
              "34           None  Geo-political entity   \n",
              "35           None                  None   \n",
              "36        pronoun                  None   \n",
              "37           Noun                  None   \n",
              "38           None                  None   \n",
              "39           None                  None   \n",
              "40           None                  None   \n",
              "41           None             Adjective   \n",
              "42           None                  None   \n",
              "43           None                  None   \n",
              "44           None                  None   \n",
              "45           None                  None   \n",
              "46           None                  None   \n",
              "47           None                  None   \n",
              "48           None                  None   \n",
              "49           None                  None   \n",
              "50           None                  None   \n",
              "51           Noun                  None   \n",
              "52           None                  None   \n",
              "53        pronoun                  Noun   \n",
              "54        pronoun                  None   \n",
              "55        pronoun                  Noun   \n",
              "56        Product                  None   \n",
              "57        Product                  None   \n",
              "58        Product                  None   \n",
              "59        Product                  None   \n",
              "60     Determiner                  None   \n",
              "61     Determiner                  None   \n",
              "62           Noun                  Noun   \n",
              "63           Noun                  None   \n",
              "64        pronoun                  None   \n",
              "65           None                  None   \n",
              "66           None                  None   \n",
              "67           None                  None   \n",
              "68        Product                  None   \n",
              "69        pronoun                  None   \n",
              "70        pronoun                  None   \n",
              "71        pronoun                  None   \n",
              "72           Noun                  None   \n",
              "73           Noun                  None   \n",
              "74           Noun                  None   \n",
              "75           Noun                  None   \n",
              "76           None                  Noun   \n",
              "77           Noun                  None   \n",
              "78           Noun                  None   \n",
              "79           None                  None   \n",
              "80        Product                  None   \n",
              "81           None                  None   \n",
              "82           Noun                  None   \n",
              "83           None                  None   \n",
              "84           Noun                  None   \n",
              "85        Ordinal                  None   \n",
              "86        Ordinal                  None   \n",
              "87        pronoun                  None   \n",
              "88        pronoun                  None   \n",
              "89   Organization                  None   \n",
              "90           Noun                  None   \n",
              "91           Noun                  Noun   \n",
              "92           None                  None   \n",
              "93        pronoun                  None   \n",
              "94        pronoun                  None   \n",
              "95           None                  Noun   \n",
              "96           None                  None   \n",
              "97           Noun                  None   \n",
              "98           None                  None   \n",
              "99           Noun                  Noun   \n",
              "100          Noun                  None   \n",
              "101          Noun                  Noun   \n",
              "102          Noun                  None   \n",
              "103          None                  None   \n",
              "104          Noun                  None   \n",
              "105          Noun                  None   \n",
              "106          None                  None   \n",
              "107          Noun                  Noun   \n",
              "108       pronoun                  Noun   \n",
              "109       pronoun                  None   \n",
              "110       pronoun                  None   \n",
              "111          None                  Noun   \n",
              "112          None                  None   \n",
              "113          None                  None   \n",
              "114       pronoun                  None   \n",
              "115       pronoun                  None   \n",
              "116      Cardinal                  None   \n",
              "117          None                  None   \n",
              "118          None                  None   \n",
              "119          None                  None   \n",
              "120          None                  None   \n",
              "121          None                  Noun   \n",
              "122          None                  Noun   \n",
              "123       pronoun                  None   \n",
              "124          None                  None   \n",
              "125       pronoun              Quantity   \n",
              "126       pronoun                  Noun   \n",
              "127          Noun                  None   \n",
              "128       pronoun             Adjective   \n",
              "129       pronoun                  None   \n",
              "130       pronoun          Foreign word   \n",
              "131       pronoun             Adjective   \n",
              "132       pronoun             Adjective   \n",
              "133          Noun          Foreign word   \n",
              "134       pronoun                  None   \n",
              "135       pronoun                  Noun   \n",
              "136    Determiner                  None   \n",
              "137          Verb                  None   \n",
              "138          Noun                  None   \n",
              "139          None             Adjective   \n",
              "140          None                  None   \n",
              "141      Cardinal                  Verb   \n",
              "142       pronoun                  None   \n",
              "143       pronoun                  Noun   \n",
              "144       pronoun                  None   \n",
              "145       pronoun                  None   \n",
              "146          None                  None   \n",
              "147          Noun                  None   \n",
              "148          Noun                  Noun   \n",
              "149          None                  None   \n",
              "150          None                  Noun   \n",
              "151       pronoun                  None   \n",
              "152          None                  Noun   \n",
              "153       pronoun                  None   \n",
              "154          None                  None   \n",
              "155          None                  None   \n",
              "156       pronoun                  None   \n",
              "157       pronoun                  None   \n",
              "158          None                  None   \n",
              "159          None                  None   \n",
              "160          Noun                  None   \n",
              "161       pronoun                  None   \n",
              "162       pronoun                  None   \n",
              "163       pronoun                  None   \n",
              "164       pronoun                  None   \n",
              "165       pronoun                  None   \n",
              "166       pronoun                  None   \n",
              "167       pronoun                  None   \n",
              "168       pronoun                  Noun   \n",
              "169          None                  None   \n",
              "170          Noun                  None   \n",
              "171          None                  Noun   \n",
              "172       pronoun                  None   \n",
              "173       pronoun                  Noun   \n",
              "174       pronoun                  None   \n",
              "175       pronoun                  None   \n",
              "176          None                  None   \n",
              "177          Noun                  None   \n",
              "178          None                  None   \n",
              "179          None                  Noun   \n",
              "180          None                  None   \n",
              "181   Punctuation                  None   \n",
              "182      Cardinal                  None   \n",
              "183          None                  None   \n",
              "184          None                  None   \n",
              "185       pronoun                  Noun   \n",
              "186       pronoun                  None   \n",
              "187          Noun                  None   \n",
              "188       pronoun                  Noun   \n",
              "189          Noun                  None   \n",
              "190       pronoun                  None   \n",
              "191       pronoun                  Noun   \n",
              "192          None                  None   \n",
              "193       pronoun                  None   \n",
              "194          Noun                  None   \n",
              "195       pronoun                  Verb   \n",
              "196          None                  None   \n",
              "197       pronoun                  None   \n",
              "198       pronoun                  Noun   \n",
              "199       pronoun                  None   \n",
              "200       pronoun                  None   \n",
              "201          Noun                  None   \n",
              "202          None                  Noun   \n",
              "203       pronoun                  None   \n",
              "204          Noun                  None   \n",
              "205       pronoun                  None   \n",
              "206          None                  None   \n",
              "207          Noun                  None   \n",
              "208          None                  None   \n",
              "209          None                  None   \n",
              "210          None                  Noun   \n",
              "211       pronoun                  None   \n",
              "212       pronoun                  None   \n",
              "213      Cardinal                  Noun   \n",
              "214    Determiner                  Noun   \n",
              "215    Determiner                  None   \n",
              "216       Ordinal                  None   \n",
              "217          Noun                  None   \n",
              "218          None                  None   \n",
              "219       pronoun                  None   \n",
              "220          None                  None   \n",
              "221          Noun                  None   \n",
              "222          None                  None   \n",
              "223          None                  None   \n",
              "224          None                  None   \n",
              "225       pronoun                  None   \n",
              "226          None                  None   \n",
              "227          Noun                  None   \n",
              "228          Noun                  None   \n",
              "229          Verb                  None   \n",
              "230          Noun                  None   \n",
              "231          None                  None   \n",
              "232       pronoun                  None   \n",
              "233          Noun                  Noun   \n",
              "234       pronoun                  None   \n",
              "235     Adjective                  None   \n",
              "236       pronoun                  None   \n",
              "237          Noun                  None   \n",
              "238          Noun                  Verb   \n",
              "239       pronoun                  None   \n",
              "240       pronoun                  None   \n",
              "241       pronoun                  None   \n",
              "242          None                  None   \n",
              "243          None                  None   \n",
              "244       pronoun                  None   \n",
              "245          None                  Time   \n",
              "246          None                  Noun   \n",
              "247          None                  Noun   \n",
              "248       pronoun                  None   \n",
              "249          None                  None   \n",
              "250       pronoun                  None   \n",
              "251          None                  Noun   \n",
              "252       pronoun                  None   \n",
              "253          None               pronoun   \n",
              "254       pronoun                  None   \n",
              "255       pronoun                  None   \n",
              "256       pronoun                  None   \n",
              "257          Noun                  Noun   \n",
              "258       pronoun                  Noun   \n",
              "259       pronoun                  None   \n",
              "260       pronoun                  None   \n",
              "261       pronoun                  None   \n",
              "262       pronoun                  None   \n",
              "263          None                  None   \n",
              "264          Noun                  None   \n",
              "265          None                  None   \n",
              "266          Noun                  None   \n",
              "267          Noun                  None   \n",
              "268          Noun                  Date   \n",
              "269          None                  None   \n",
              "270          None                  None   \n",
              "271       pronoun                  None   \n",
              "272       pronoun                  None   \n",
              "273       pronoun                  None   \n",
              "274       pronoun                  None   \n",
              "275          Noun                  None   \n",
              "276          None                  None   \n",
              "277          None                  None   \n",
              "278          None                  None   \n",
              "279       pronoun                  None   \n",
              "280       pronoun                  None   \n",
              "281          Noun                  Noun   \n",
              "282       pronoun                  None   \n",
              "283       pronoun                  None   \n",
              "284       pronoun                  None   \n",
              "285       pronoun                  None   \n",
              "286       pronoun                  None   \n",
              "287          Noun                  None   \n",
              "288       pronoun                  None   \n",
              "289       pronoun                  Name   \n",
              "290       pronoun                  None   \n",
              "291       pronoun                  Name   \n",
              "292       pronoun                  None   \n",
              "293          None                  None   \n",
              "294          Noun                  None   \n",
              "295       pronoun                  Name   \n",
              "296          None                  None   \n",
              "297          Noun                  None   \n",
              "298          None                  Noun   \n",
              "299          Noun                  None   \n",
              "300       pronoun                  None   \n",
              "301       pronoun                  None   \n",
              "302       pronoun                  None   \n",
              "303          None                  Noun   \n",
              "304          None                  Noun   \n",
              "305       Product                  None   \n",
              "306       Product                  None   \n",
              "307       Product                  None   \n",
              "308          None                  None   \n",
              "309       pronoun                  None   \n",
              "310       pronoun               pronoun   \n",
              "311          Noun                  None   \n",
              "312       pronoun                  Noun   \n",
              "313          Noun                  Noun   \n",
              "314          Noun                  Noun   \n",
              "315       pronoun                  None   \n",
              "316       pronoun                  None   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   sentence  \n",
              "0                                 Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .  \n",
              "1                                 Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .  \n",
              "2                                 Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .  \n",
              "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          The best performing models also connect the encoder and decoder through an attention mechanism .  \n",
              "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                            We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .  \n",
              "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "8                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .  \n",
              "9                                                                                                                                                                                                                                                                                                                                                                             On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "10                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "11                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "12                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "13                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "14                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .  \n",
              "15                                                                                                                                                                                                                                                                                                                                     1 Introduction Recurrent neural networks , long short term memory 12 and gated recurrent 7 neural networks in particular , have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation 29 , 2 , 5 .  \n",
              "16                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .  \n",
              "17                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Listing order is random .  \n",
              "18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Jakob proposed replacing RNNs with self attention and started the effort to evaluate this idea .  \n",
              "19                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Ashish , with Illia , designed and implemented the first Transformer models and has been crucially involved in every aspect of this work .  \n",
              "20                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "21                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "22                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "23                                                                                                                                                                                                                                                                                                                                                                                                                                                           Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .  \n",
              "24                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Niki designed , implemented , tuned and evaluated countless model variants in our original codebase and tensor2tensor .  \n",
              "25                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .  \n",
              "26                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "27                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "28                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "29                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "30                                                                                                                                                                                                                                                                                                                                                                                                                              Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .  \n",
              "31                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Work performed while at Google Research .  \n",
              "32                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .  \n",
              "33                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .  \n",
              "34                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .  \n",
              "35                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Recurrent models typically factor computation along the symbol positions of the input and output sequences .  \n",
              "36                                                                                                                                                                                                                                                                                                                                                                                                                                                      Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .  \n",
              "37                                                                                                                                                                                                                                                                                                                                                                                                                                                      Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .  \n",
              "38                                                                                                                                                                                                                                                                                                                                                                                                                                        This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .  \n",
              "39                                                                                                                                                                                                                                                                                                                                                                                                                                        This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .  \n",
              "40                                                                                                                                                                                                                                                                                                                                                                                                                                        This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .  \n",
              "41                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "42                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "43                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "44                                                                                                                                                                                                                                                                                                                                                                                                                            Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .  \n",
              "45                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "46                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "47                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "48                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "49                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "50                                                                                                                                                                                                                                                                                                                                                                                                Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .  \n",
              "51                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "52                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .  \n",
              "53                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .  \n",
              "54                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .  \n",
              "55                                                                                                                                                                                                                                                                                                                                                                                                                                        In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .  \n",
              "56                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "57                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "58                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "59                                                                                                                                                                                                                                                                                                                                                                                                                                         The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .  \n",
              "60                                                                                                                                                                                                                                                                                                                                 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .  \n",
              "61                                                                                                                                                                                                                                                                                                                                 2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .  \n",
              "62                                                                                                                                                                                                                                                                                                                                                                                                                      In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .  \n",
              "63                                                                                                                                                                                                                                                                                                                                                                                                                      In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .  \n",
              "64                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        This makes it more difficult to learn dependencies between distant positions 11 .  \n",
              "65                                                                                                                                                                                                                                                                                                                                                                                                                                                     Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .  \n",
              "66                                                                                                                                                                                                                                                                                                                                                                                                                                                     Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .  \n",
              "67                                                                                                                                                                                                                                                                                                                                                                                                               Self attention has been used successfully in a variety of tasks including reading comprehension , abstractive summarization , textual entailment and learning task independent sentence representations 4 , 22 , 23 , 19 .  \n",
              "68                                                                                                                                                                                                                                                                                                                                                                                                       To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .  \n",
              "69                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .  \n",
              "70                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .  \n",
              "71                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .  \n",
              "72                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "73                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .  \n",
              "74                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .  \n",
              "75                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .  \n",
              "76                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .  \n",
              "77                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .  \n",
              "78                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .  \n",
              "79                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "80                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "81                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "82                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "83                                                                                                                                                                                                                                                                                                                                                                                                                The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .  \n",
              "84                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Each layer has two sub layers .  \n",
              "85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .  \n",
              "86                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .  \n",
              "87                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .  \n",
              "88                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .  \n",
              "89                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .  \n",
              "90                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Decoder : The decoder is also composed of a stack of N 6 identical layers .  \n",
              "91                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Decoder : The decoder is also composed of a stack of N 6 identical layers .  \n",
              "92                                                                                                                                                                                                                                                                                                                                                                                                                                                             In addition to the two sub layers in each encoder layer , the decoder inserts a third sub layer , which performs multi head attention over the output of the encoder stack .  \n",
              "93                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "94                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .  \n",
              "95                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .  \n",
              "96                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .  \n",
              "97                                                                                                                                                                                                                                                  This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .  \n",
              "98                                                                                                                                                                                                                                                  This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .  \n",
              "99                                                                                                                                                                                                                                                  This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .  \n",
              "100                                                                                                                                                                                                                                                                                                                                                                                                                                                      The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "101                                                                                                                                                                                                                                                                                                                                                                                                                                                      The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "102                                                                                                                                                                                                                                                                                                                                                                                                                                                      The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .  \n",
              "103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             3.2.1 Scaled Dot Product Attention We call our particular attention Scaled Dot Product Attention Figure 2 .  \n",
              "104                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The input consists of queries and keys of dimension dk , and values of dimension dv .  \n",
              "105                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   The input consists of queries and keys of dimension dk , and values of dimension dv .  \n",
              "106                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   right Multi Head Attention consists of several attention layers running in parallel .  \n",
              "107                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            query with all keys , divide each by dk , and apply a softmax function to obtain the weights on the values .  \n",
              "108                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .  \n",
              "109                                                                                                                                                                                                                                                                                                                                                                                                                                       We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .  \n",
              "110                                                                                                                                                                                                                                                                                                                                                                                                                                       We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .  \n",
              "111                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Additive attention computes the compatibility function using a feed forward network with a single hidden layer .  \n",
              "112                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Additive attention computes the compatibility function using a feed forward network with a single hidden layer .  \n",
              "113                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Additive attention computes the compatibility function using a feed forward network with a single hidden layer .  \n",
              "114                                                                                                                                                                                                                                                                                                                                                                                                                       While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "115                                                                                                                                                                                                                                                                                                                                                                                                                       While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "116                                                                                                                                                                                                                                                                                                                                                                                                                       While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "117                                                                                                                                                                                                                                                                                                                                                                                                                       While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "118                                                                                                                                                                                                                                                                                                                                                                                                                       While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .  \n",
              "119                                                                                                                                                                                                                                                                                                                                                                                                                                                                    While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "120                                                                                                                                                                                                                                                                                                                                                                                                                                                                    While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .  \n",
              "121                                                                                                                                                                                                                                                                                                                                                                                                                                                                We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "122                                                                                                                                                                                                                                                                                                                                                                                                                                                                We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "123                                                                                                                                                                                                                                                                                                                                                                                                                                                                We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "124                                                                                                                                                                                                                                                                                                                                                                                                                                                                We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .  \n",
              "125                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         To counteract this effect , we scale the dot products by 1 dk .  \n",
              "126                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         To counteract this effect , we scale the dot products by 1 dk .  \n",
              "127                                                                                                                                                                                                                                                                                                                             3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "128                                                                                                                                                                                                                                                                                                                             3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "129                                                                                                                                                                                                                                                                                                                             3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "130                                                                                                                                                                                                                                                                                                                             3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "131                                                                                                                                                                                                                                                                                                                             3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "132                                                                                                                                                                                                                                                                                                                             3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "133                                                                                                                                                                                                                                                                                                                             3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .  \n",
              "134                                                                                                                                                                                                                                                                                                                                                                                                                                                                           On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .  \n",
              "135                                                                                                                                                                                                                                                                                                                                                                                                                                                                           On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .  \n",
              "136                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             These are concatenated and once again projected , resulting in the final values , as depicted in Figure 2 .  \n",
              "137                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                With a single attention head , averaging inhibits this .  \n",
              "138                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "139                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "140                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "141                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .  \n",
              "142                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Then their dot product , q k dk i 1 qiki , has mean 0 and variance dk .  \n",
              "143                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In this work we employ h 8 parallel attention layers , or heads .  \n",
              "144                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       In this work we employ h 8 parallel attention layers , or heads .  \n",
              "145                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            For each of these we use dk dv dmodel h 64 .  \n",
              "146                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .  \n",
              "147                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The encoder contains self attention layers .  \n",
              "148                                                                                                                                                                                                                                                                                                                                                                                                                                                                              In a self attention layer all of the keys , values and queries come from the same place , in this case , the output of the previous layer in the encoder .  \n",
              "149                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .  \n",
              "150                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .  \n",
              "151                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  We need to prevent leftward information flow in the decoder to preserve the auto regressive property .  \n",
              "152                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  We need to prevent leftward information flow in the decoder to preserve the auto regressive property .  \n",
              "153                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We implement this inside of scaled dot product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections .  \n",
              "154                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         This consists of two linear transformations with a ReLU activation in between .  \n",
              "155                                                                                                                                                                                                                                                                                                                                                                                                                                                                             FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "156                                                                                                                                                                                                                                                                                                                                                                                                                                                                             FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "157                                                                                                                                                                                                                                                                                                                                                                                                                                                                             FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .  \n",
              "158                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Another way of describing this is as two convolutions with kernel size 1 .  \n",
              "159                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .  \n",
              "160                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .  \n",
              "161                                                                                                                                                                                                                                                                                                                                                                                                                                                   3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .  \n",
              "162                                                                                                                                                                                                                                                                                                                                                                                                                                                   3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .  \n",
              "163                                                                                                                                                                                                                                                                                                                                                                                                                                                   3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .  \n",
              "164                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .  \n",
              "165                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "166                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .  \n",
              "167                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         In the embedding layers , we multiply those weights by dmodel .  \n",
              "168                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         In the embedding layers , we multiply those weights by dmodel .  \n",
              "169                                                                                                                                                                                                                                                                                                                                                                           3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "170                                                                                                                                                                                                                                                                                                                                                                           3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "171                                                                                                                                                                                                                                                                                                                                                                           3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .  \n",
              "172                                                                                                                                                                                                                                                                                                                                                                                                                                To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .  \n",
              "173                                                                                                                                                                                                                                                                                                                                                                                                                                To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .  \n",
              "174                                                                                                                                                                                                                                                                                                                                                                                                                                To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .  \n",
              "175                                                                                                                                                                                                                                                                                                                                                                                                                                To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .  \n",
              "176                                                                                                                                                                                                                                                                                                                                                                                                                                                                   n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "177                                                                                                                                                                                                                                                                                                                                                                                                                                                                   n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "178                                                                                                                                                                                                                                                                                                                                                                                                                                                                   n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "179                                                                                                                                                                                                                                                                                                                                                                                                                                                                   n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "180                                                                                                                                                                                                                                                                                                                                                                                                                                                                   n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "181                                                                                                                                                                                                                                                                                                                                                                                                                                                                   n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .  \n",
              "182                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "183                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "184                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .  \n",
              "185                                                                                                                                                                                                                                                                                                                                                                                                                                   In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .  \n",
              "186                                                                                                                                                                                                                                                                                                                                                                                                                                   In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .  \n",
              "187                                                                                                                                                                                                                                                                                                                                                                                                                                   In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .  \n",
              "188                                                                                                                                                                                                                                                                                                                                                                                                                                   In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .  \n",
              "189                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The wavelengths form a geometric progression from 2 to 10000 2 .  \n",
              "190                                                                                                                                                                                                                                                                                                                                                                                                                     We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "191                                                                                                                                                                                                                                                                                                                                                                                                                     We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "192                                                                                                                                                                                                                                                                                                                                                                                                                     We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "193                                                                                                                                                                                                                                                                                                                                                                                                                     We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "194                                                                                                                                                                                                                                                                                                                                                                                                                     We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .  \n",
              "195                                                                                                                                                                                                                                                                                                                                                                                                                                                                         We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .  \n",
              "196                                                                                                                                                                                                                                                                                                                                                                                                                                                                         We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .  \n",
              "197                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "198                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "199                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "200                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "201                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .  \n",
              "202                                                                                                                                                                                                                                                  4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "203                                                                                                                                                                                                                                                  4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "204                                                                                                                                                                                                                                                  4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "205                                                                                                                                                                                                                                                  4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "206                                                                                                                                                                                                                                                  4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "207                                                                                                                                                                                                                                                  4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "208                                                                                                                                                                                                                                                  4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "209                                                                                                                                                                                                                                                  4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "210                                                                                                                                                                                                                                                  4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .  \n",
              "211                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Motivating our use of self attention we consider three desiderata .  \n",
              "212                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Motivating our use of self attention we consider three desiderata .  \n",
              "213                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   One is the total computational complexity per layer .  \n",
              "214                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .  \n",
              "215                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .  \n",
              "216                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           The third is the path length between long range dependencies in the network .  \n",
              "217                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Learning long range dependencies is a key challenge in many sequence transduction tasks .  \n",
              "218                                                                                                                                                                                                                                                                                                                                                                                                                                                                               One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .  \n",
              "219                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long range dependencies 11 .  \n",
              "220                                                                                                                                                                                                                                                                                                                                                                                                                                         As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .  \n",
              "221                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "222                                                                                                                                                                                                                                                                            In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .  \n",
              "223                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "224                                                                                                                                                                                                                                                                                                                                                                                                 To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .  \n",
              "225                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           We plan to investigate this approach further in future work .  \n",
              "226                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           A single convolutional layer with kernel width k n does not connect all pairs of input and output positions .  \n",
              "227                                                                                                                                                                                                                                                                                                                                                                                                  Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .  \n",
              "228                                                                                                                                                                                                                                                                                                                                                                                                  Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .  \n",
              "229                                                                                                                                                                                                                                                                                                                                                                                                  Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .  \n",
              "230                                                                                                                                                                                                                                                                                                                                                                                                                                     Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .  \n",
              "231                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                As side benefit , self attention could yield more interpretable models .  \n",
              "232                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We inspect attention distributions from our models and present and discuss examples in the appendix .  \n",
              "233                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We inspect attention distributions from our models and present and discuss examples in the appendix .  \n",
              "234                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We inspect attention distributions from our models and present and discuss examples in the appendix .  \n",
              "235                                                                                                                                                                                                                                                                                                                                                                                                                                                    Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .  \n",
              "236                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .  \n",
              "237                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Sentences were encoded using byte pair encoding 3 , which has a shared sourcetarget vocabulary of about 37000 tokens .  \n",
              "238                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Sentences were encoded using byte pair encoding 3 , which has a shared sourcetarget vocabulary of about 37000 tokens .  \n",
              "239                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "240                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "241                                                                                                                                                                                                                                                                                                                                                                                                                                                              For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .  \n",
              "242                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Sentence pairs were batched together by approximate sequence length .  \n",
              "243                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .  \n",
              "244                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs .  \n",
              "245                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .  \n",
              "246                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .  \n",
              "247                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .  \n",
              "248                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We trained the base models for a total of 100,000 steps or 12 hours .  \n",
              "249                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                The big models were trained for 300,000 steps 3.5 days .  \n",
              "250                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              5.3 Optimizer We used the Adam optimizer 17 with 1 0.9 , 2 0.98 and 10 9 .  \n",
              "251                                                                                                                                                                                                                                                                                 We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .  \n",
              "252                                                                                                                                                                                                                                                                                 We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .  \n",
              "253                                                                                                                                                                                                                                                                                 We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .  \n",
              "254                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             We used warmup steps 4000 .  \n",
              "255                                                                                                                                                                                                                                                                                                                                                                                                                          5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .  \n",
              "256                                                                                                                                                                                                                                                                                                                                                                                                                          5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .  \n",
              "257                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .  \n",
              "258                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .  \n",
              "259                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .  \n",
              "260                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       For the base model , we use a rate of Pdrop 0.1 .  \n",
              "261                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       For the base model , we use a rate of Pdrop 0.1 .  \n",
              "262                                                                                              Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .  \n",
              "263                                                                                              Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .  \n",
              "264                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  This hurts perplexity , as the model learns to be more unsure , but improves accuracy and BLEU score .  \n",
              "265                                                                                                                                                                                                                                                                                                                                        6 Results 6.1 Machine Translation On the WMT 2014 English to German translation task , the big transformer model Transformer big in Table 2 outperforms the best previously reported models including ensembles by more than 2.0 BLEU , establishing a new state of the art BLEU score of 28.4 .  \n",
              "266                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The configuration of this model is listed in the bottom line of Table 3 .  \n",
              "267                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Training took 3.5 days on 8 P100 GPUs .  \n",
              "268                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Training took 3.5 days on 8 P100 GPUs .  \n",
              "269                                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .  \n",
              "270                                                                                                                                                                                                                                                                                                                                                                                            On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .  \n",
              "271                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .  \n",
              "272                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .  \n",
              "273                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              For the big models , we averaged the last 20 checkpoints .  \n",
              "274                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We used beam search with a beam size of 4 and length penalty 0.6 31 .  \n",
              "275                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        These hyperparameters were chosen after experimentation on the development set .  \n",
              "276                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "277                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "278                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .  \n",
              "279                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "280                                                                                                                                                                                                                                                                                                                                                                                                        We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .  \n",
              "281                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "282                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "283                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "284                                                                                                                                                                                                                                                                                                                                                                                  6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .  \n",
              "285                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                We used beam search as described in the previous section , but no checkpoint averaging .  \n",
              "286                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We present these results in Table 3 .  \n",
              "287                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   We present these results in Table 3 .  \n",
              "288                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "289                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "290                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "291                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "292                                                                                                                                                                                                                                                                                                                                                                                                                                                     In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .  \n",
              "293                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "294                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .  \n",
              "295                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .  \n",
              "296                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Unlisted values are identical to those of the base model .  \n",
              "297                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   All metrics are on the English to German translation development set , newstest2013 .  \n",
              "298                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Listed perplexities are per wordpiece , according to our byte pair encoding , and should not be compared to per word perplexities .  \n",
              "299   N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .  \n",
              "300                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .  \n",
              "301                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .  \n",
              "302                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .  \n",
              "303                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "304                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "305                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "306                                                                                                                                                                                                                                                                                                                                                                                    7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .  \n",
              "307                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .  \n",
              "308                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  In the former task our best model outperforms even all previously reported ensembles .  \n",
              "309                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We are excited about the future of attention based models and plan to apply them to other tasks .  \n",
              "310                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We are excited about the future of attention based models and plan to apply them to other tasks .  \n",
              "311                                                                                                                                                                                                                                                                                                                                                                                        We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .  \n",
              "312                                                                                                                                                                                                                                                                                                                                                                                        We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .  \n",
              "313                                                                                                                                                                                                                                                                                                                                                                                        We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .  \n",
              "314                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .  \n",
              "315                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .  \n",
              "316                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5e0fa12-c8cd-48b6-b136-25da0e9c2c7f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>subject</th>\n",
              "      <th>relation</th>\n",
              "      <th>object</th>\n",
              "      <th>subject_type</th>\n",
              "      <th>object_type</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>convolutional</td>\n",
              "      <td>include</td>\n",
              "      <td>encoder</td>\n",
              "      <td>Adjective</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>convolutional</td>\n",
              "      <td>include</td>\n",
              "      <td>decoder</td>\n",
              "      <td>Adjective</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Attention</td>\n",
              "      <td>is All</td>\n",
              "      <td>you Need</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Attention is All you Need Attention Is All You Need Ashish Vaswani Google Brain avaswani google.com Noam Shazeer Google Brain noam google.com Niki Parmar Google Research nikip google.com Jakob Uszkoreit Google Research usz google.com Llion Jones Google Research llion google.com Aidan N. Gomez University of Toronto aidan cs.toronto.edu ukasz Kaiser Google Brain lukaszkaiser google.com Illia Polosukhin illia.polosukhin gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>performing models</td>\n",
              "      <td>also connect decoder through</td>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>performing models</td>\n",
              "      <td>also connect encoder through</td>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>The best performing models also connect the encoder and decoder through an attention mechanism .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>Transformer</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Product</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>We</td>\n",
              "      <td>propose</td>\n",
              "      <td>new simple network architecture based solely on attention mechanisms dispensing with recurrence entirely</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We propose a new simple network architecture , the Transformer , based solely on attention mechanisms , dispensing with recurrence and convolutions entirely .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Our model</td>\n",
              "      <td>improving over</td>\n",
              "      <td>existing best results including ensembles by over 2 BLEU</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Our model</td>\n",
              "      <td>achieves</td>\n",
              "      <td>28.4 BLEU on WMT 2014 Englishto German translation task</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Our model achieves 28.4 BLEU on the WMT 2014 Englishto German translation task , improving over the existing best results , including ensembles , by over 2 BLEU .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>new single model state of art BLEU score of 41.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes model state after</td>\n",
              "      <td>training</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes</td>\n",
              "      <td>small fraction of training costs of best models from literature</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes model state On</td>\n",
              "      <td>WMT 2014 English to French translation task</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes fraction On</td>\n",
              "      <td>WMT 2014 English to French translation task</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>our model</td>\n",
              "      <td>establishes fraction after</td>\n",
              "      <td>training</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our model establishes a new single model state of the art BLEU score of 41.0 after training for 3.5 days on eight GPUs , a small fraction of the training costs of the best models from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1 Introduction Recurrent neural networks</td>\n",
              "      <td>memory in</td>\n",
              "      <td>particular</td>\n",
              "      <td>None</td>\n",
              "      <td>Adjective</td>\n",
              "      <td>1 Introduction Recurrent neural networks , long short term memory 12 and gated recurrent 7 neural networks in particular , have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation 29 , 2 , 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Numerous efforts</td>\n",
              "      <td>push</td>\n",
              "      <td>boundaries of recurrent language models</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Numerous efforts have since continued to push the boundaries of recurrent language models and encoder decoder architectures 31 , 21 , 13 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Listing order</td>\n",
              "      <td>is</td>\n",
              "      <td>random</td>\n",
              "      <td>None</td>\n",
              "      <td>Adjective</td>\n",
              "      <td>Listing order is random .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Jakob</td>\n",
              "      <td>replacing RNNs with</td>\n",
              "      <td>self attention</td>\n",
              "      <td>Person</td>\n",
              "      <td>None</td>\n",
              "      <td>Jakob proposed replacing RNNs with self attention and started the effort to evaluate this idea .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Ashish</td>\n",
              "      <td>is with</td>\n",
              "      <td>Illia designed</td>\n",
              "      <td>Person</td>\n",
              "      <td>None</td>\n",
              "      <td>Ashish , with Illia , designed and implemented the first Transformer models and has been crucially involved in every aspect of this work .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Noam</td>\n",
              "      <td>became</td>\n",
              "      <td>other person involved in nearly detail</td>\n",
              "      <td>Person</td>\n",
              "      <td>None</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Noam</td>\n",
              "      <td>proposed</td>\n",
              "      <td>multi head attention</td>\n",
              "      <td>Person</td>\n",
              "      <td>None</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Noam</td>\n",
              "      <td>proposed</td>\n",
              "      <td>scaled dot product attention</td>\n",
              "      <td>Person</td>\n",
              "      <td>None</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Noam</td>\n",
              "      <td>proposed</td>\n",
              "      <td>parameter free position representation</td>\n",
              "      <td>Person</td>\n",
              "      <td>None</td>\n",
              "      <td>Noam proposed scaled dot product attention , multi head attention and the parameter free position representation and became the other person involved in nearly every detail .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>evaluated countless model variants</td>\n",
              "      <td>is in</td>\n",
              "      <td>our original codebase</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Niki designed , implemented , tuned and evaluated countless model variants in our original codebase and tensor2tensor .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Llion</td>\n",
              "      <td>also experimented with</td>\n",
              "      <td>novel model variants</td>\n",
              "      <td>Organization</td>\n",
              "      <td>None</td>\n",
              "      <td>Llion also experimented with novel model variants , was responsible for our initial codebase , and efficient inference and visualizations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>replacing</td>\n",
              "      <td>our earlier codebase</td>\n",
              "      <td>Person</td>\n",
              "      <td>None</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>designing</td>\n",
              "      <td>various parts of tensor2tensor</td>\n",
              "      <td>Person</td>\n",
              "      <td>None</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Lukasz</td>\n",
              "      <td>spent at_time</td>\n",
              "      <td>countless long days</td>\n",
              "      <td>Person</td>\n",
              "      <td>None</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>results</td>\n",
              "      <td>massively accelerating</td>\n",
              "      <td>our research</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Aidan</td>\n",
              "      <td>spent at_time</td>\n",
              "      <td>countless long days</td>\n",
              "      <td>Person</td>\n",
              "      <td>None</td>\n",
              "      <td>Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor , replacing our earlier codebase , greatly improving results and massively accelerating our research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Work</td>\n",
              "      <td>performed at</td>\n",
              "      <td>Google Research</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Organization</td>\n",
              "      <td>Work performed while at Google Research .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>31st Conference</td>\n",
              "      <td>NIPS</td>\n",
              "      <td>Long Beach</td>\n",
              "      <td>None</td>\n",
              "      <td>Geo-political entity</td>\n",
              "      <td>31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>31st Conference</td>\n",
              "      <td>NIPS</td>\n",
              "      <td>2017</td>\n",
              "      <td>None</td>\n",
              "      <td>Date</td>\n",
              "      <td>31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>31st Conference</td>\n",
              "      <td>NIPS</td>\n",
              "      <td>CA</td>\n",
              "      <td>None</td>\n",
              "      <td>Geo-political entity</td>\n",
              "      <td>31st Conference on Neural Information Processing Systems NIPS 2017 , Long Beach , CA , USA .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Recurrent models</td>\n",
              "      <td>typically factor</td>\n",
              "      <td>computation along symbol positions of input sequences</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Recurrent models typically factor computation along the symbol positions of the input and output sequences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>they</td>\n",
              "      <td>Aligning positions to</td>\n",
              "      <td>steps in computation time</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>steps</td>\n",
              "      <td>is in</td>\n",
              "      <td>computation time</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Aligning the positions to steps in computation time , they generate a sequence of hidden states ht , as a function of the previous hidden state ht 1 and the input for position t .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>sequential nature</td>\n",
              "      <td>precludes</td>\n",
              "      <td>parallelization within training examples</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>sequential nature</td>\n",
              "      <td>precludes parallelization</td>\n",
              "      <td>memory constraints limit</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>memory constraints</td>\n",
              "      <td>limit</td>\n",
              "      <td>batching across examples</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>also improving model performance in case of</td>\n",
              "      <td>latter</td>\n",
              "      <td>None</td>\n",
              "      <td>Adjective</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>has achieved</td>\n",
              "      <td>significant improvements in computational efficiency</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>Recent work</td>\n",
              "      <td>has achieved improvements through</td>\n",
              "      <td>factorization tricks 18</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>significant improvements</td>\n",
              "      <td>is in</td>\n",
              "      <td>computational efficiency</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Recent work has achieved significant improvements in computational efficiency through factorization tricks 18 and conditional computation 26 , while also improving model performance in case of the latter .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>compelling sequence modeling</td>\n",
              "      <td>is in</td>\n",
              "      <td>various tasks</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>integral part of compelling sequence modeling in various tasks</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>allowing modeling without</td>\n",
              "      <td>regard to their distance in input sequences</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>allowing</td>\n",
              "      <td>modeling of dependencies</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>their distance</td>\n",
              "      <td>is in</td>\n",
              "      <td>input sequences</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Attention mechanisms</td>\n",
              "      <td>have become</td>\n",
              "      <td>allowing without regard to their distance in input sequences</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks , allowing modeling of dependencies without regard to their distance in the input or output sequences 2 , 16 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>conjunction</td>\n",
              "      <td>is with</td>\n",
              "      <td>recurrent network</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>such attention mechanisms</td>\n",
              "      <td>however are used in</td>\n",
              "      <td>conjunction with recurrent network</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>In all but a few cases 22 , however , such attention mechanisms are used in conjunction with a recurrent network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>we</td>\n",
              "      <td>propose model architecture In</td>\n",
              "      <td>work</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>we</td>\n",
              "      <td>propose</td>\n",
              "      <td>instead relying entirely</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>we</td>\n",
              "      <td>propose Transformer In</td>\n",
              "      <td>work</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>In this work we propose the Transformer , a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>reach state in</td>\n",
              "      <td>translation quality</td>\n",
              "      <td>Product</td>\n",
              "      <td>None</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>reach</td>\n",
              "      <td>new state of art</td>\n",
              "      <td>Product</td>\n",
              "      <td>None</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>reach state</td>\n",
              "      <td>trained for as little as twelve hours on eight P100 GPUs</td>\n",
              "      <td>Product</td>\n",
              "      <td>None</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>allows for</td>\n",
              "      <td>significantly more parallelization</td>\n",
              "      <td>Product</td>\n",
              "      <td>None</td>\n",
              "      <td>The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>all</td>\n",
              "      <td>use networks as</td>\n",
              "      <td>basic building block</td>\n",
              "      <td>Determiner</td>\n",
              "      <td>None</td>\n",
              "      <td>2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>all</td>\n",
              "      <td>use</td>\n",
              "      <td>convolutional neural networks</td>\n",
              "      <td>Determiner</td>\n",
              "      <td>None</td>\n",
              "      <td>2 Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU 20 , ByteNet 15 and ConvS2S 8 , all of which use convolutional neural networks as basic building block , computing hidden representations in parallel for all input and output positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>number</td>\n",
              "      <td>grows In</td>\n",
              "      <td>models</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>number</td>\n",
              "      <td>grows in</td>\n",
              "      <td>distance between positions</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>In these models , the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions , linearly for ConvS2S and logarithmically for ByteNet .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>it</td>\n",
              "      <td>learn</td>\n",
              "      <td>dependencies between distant positions 11</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>This makes it more difficult to learn dependencies between distant positions 11 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65</th>\n",
              "      <td>Self attention</td>\n",
              "      <td>is</td>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>66</th>\n",
              "      <td>attention mechanism</td>\n",
              "      <td>relating</td>\n",
              "      <td>different positions of single sequence</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Self attention , sometimes called intra attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>Self attention</td>\n",
              "      <td>has</td>\n",
              "      <td>has used successfully</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Self attention has been used successfully in a variety of tasks including reading comprehension , abstractive summarization , textual entailment and learning task independent sentence representations 4 , 22 , 23 , 19 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>is</td>\n",
              "      <td>first transduction model relying entirely on self attention</td>\n",
              "      <td>Product</td>\n",
              "      <td>None</td>\n",
              "      <td>To the best of our knowledge , however , the Transformer is the first transduction model relying entirely on self attention to compute representations of its input and output without using sequencealigned RNNs or convolution .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>we</td>\n",
              "      <td>discuss</td>\n",
              "      <td>its advantages</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>we</td>\n",
              "      <td>will describe Transformer In</td>\n",
              "      <td>following sections</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>we</td>\n",
              "      <td>motivate</td>\n",
              "      <td>self attention</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In the following sections , we will describe the Transformer , motivate self attention and discuss its advantages over models such as 14 , 15 and 8 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>encoder</td>\n",
              "      <td>Here maps</td>\n",
              "      <td>input sequence of symbol representations x1</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>encoder</td>\n",
              "      <td>maps</td>\n",
              "      <td>xn to sequence of continuous representations</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Here , the encoder maps an input sequence of symbol representations x1 , ... , xn to a sequence of continuous representations z z1 , ... , zn .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>decoder</td>\n",
              "      <td>generates</td>\n",
              "      <td>ym of symbols one element at time</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>decoder</td>\n",
              "      <td>generates</td>\n",
              "      <td>output sequence y1</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>76</th>\n",
              "      <td>output sequence y1</td>\n",
              "      <td>ym at</td>\n",
              "      <td>time</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Given z , the decoder then generates an output sequence y1 , ... , ym of symbols one element at a time .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>77</th>\n",
              "      <td>model</td>\n",
              "      <td>consuming symbols as</td>\n",
              "      <td>additional input</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>78</th>\n",
              "      <td>model</td>\n",
              "      <td>consuming</td>\n",
              "      <td>previously generated symbols</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>At each step the model is auto regressive 9 , consuming the previously generated symbols as additional input when generating the next .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>overall architecture</td>\n",
              "      <td>shown in</td>\n",
              "      <td>left halves Figure 1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>follows respectively</td>\n",
              "      <td>overall architecture</td>\n",
              "      <td>Product</td>\n",
              "      <td>None</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>overall architecture</td>\n",
              "      <td>using</td>\n",
              "      <td>stacked self attention</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>point</td>\n",
              "      <td>wise</td>\n",
              "      <td>fully connected layers for encoder</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>overall architecture</td>\n",
              "      <td>using</td>\n",
              "      <td>point wise</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>The Transformer follows this overall architecture using stacked self attention and point wise , fully connected layers for both the encoder and decoder , shown in the left and right halves of Figure 1 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>layer</td>\n",
              "      <td>has</td>\n",
              "      <td>two sub layers</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Each layer has two sub layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>first</td>\n",
              "      <td>is</td>\n",
              "      <td>multi head self attention mechanism</td>\n",
              "      <td>Ordinal</td>\n",
              "      <td>None</td>\n",
              "      <td>The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>second</td>\n",
              "      <td>is</td>\n",
              "      <td>simple position2 Figure 1</td>\n",
              "      <td>Ordinal</td>\n",
              "      <td>None</td>\n",
              "      <td>The first is a multi head self attention mechanism , and the second is a simple , position2 Figure 1 : The Transformer model architecture .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>We</td>\n",
              "      <td>followed by</td>\n",
              "      <td>layer normalization 1</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>We</td>\n",
              "      <td>employ</td>\n",
              "      <td>residual connection 10 around each of two sub layers</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We employ a residual connection 10 around each of the two sub layers , followed by layer normalization 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89</th>\n",
              "      <td>Sublayer x</td>\n",
              "      <td>is</td>\n",
              "      <td>where function implemented by sub layer itself</td>\n",
              "      <td>Organization</td>\n",
              "      <td>None</td>\n",
              "      <td>That is , the output of each sub layer is LayerNorm x Sublayer x , where Sublayer x is the function implemented by the sub layer itself .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>90</th>\n",
              "      <td>decoder</td>\n",
              "      <td>is also composed of</td>\n",
              "      <td>stack of N 6 identical layers</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Decoder : The decoder is also composed of a stack of N 6 identical layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>Decoder</td>\n",
              "      <td>composed of</td>\n",
              "      <td>stack</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Decoder : The decoder is also composed of a stack of N 6 identical layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>two sub layers</td>\n",
              "      <td>is in</td>\n",
              "      <td>encoder layer</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>In addition to the two sub layers in each encoder layer , the decoder inserts a third sub layer , which performs multi head attention over the output of the encoder stack .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>93</th>\n",
              "      <td>we</td>\n",
              "      <td>Similar employ</td>\n",
              "      <td>residual connections around each of sub layers</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>we</td>\n",
              "      <td>followed by</td>\n",
              "      <td>layer normalization</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>Similar to the encoder , we employ residual connections around each of the sub layers , followed by layer normalization .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>self attention sub layer</td>\n",
              "      <td>is in</td>\n",
              "      <td>decoder</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>self attention sub layer</td>\n",
              "      <td>prevent positions</td>\n",
              "      <td>attending to subsequent positions</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>We also modify the self attention sub layer in the decoder stack to prevent positions from attending to subsequent positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>predictions</td>\n",
              "      <td>depend on</td>\n",
              "      <td>outputs at positions less than i. 3.2 Attention</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>output embeddings</td>\n",
              "      <td>are offset by</td>\n",
              "      <td>one position</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>masking</td>\n",
              "      <td>combined with</td>\n",
              "      <td>fact</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>This masking , combined with fact that the output embeddings are offset by one position , ensures that the predictions for position i can depend only on the known outputs at positions less than i. 3.2 Attention An attention function can be described as mapping a query and a set of key value pairs to an output , where the query , keys , values , and output are all vectors .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>output</td>\n",
              "      <td>is computed as</td>\n",
              "      <td>weighted sum of values</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>weight</td>\n",
              "      <td>assigned to</td>\n",
              "      <td>value</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>102</th>\n",
              "      <td>weight</td>\n",
              "      <td>is</td>\n",
              "      <td>where computed by compatibility function of query with corresponding key</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>The output is computed as a weighted sum of the values , where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>our particular attention</td>\n",
              "      <td>Scaled</td>\n",
              "      <td>Dot Product Attention Figure 2</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>3.2.1 Scaled Dot Product Attention We call our particular attention Scaled Dot Product Attention Figure 2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>input</td>\n",
              "      <td>consists of</td>\n",
              "      <td>queries of dimension dk</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>The input consists of queries and keys of dimension dk , and values of dimension dv .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>105</th>\n",
              "      <td>input</td>\n",
              "      <td>values of</td>\n",
              "      <td>dimension dv</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>The input consists of queries and keys of dimension dk , and values of dimension dv .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106</th>\n",
              "      <td>Multi Head Attention</td>\n",
              "      <td>consists of</td>\n",
              "      <td>several attention layers running in parallel</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>right Multi Head Attention consists of several attention layers running in parallel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>query</td>\n",
              "      <td>is with</td>\n",
              "      <td>keys</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>query with all keys , divide each by dk , and apply a softmax function to obtain the weights on the values .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>108</th>\n",
              "      <td>we</td>\n",
              "      <td>compute attention function In</td>\n",
              "      <td>practice</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>In practice , we compute the attention function on a set of queries simultaneously , packed together into a matrix Q .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>We</td>\n",
              "      <td>Attention</td>\n",
              "      <td>V softmax QKT dk V 1</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>110</th>\n",
              "      <td>We</td>\n",
              "      <td>Attention</td>\n",
              "      <td>dot product multiplicative attention</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We compute the matrix of outputs as : Attention Q , K , V softmax QKT dk V 1 The two most commonly used attention functions are additive attention 2 , and dot product multiplicative attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111</th>\n",
              "      <td>compatibility function</td>\n",
              "      <td>using</td>\n",
              "      <td>feed</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Additive attention computes the compatibility function using a feed forward network with a single hidden layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>112</th>\n",
              "      <td>Additive attention</td>\n",
              "      <td>computes</td>\n",
              "      <td>compatibility function</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Additive attention computes the compatibility function using a feed forward network with a single hidden layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>forward network</td>\n",
              "      <td>is with</td>\n",
              "      <td>single hidden layer</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Additive attention computes the compatibility function using a feed forward network with a single hidden layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>it</td>\n",
              "      <td>can</td>\n",
              "      <td>can implemented</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>it</td>\n",
              "      <td>using</td>\n",
              "      <td>highly optimized matrix multiplication code</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>two</td>\n",
              "      <td>are similar in</td>\n",
              "      <td>theoretical complexity</td>\n",
              "      <td>Cardinal</td>\n",
              "      <td>None</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>dot product attention</td>\n",
              "      <td>is much faster</td>\n",
              "      <td>similar in theoretical complexity</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>dot product attention</td>\n",
              "      <td>is much faster</td>\n",
              "      <td>can implemented</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>While the two are similar in theoretical complexity , dot product attention is much faster and more space efficient in practice , since it can be implemented using highly optimized matrix multiplication code .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>for small values of dk two mechanisms perform similarly</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>additive attention</td>\n",
              "      <td>outperforms dot product attention</td>\n",
              "      <td>scaling for larger values of dk 3</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>While for small values of dk the two mechanisms perform similarly , additive attention outperforms dot product attention without scaling for larger values of dk 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>121</th>\n",
              "      <td>dot products</td>\n",
              "      <td>grow in</td>\n",
              "      <td>magnitude</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122</th>\n",
              "      <td>dot products</td>\n",
              "      <td>pushing softmax function into</td>\n",
              "      <td>regions</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>it</td>\n",
              "      <td>has</td>\n",
              "      <td>extremely small gradients 4</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>dot products</td>\n",
              "      <td>grow for</td>\n",
              "      <td>large values of dk</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>We suspect that for large values of dk , the dot products grow large in magnitude , pushing the softmax function into regions where it has extremely small gradients 4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>125</th>\n",
              "      <td>we</td>\n",
              "      <td>scale dot products by</td>\n",
              "      <td>1 dk</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Quantity</td>\n",
              "      <td>To counteract this effect , we scale the dot products by 1 dk .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>126</th>\n",
              "      <td>we</td>\n",
              "      <td>counteract</td>\n",
              "      <td>effect</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>To counteract this effect , we scale the dot products by 1 dk .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>127</th>\n",
              "      <td>queries</td>\n",
              "      <td>learned respectively</td>\n",
              "      <td>linear projections</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>128</th>\n",
              "      <td>it</td>\n",
              "      <td>linearly project values h times with</td>\n",
              "      <td>different</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Adjective</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>129</th>\n",
              "      <td>it</td>\n",
              "      <td>learned respectively</td>\n",
              "      <td>linear projections</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130</th>\n",
              "      <td>it</td>\n",
              "      <td>learned projections respectively to</td>\n",
              "      <td>dk</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Foreign word</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>131</th>\n",
              "      <td>it</td>\n",
              "      <td>linearly project keys with</td>\n",
              "      <td>different</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Adjective</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>132</th>\n",
              "      <td>it</td>\n",
              "      <td>linearly project queries with</td>\n",
              "      <td>different</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Adjective</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>133</th>\n",
              "      <td>queries</td>\n",
              "      <td>learned projections respectively to</td>\n",
              "      <td>dk</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Foreign word</td>\n",
              "      <td>3.2.2 Multi Head Attention Instead of performing a single attention function with dmodel dimensional keys , values and queries , we found it beneficial to linearly project the queries , keys and values h times with different , learned linear projections to dk , dk and dv dimensions , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>we</td>\n",
              "      <td>yielding</td>\n",
              "      <td>dv dimensional output values</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>135</th>\n",
              "      <td>we</td>\n",
              "      <td>perform attention function in</td>\n",
              "      <td>parallel</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>On each of these projected versions of queries , keys and values we then perform the attention function in parallel , yielding dv dimensional output values .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>136</th>\n",
              "      <td>These</td>\n",
              "      <td>depicted in</td>\n",
              "      <td>Figure 2</td>\n",
              "      <td>Determiner</td>\n",
              "      <td>None</td>\n",
              "      <td>These are concatenated and once again projected , resulting in the final values , as depicted in Figure 2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>137</th>\n",
              "      <td>averaging</td>\n",
              "      <td>inhibits With</td>\n",
              "      <td>single attention head</td>\n",
              "      <td>Verb</td>\n",
              "      <td>None</td>\n",
              "      <td>With a single attention head , averaging inhibits this .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>138</th>\n",
              "      <td>q</td>\n",
              "      <td>components of are</td>\n",
              "      <td>independent random variables</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>139</th>\n",
              "      <td>dot products</td>\n",
              "      <td>get</td>\n",
              "      <td>large</td>\n",
              "      <td>None</td>\n",
              "      <td>Adjective</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>140</th>\n",
              "      <td>independent random variables</td>\n",
              "      <td>is with</td>\n",
              "      <td>mean 0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>4To</td>\n",
              "      <td>illustrate</td>\n",
              "      <td>assume</td>\n",
              "      <td>Cardinal</td>\n",
              "      <td>Verb</td>\n",
              "      <td>4To illustrate why the dot products get large , assume that the components of q and k are independent random variables with mean 0 and variance 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>their</td>\n",
              "      <td>product</td>\n",
              "      <td>q k dk i 1 qiki</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>Then their dot product , q k dk i 1 qiki , has mean 0 and variance dk .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>we</td>\n",
              "      <td>employ heads In</td>\n",
              "      <td>work</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>In this work we employ h 8 parallel attention layers , or heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>we</td>\n",
              "      <td>employ</td>\n",
              "      <td>h 8 parallel attention layers</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In this work we employ h 8 parallel attention layers , or heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>we</td>\n",
              "      <td>use</td>\n",
              "      <td>dk dv dmodel h 64</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>For each of these we use dk dv dmodel h 64 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>total computational cost</td>\n",
              "      <td>is</td>\n",
              "      <td>Due to reduced dimension of head similar to that of single head attention with full dimensionality</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Due to the reduced dimension of each head , the total computational cost is similar to that of single head attention with full dimensionality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>encoder</td>\n",
              "      <td>contains</td>\n",
              "      <td>self attention layers</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>The encoder contains self attention layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>output</td>\n",
              "      <td>is in</td>\n",
              "      <td>encoder</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>In a self attention layer all of the keys , values and queries come from the same place , in this case , the output of the previous layer in the encoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>Similarly allow</td>\n",
              "      <td>position in decoder</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>150</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>is in</td>\n",
              "      <td>decoder</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Similarly , self attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>151</th>\n",
              "      <td>We</td>\n",
              "      <td>prevent</td>\n",
              "      <td>leftward information flow in decoder</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We need to prevent leftward information flow in the decoder to preserve the auto regressive property .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>leftward information flow</td>\n",
              "      <td>is in</td>\n",
              "      <td>decoder</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>We need to prevent leftward information flow in the decoder to preserve the auto regressive property .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>We</td>\n",
              "      <td>implement</td>\n",
              "      <td>inside of scaled dot product attention</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We implement this inside of scaled dot product attention by masking out setting to all values in the input of the softmax which correspond to illegal connections .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>two linear transformations</td>\n",
              "      <td>is with</td>\n",
              "      <td>ReLU activation in</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>This consists of two linear transformations with a ReLU activation in between .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>linear transformations</td>\n",
              "      <td>are same across</td>\n",
              "      <td>different positions</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>they</td>\n",
              "      <td>FFN use</td>\n",
              "      <td>different parameters</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>they</td>\n",
              "      <td>FFN use parameters from</td>\n",
              "      <td>layer to layer</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>FFN x max 0 , xW1 b1 W2 b2 2 While the linear transformations are the same across different positions , they use different parameters from layer to layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>two convolutions</td>\n",
              "      <td>is with</td>\n",
              "      <td>kernel size 1</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Another way of describing this is as two convolutions with kernel size 1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>inner layer</td>\n",
              "      <td>has</td>\n",
              "      <td>dimensionality dff 2048</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>dimensionality</td>\n",
              "      <td>is</td>\n",
              "      <td>dmodel 512</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>161</th>\n",
              "      <td>we</td>\n",
              "      <td>convert input tokens to</td>\n",
              "      <td>vectors of dimension dmodel</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>we</td>\n",
              "      <td>convert output tokens to</td>\n",
              "      <td>vectors of dimension dmodel</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>we</td>\n",
              "      <td>use</td>\n",
              "      <td>learned embeddings</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>3.4 Embeddings and Softmax Similarly to other sequence transduction models , we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>We</td>\n",
              "      <td>also use</td>\n",
              "      <td>the usual</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next token probabilities .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165</th>\n",
              "      <td>we</td>\n",
              "      <td>share</td>\n",
              "      <td>same weight matrix between two embedding layers similar to 24</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>we</td>\n",
              "      <td>share weight matrix In</td>\n",
              "      <td>our model</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In our model , we share the same weight matrix between the two embedding layers and the pre softmax linear transformation , similar to 24 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>we</td>\n",
              "      <td>multiply weights In</td>\n",
              "      <td>embedding layers</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In the embedding layers , we multiply those weights by dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>we</td>\n",
              "      <td>multiply weights by</td>\n",
              "      <td>dmodel</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>In the embedding layers , we multiply those weights by dmodel .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>3.5 Positional Encoding</td>\n",
              "      <td>must inject</td>\n",
              "      <td>information about relative position of tokens in sequence</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>model</td>\n",
              "      <td>make</td>\n",
              "      <td>use of order of sequence</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>relative position</td>\n",
              "      <td>is in</td>\n",
              "      <td>sequence</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>3.5 Positional Encoding Since our model contains no recurrence and no convolution , in order for the model to make use of the order of the sequence , we must inject some information about the relative or absolute position of the tokens in the sequence .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>we</td>\n",
              "      <td>add encodings to</td>\n",
              "      <td>input embeddings</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>we</td>\n",
              "      <td>add encodings To</td>\n",
              "      <td>end</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>we</td>\n",
              "      <td>add encodings at</td>\n",
              "      <td>5 Table 1</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>we</td>\n",
              "      <td>add</td>\n",
              "      <td>positional encodings</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>To this end , we add positional encodings to the input embeddings at the 5 Table 1 : Maximum path lengths , per layer complexity and minimum number of sequential operations for different layer types .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>k</td>\n",
              "      <td>size in</td>\n",
              "      <td>restricted self attention</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>size</td>\n",
              "      <td>is in</td>\n",
              "      <td>restricted self attention</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>k</td>\n",
              "      <td>is</td>\n",
              "      <td>kernel size of convolutions</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>k</td>\n",
              "      <td>size of</td>\n",
              "      <td>neighborhood</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>d</td>\n",
              "      <td>is</td>\n",
              "      <td>representation dimension</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>n</td>\n",
              "      <td>is</td>\n",
              "      <td>sequence length</td>\n",
              "      <td>Punctuation</td>\n",
              "      <td>None</td>\n",
              "      <td>n is the sequence length , d is the representation dimension , k is the kernel size of convolutions and r the size of the neighborhood in restricted self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>two</td>\n",
              "      <td>can</td>\n",
              "      <td>can summed</td>\n",
              "      <td>Cardinal</td>\n",
              "      <td>None</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>positional encodings</td>\n",
              "      <td>have</td>\n",
              "      <td>same dimension dmodel as embeddings</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>positional encodings</td>\n",
              "      <td>have dimension dmodel</td>\n",
              "      <td>can summed</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>The positional encodings have the same dimension dmodel as the embeddings , so that the two can be summed .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>i</td>\n",
              "      <td>is</td>\n",
              "      <td>dimension</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>we</td>\n",
              "      <td>use</td>\n",
              "      <td>sine functions of different frequencies</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>pos</td>\n",
              "      <td>is</td>\n",
              "      <td>where position</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>we</td>\n",
              "      <td>use sine functions In</td>\n",
              "      <td>work</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>In this work , we use sine and cosine functions of different frequencies : PE pos ,2 i sin pos 100002i dmodel PE pos ,2 i 1 cos pos 100002i dmodel where pos is the position and i is the dimension .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>wavelengths</td>\n",
              "      <td>form</td>\n",
              "      <td>geometric progression</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>The wavelengths form a geometric progression from 2 to 10000 2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>it</td>\n",
              "      <td>allow</td>\n",
              "      <td>easily learn</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>it</td>\n",
              "      <td>allow</td>\n",
              "      <td>model</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>PEpos k</td>\n",
              "      <td>can</td>\n",
              "      <td>since for fixed offset k can represented as linear function of PEpos</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>We</td>\n",
              "      <td>chose function</td>\n",
              "      <td>we hypothesized</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>model</td>\n",
              "      <td>easily learn</td>\n",
              "      <td>attend by relative positions</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions , since for any fixed offset k , PEpos k can be represented as a linear function of PEpos .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>We</td>\n",
              "      <td>also experimented</td>\n",
              "      <td>using</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Verb</td>\n",
              "      <td>We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>identical results</td>\n",
              "      <td>see</td>\n",
              "      <td>Table 3 row E</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>We also experimented with using learned positional embeddings 8 instead , and found that the two versions produced nearly identical results see Table 3 row E .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>We</td>\n",
              "      <td>chose</td>\n",
              "      <td>sinusoidal version</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>it</td>\n",
              "      <td>may allow</td>\n",
              "      <td>model</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>We</td>\n",
              "      <td>chose version</td>\n",
              "      <td>it may allow</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>it</td>\n",
              "      <td>may allow</td>\n",
              "      <td>extrapolate to sequence lengths longer than ones encountered during training</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>model</td>\n",
              "      <td>extrapolate to</td>\n",
              "      <td>sequence lengths longer than ones encountered during training</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>4 Self Attention</td>\n",
              "      <td>zn with</td>\n",
              "      <td>xi</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>we</td>\n",
              "      <td>compare</td>\n",
              "      <td>various aspects of self attention layers</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>section</td>\n",
              "      <td>In Attention is</td>\n",
              "      <td>zn with xi such hidden layer</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>205</th>\n",
              "      <td>we</td>\n",
              "      <td>compare aspects to</td>\n",
              "      <td>recurrent layers commonly used</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>206</th>\n",
              "      <td>hidden layer</td>\n",
              "      <td>is in</td>\n",
              "      <td>typical sequence transduction encoder</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207</th>\n",
              "      <td>section</td>\n",
              "      <td>In Attention is</td>\n",
              "      <td>xn to sequence of equal length z1</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>208</th>\n",
              "      <td>4 Self Attention</td>\n",
              "      <td>zn such as</td>\n",
              "      <td>hidden layer in typical sequence transduction encoder</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>hidden layer</td>\n",
              "      <td>such as zn is</td>\n",
              "      <td>zi Rd</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>210</th>\n",
              "      <td>4 Self Attention</td>\n",
              "      <td>is In</td>\n",
              "      <td>section</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>4 Why Self Attention In this section we compare various aspects of self attention layers to the recurrent and convolutional layers commonly used for mapping one variable length sequence of symbol representations x1 , ... , xn to another sequence of equal length z1 , ... , zn , with xi , zi Rd , such as a hidden layer in a typical sequence transduction encoder or decoder .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>we</td>\n",
              "      <td>Motivating</td>\n",
              "      <td>our use of self attention</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>Motivating our use of self attention we consider three desiderata .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>212</th>\n",
              "      <td>we</td>\n",
              "      <td>consider</td>\n",
              "      <td>three desiderata</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>Motivating our use of self attention we consider three desiderata .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>213</th>\n",
              "      <td>One</td>\n",
              "      <td>is total computational complexity per</td>\n",
              "      <td>layer</td>\n",
              "      <td>Cardinal</td>\n",
              "      <td>Noun</td>\n",
              "      <td>One is the total computational complexity per layer .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>214</th>\n",
              "      <td>Another</td>\n",
              "      <td>is amount of</td>\n",
              "      <td>computation</td>\n",
              "      <td>Determiner</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>Another</td>\n",
              "      <td>is amount</td>\n",
              "      <td>can parallelized</td>\n",
              "      <td>Determiner</td>\n",
              "      <td>None</td>\n",
              "      <td>Another is the amount of computation that can be parallelized , as measured by the minimum number of sequential operations required .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>third</td>\n",
              "      <td>is</td>\n",
              "      <td>path length between long range dependencies in network</td>\n",
              "      <td>Ordinal</td>\n",
              "      <td>None</td>\n",
              "      <td>The third is the path length between long range dependencies in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>dependencies</td>\n",
              "      <td>is key challenge in</td>\n",
              "      <td>many sequence transduction tasks</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Learning long range dependencies is a key challenge in many sequence transduction tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>218</th>\n",
              "      <td>forward signals</td>\n",
              "      <td>have</td>\n",
              "      <td>traverse in network</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219</th>\n",
              "      <td>it</td>\n",
              "      <td>learn</td>\n",
              "      <td>long range dependencies 11</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>The shorter these paths between any combination of positions in the input and output sequences , the easier it is to learn long range dependencies 11 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>220</th>\n",
              "      <td>self attention layer</td>\n",
              "      <td>connects positions with</td>\n",
              "      <td>constant number of sequentially executed operations</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>As noted in Table 1 , a self attention layer connects all positions with a constant number of sequentially executed operations , whereas a recurrent layer requires O n sequential operations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>221</th>\n",
              "      <td>case</td>\n",
              "      <td>is with</td>\n",
              "      <td>sentence representations used by state of art models in machine translations such word piece 31</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>222</th>\n",
              "      <td>self attention layers</td>\n",
              "      <td>are</td>\n",
              "      <td>In terms of computational complexity faster than recurrent layers</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>In terms of computational complexity , self attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d , which is most often the case with sentence representations used by state of the art models in machine translations , such as word piece 31 and byte pair 25 representations .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>223</th>\n",
              "      <td>self attention</td>\n",
              "      <td>could</td>\n",
              "      <td>could restricted</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>224</th>\n",
              "      <td>self attention</td>\n",
              "      <td>improve</td>\n",
              "      <td>computational performance for tasks</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>To improve computational performance for tasks involving very long sequences , self attention could be restricted to considering only a neighborhood of size r in 6 the input sequence centered around the respective output position .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>225</th>\n",
              "      <td>We</td>\n",
              "      <td>investigate approach further in</td>\n",
              "      <td>future work</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We plan to investigate this approach further in future work .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>226</th>\n",
              "      <td>single convolutional layer</td>\n",
              "      <td>is with</td>\n",
              "      <td>kernel width k n</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>A single convolutional layer with kernel width k n does not connect all pairs of input and output positions .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>stack</td>\n",
              "      <td>increasing</td>\n",
              "      <td>length of longest paths between two positions in network</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>228</th>\n",
              "      <td>case</td>\n",
              "      <td>is in</td>\n",
              "      <td>case of dilated convolutions 15</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>229</th>\n",
              "      <td>Doing</td>\n",
              "      <td>requires</td>\n",
              "      <td>stack of O n k convolutional layers</td>\n",
              "      <td>Verb</td>\n",
              "      <td>None</td>\n",
              "      <td>Doing so requires a stack of O n k convolutional layers in the case of contiguous kernels , or O logk n in the case of dilated convolutions 15 , increasing the length of the longest paths between any two positions in the network .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>230</th>\n",
              "      <td>complexity</td>\n",
              "      <td>is</td>\n",
              "      <td>Even with k n however equal to combination of self attention layer</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Even with k n , however , the complexity of a separable convolution is equal to the combination of a self attention layer and a point wise feed forward layer , the approach we take in our model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>231</th>\n",
              "      <td>self attention</td>\n",
              "      <td>could yield</td>\n",
              "      <td>more interpretable models</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>As side benefit , self attention could yield more interpretable models .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>232</th>\n",
              "      <td>We</td>\n",
              "      <td>inspect attention distributions from</td>\n",
              "      <td>our models</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We inspect attention distributions from our models and present and discuss examples in the appendix .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>233</th>\n",
              "      <td>examples</td>\n",
              "      <td>is in</td>\n",
              "      <td>appendix</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>We inspect attention distributions from our models and present and discuss examples in the appendix .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>234</th>\n",
              "      <td>We</td>\n",
              "      <td>discuss</td>\n",
              "      <td>examples in appendix</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We inspect attention distributions from our models and present and discuss examples in the appendix .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235</th>\n",
              "      <td>many</td>\n",
              "      <td>exhibit</td>\n",
              "      <td>behavior related to syntactic structure of sentences</td>\n",
              "      <td>Adjective</td>\n",
              "      <td>None</td>\n",
              "      <td>Not only do individual attention heads clearly learn to perform different tasks , many appear to exhibit behavior related to the syntactic and semantic structure of the sentences .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>236</th>\n",
              "      <td>We</td>\n",
              "      <td>trained on</td>\n",
              "      <td>standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>5.1 Training Data and Batching We trained on the standard WMT 2014 English German dataset consisting of about 4.5 million sentence pairs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>237</th>\n",
              "      <td>Sentences</td>\n",
              "      <td>using</td>\n",
              "      <td>byte pair</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Sentences were encoded using byte pair encoding 3 , which has a shared sourcetarget vocabulary of about 37000 tokens .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>Sentences</td>\n",
              "      <td>were</td>\n",
              "      <td>encoded</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Verb</td>\n",
              "      <td>Sentences were encoded using byte pair encoding 3 , which has a shared sourcetarget vocabulary of about 37000 tokens .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>we</td>\n",
              "      <td>used For</td>\n",
              "      <td>English French</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>significantly larger WMT 2014 English French dataset consisting of 36M sentences</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>we</td>\n",
              "      <td>used into</td>\n",
              "      <td>32000 word piece vocabulary 31</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>For English French , we used the significantly larger WMT 2014 English French dataset consisting of 36M sentences and split tokens into a 32000 word piece vocabulary 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>Sentence pairs</td>\n",
              "      <td>were batched together by</td>\n",
              "      <td>approximate sequence length</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Sentence pairs were batched together by approximate sequence length .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>training batch</td>\n",
              "      <td>contained</td>\n",
              "      <td>set of sentence pairs</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>244</th>\n",
              "      <td>We</td>\n",
              "      <td>trained</td>\n",
              "      <td>our models</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>5.2 Hardware and Schedule We trained our models on one machine with 8 NVIDIA P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>training step</td>\n",
              "      <td>took</td>\n",
              "      <td>about 0.4 seconds</td>\n",
              "      <td>None</td>\n",
              "      <td>Time</td>\n",
              "      <td>For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>our base models</td>\n",
              "      <td>described throughout</td>\n",
              "      <td>paper</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>our base models</td>\n",
              "      <td>using</td>\n",
              "      <td>hyperparameters</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>For our base models using the hyperparameters described throughout the paper , each training step took about 0.4 seconds .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>We</td>\n",
              "      <td>trained</td>\n",
              "      <td>base models</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We trained the base models for a total of 100,000 steps or 12 hours .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>big models</td>\n",
              "      <td>were trained for</td>\n",
              "      <td>300,000 steps 3.5 days</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>The big models were trained for 300,000 steps 3.5 days .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>250</th>\n",
              "      <td>We</td>\n",
              "      <td>used</td>\n",
              "      <td>Adam optimizer 17</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>5.3 Optimizer We used the Adam optimizer 17 with 1 0.9 , 2 0.98 and 10 9 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>251</th>\n",
              "      <td>first warmup steps</td>\n",
              "      <td>training</td>\n",
              "      <td>steps</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>252</th>\n",
              "      <td>We</td>\n",
              "      <td>varied</td>\n",
              "      <td>learning rate</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>253</th>\n",
              "      <td>learning rate</td>\n",
              "      <td>decreasing thereafter proportionally</td>\n",
              "      <td>it</td>\n",
              "      <td>None</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>We varied the learning rate over the course of training , according to the formula : lrate d 0.5 model min step num 0.5 , step num warmup steps 1.5 3 This corresponds to increasing the learning rate linearly for the first warmup steps training steps , and decreasing it thereafter proportionally to the inverse square root of the step number .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>We</td>\n",
              "      <td>used</td>\n",
              "      <td>warmup steps 4000</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We used warmup steps 4000 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>We</td>\n",
              "      <td>employ</td>\n",
              "      <td>three types of regularization</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>We</td>\n",
              "      <td>apply</td>\n",
              "      <td>dropout 27</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>5.4 Regularization We employ three types of regularization during training : Residual Dropout We apply dropout 27 to the output of each sub layer , before it is added to the sub layer input and normalized .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>sums</td>\n",
              "      <td>is in</td>\n",
              "      <td>encoder</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>we</td>\n",
              "      <td>apply dropout In</td>\n",
              "      <td>addition</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>259</th>\n",
              "      <td>we</td>\n",
              "      <td>apply dropout to</td>\n",
              "      <td>sums of embeddings in encoder</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>we</td>\n",
              "      <td>use</td>\n",
              "      <td>rate of Pdrop 0.1</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>For the base model , we use a rate of Pdrop 0.1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>we</td>\n",
              "      <td>use rate For</td>\n",
              "      <td>base model</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>For the base model , we use a rate of Pdrop 0.1 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>we</td>\n",
              "      <td>employed</td>\n",
              "      <td>label smoothing of value ls 0.1 30</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>Model BLEU</td>\n",
              "      <td>Training</td>\n",
              "      <td>Cost FLOPs EN</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Model BLEU Training Cost FLOPs EN DE EN FR EN DE EN FR ByteNet 15 23.75 Deep Att PosUnk 32 39.2 1.0 1020 GNMT RL 31 24.6 39.92 2.3 1019 1.4 1020 ConvS2S 8 25.16 40.46 9.6 1018 1.5 1020 MoE 26 26.03 40.56 2.0 1019 1.2 1020 Deep Att PosUnk Ensemble 32 40.4 8.0 1020 GNMT RL Ensemble 31 26.30 41.16 1.8 1020 1.1 1021 ConvS2S Ensemble 8 26.36 41.29 7.7 1019 1.2 1021 Transformer base model 27.3 38.1 3.3 1018 Transformer big 28.4 41.0 2.3 1019 Label Smoothing During training , we employed label smoothing of value ls 0.1 30 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>model</td>\n",
              "      <td>learns</td>\n",
              "      <td>more unsure</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>This hurts perplexity , as the model learns to be more unsure , but improves accuracy and BLEU score .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>265</th>\n",
              "      <td>German translation task</td>\n",
              "      <td>to Translation is</td>\n",
              "      <td>big transformer model Transformer big in Table 2</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>6 Results 6.1 Machine Translation On the WMT 2014 English to German translation task , the big transformer model Transformer big in Table 2 outperforms the best previously reported models including ensembles by more than 2.0 BLEU , establishing a new state of the art BLEU score of 28.4 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>266</th>\n",
              "      <td>configuration</td>\n",
              "      <td>is listed in</td>\n",
              "      <td>bottom line of Table 3</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>The configuration of this model is listed in the bottom line of Table 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>267</th>\n",
              "      <td>Training</td>\n",
              "      <td>took on</td>\n",
              "      <td>8 P100 GPUs</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>Training took 3.5 days on 8 P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>268</th>\n",
              "      <td>Training</td>\n",
              "      <td>took at_time</td>\n",
              "      <td>3.5 days</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Date</td>\n",
              "      <td>Training took 3.5 days on 8 P100 GPUs .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>269</th>\n",
              "      <td>our big model</td>\n",
              "      <td>achieves</td>\n",
              "      <td>BLEU score of 41.0</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>270</th>\n",
              "      <td>our big model</td>\n",
              "      <td>achieves BLEU score On</td>\n",
              "      <td>WMT 2014 English to French translation task</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>On the WMT 2014 English to French translation task , our big model achieves a BLEU score of 41.0 , outperforming all of the previously published single models , at less than 1 4 the training cost of the previous state of the art model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>we</td>\n",
              "      <td>used model For</td>\n",
              "      <td>base models</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>we</td>\n",
              "      <td>used</td>\n",
              "      <td>single model obtained</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>For the base models , we used a single model obtained by averaging the last 5 checkpoints , which were written at 10 minute intervals .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>we</td>\n",
              "      <td>averaged</td>\n",
              "      <td>last 20 checkpoints</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>For the big models , we averaged the last 20 checkpoints .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>We</td>\n",
              "      <td>used beam search with</td>\n",
              "      <td>beam size</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We used beam search with a beam size of 4 and length penalty 0.6 31 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>hyperparameters</td>\n",
              "      <td>were chosen after</td>\n",
              "      <td>experimentation on development set</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>These hyperparameters were chosen after experimentation on the development set .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>276</th>\n",
              "      <td>Table 2</td>\n",
              "      <td>summarizes</td>\n",
              "      <td>our results</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>277</th>\n",
              "      <td>Table 2</td>\n",
              "      <td>compares</td>\n",
              "      <td>our translation quality</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>278</th>\n",
              "      <td>Table 2</td>\n",
              "      <td>compares training costs to</td>\n",
              "      <td>other model architectures from literature</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>279</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>estimate of sustained single precision</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>We</td>\n",
              "      <td>estimate</td>\n",
              "      <td>number of floating point operations used</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We estimate the number of floating point operations used to train a model by multiplying the training time , the number of GPUs used , and an estimate of the sustained single precision floating point capacity of each GPU 5 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>281</th>\n",
              "      <td>change</td>\n",
              "      <td>is in</td>\n",
              "      <td>performance</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>282</th>\n",
              "      <td>we</td>\n",
              "      <td>varied in</td>\n",
              "      <td>different ways</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>283</th>\n",
              "      <td>we</td>\n",
              "      <td>measuring</td>\n",
              "      <td>change in performance on English to German translation on development set</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284</th>\n",
              "      <td>we</td>\n",
              "      <td>varied</td>\n",
              "      <td>our base model</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>6.2 Model Variations To evaluate the importance of different components of the Transformer , we varied our base model in different ways , measuring the change in performance on English to German translation on the development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>285</th>\n",
              "      <td>We</td>\n",
              "      <td>described in</td>\n",
              "      <td>previous section</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We used beam search as described in the previous section , but no checkpoint averaging .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>286</th>\n",
              "      <td>We</td>\n",
              "      <td>present</td>\n",
              "      <td>results in Table 3</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We present these results in Table 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>287</th>\n",
              "      <td>results</td>\n",
              "      <td>is in</td>\n",
              "      <td>Table 3</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>We present these results in Table 3 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>we</td>\n",
              "      <td>keeping amount</td>\n",
              "      <td>described in Section 3.2.2</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>we</td>\n",
              "      <td>vary number In</td>\n",
              "      <td>Table</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Name</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>we</td>\n",
              "      <td>vary</td>\n",
              "      <td>number of attention heads</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>we</td>\n",
              "      <td>vary attention key In</td>\n",
              "      <td>Table</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Name</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>292</th>\n",
              "      <td>we</td>\n",
              "      <td>keeping</td>\n",
              "      <td>amount of computation constant</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In Table 3 rows A , we vary the number of attention heads and the attention key and value dimensions , keeping the amount of computation constant , as described in Section 3.2.2 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>293</th>\n",
              "      <td>single head attention</td>\n",
              "      <td>is</td>\n",
              "      <td>0.9 BLEU worse than best setting</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>294</th>\n",
              "      <td>quality</td>\n",
              "      <td>also drops off with</td>\n",
              "      <td>too many heads</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>While single head attention is 0.9 BLEU worse than the best setting , quality also drops off with too many heads .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>295</th>\n",
              "      <td>5We</td>\n",
              "      <td>used values respectively for</td>\n",
              "      <td>K80</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Name</td>\n",
              "      <td>5We used values of 2.8 , 3.7 , 6.0 and 9.5 TFLOPS for K80 , K40 , M40 and P100 , respectively .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>296</th>\n",
              "      <td>Unlisted values</td>\n",
              "      <td>are identical to</td>\n",
              "      <td>those of base model</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>Unlisted values are identical to those of the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>metrics</td>\n",
              "      <td>are on</td>\n",
              "      <td>English to German translation development set</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>All metrics are on the English to German translation development set , newstest2013 .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>298</th>\n",
              "      <td>Listed perplexities</td>\n",
              "      <td>are per</td>\n",
              "      <td>wordpiece</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Listed perplexities are per wordpiece , according to our byte pair encoding , and should not be compared to per word perplexities .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>299</th>\n",
              "      <td>steps</td>\n",
              "      <td>dev</td>\n",
              "      <td>dev 106 base</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>N dmodel dff h dk dv Pdrop ls train PPL BLEU params steps dev dev 106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 A 1 512 512 5.29 24.9 4 128 128 5.00 25.5 16 32 32 4.91 25.8 32 16 16 5.01 25.4 B 16 5.16 25.1 58 32 5.01 25.4 60 C 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 D 0.0 5.77 24.6 0.2 4.95 25.5 0.0 4.67 25.3 0.2 5.47 25.7 E positional embedding instead of sinusoids 4.92 25.7 big 6 1024 4096 16 0.3 300K 4.33 26.4 213 In Table 3 rows B , we observe that reducing the attention key size dk hurts model quality .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>300</th>\n",
              "      <td>we</td>\n",
              "      <td>observe results to</td>\n",
              "      <td>base model</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>301</th>\n",
              "      <td>we</td>\n",
              "      <td>observe</td>\n",
              "      <td>nearly identical results</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>302</th>\n",
              "      <td>we</td>\n",
              "      <td>replace</td>\n",
              "      <td>our sinusoidal positional encoding</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>In row E we replace our sinusoidal positional encoding with learned positional embeddings 8 , and observe nearly identical results to the base model .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303</th>\n",
              "      <td>7 Conclusion</td>\n",
              "      <td>is In</td>\n",
              "      <td>work</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>304</th>\n",
              "      <td>first sequence transduction model</td>\n",
              "      <td>based entirely on</td>\n",
              "      <td>attention</td>\n",
              "      <td>None</td>\n",
              "      <td>Noun</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>replacing</td>\n",
              "      <td>recurrent layers most commonly used in encoder decoder architectures with multi</td>\n",
              "      <td>Product</td>\n",
              "      <td>None</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>headed</td>\n",
              "      <td>self attention</td>\n",
              "      <td>Product</td>\n",
              "      <td>None</td>\n",
              "      <td>7 Conclusion In this work , we presented the Transformer , the first sequence transduction model based entirely on attention , replacing the recurrent layers most commonly used in encoder decoder architectures with multi headed self attention .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>Transformer</td>\n",
              "      <td>can</td>\n",
              "      <td>For translation tasks can trained significantly faster than architectures</td>\n",
              "      <td>Product</td>\n",
              "      <td>None</td>\n",
              "      <td>For translation tasks , the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>our best model</td>\n",
              "      <td>outperforms</td>\n",
              "      <td>even previously reported ensembles</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "      <td>In the former task our best model outperforms even all previously reported ensembles .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>We</td>\n",
              "      <td>are excited about</td>\n",
              "      <td>future of attention</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>We are excited about the future of attention based models and plan to apply them to other tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>310</th>\n",
              "      <td>We</td>\n",
              "      <td>apply</td>\n",
              "      <td>them</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>We are excited about the future of attention based models and plan to apply them to other tasks .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>311</th>\n",
              "      <td>problems</td>\n",
              "      <td>involving</td>\n",
              "      <td>input modalities other than text</td>\n",
              "      <td>Noun</td>\n",
              "      <td>None</td>\n",
              "      <td>We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>312</th>\n",
              "      <td>We</td>\n",
              "      <td>extend Transformer to</td>\n",
              "      <td>problems</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>313</th>\n",
              "      <td>outputs</td>\n",
              "      <td>large inputs such as</td>\n",
              "      <td>images</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local , restricted attention mechanisms to efficiently handle large inputs and outputs such as images , audio and video .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>314</th>\n",
              "      <td>code</td>\n",
              "      <td>is available at</td>\n",
              "      <td>https</td>\n",
              "      <td>Noun</td>\n",
              "      <td>Noun</td>\n",
              "      <td>The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>315</th>\n",
              "      <td>we</td>\n",
              "      <td>train</td>\n",
              "      <td>our models</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>The code we used to train and evaluate our models is available at https : github.com tensorflow tensor2tensor .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>316</th>\n",
              "      <td>We</td>\n",
              "      <td>are</td>\n",
              "      <td>grateful to Nal Kalchbrenner for their fruitful comments</td>\n",
              "      <td>pronoun</td>\n",
              "      <td>None</td>\n",
              "      <td>Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments , corrections and inspiration .</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5e0fa12-c8cd-48b6-b136-25da0e9c2c7f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f5e0fa12-c8cd-48b6-b136-25da0e9c2c7f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f5e0fa12-c8cd-48b6-b136-25da0e9c2c7f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d9d5d882-4372-40bf-980f-9c833d719b42\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d9d5d882-4372-40bf-980f-9c833d719b42')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d9d5d882-4372-40bf-980f-9c833d719b42 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_9066968b-17b1-4482-abf0-7e779cec9cc5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('new_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_9066968b-17b1-4482-abf0-7e779cec9cc5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('new_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "new_df",
              "summary": "{\n  \"name\": \"new_df\",\n  \"rows\": 317,\n  \"fields\": [\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 140,\n        \"samples\": [\n          \"complexity\",\n          \"queries\",\n          \"we\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"relation\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 224,\n        \"samples\": [\n          \"establishes model state On\",\n          \"is much faster\",\n          \"multiply weights In\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"object\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 286,\n        \"samples\": [\n          \"training\",\n          \"too many heads\",\n          \"dmodel 512\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"Verb\",\n          \"Cardinal\",\n          \"Adjective\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"object_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"pronoun\",\n          \"Verb\",\n          \"Noun\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 150,\n        \"samples\": [\n          \" The dimensionality of input and output is dmodel 512 , and the inner layer has dimensionality dff 2048 .\",\n          \" This inherently sequential nature precludes parallelization within training examples , which becomes critical at longer sequence lengths , as memory constraints limit batching across examples .\",\n          \" In addition , we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EmDGM5TndT4x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}